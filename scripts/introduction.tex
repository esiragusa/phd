\chapter{Introduction}

The sequencing of the whole human genome, consisting of three billion base pairs of DNA, has been one of the major scientific achievements of the last decades.
Very surprisingly, this milestone has been achieved twice, by two independent groups.
The three billion dollars publicly funded \emph{Human Genome Project} (HGP) published \citep{hgp} in February 2001 the first draft covering more than 96\~\% of the euchromatic part of the human genome, and finally the full sequence in April 2003.
The privately funded company \emph{Celera Genomics} concurrently published \citep{celerahuman} a 2.91 billion base pair consensus sequence of the euchromatic portion of the human genome.
These two groups also contributed to characterize the entire genomes of other model organisms, such as mice \citep{mouse}, fruit flies \citep{celerafly}, nematode worms and yeasts.

This has been not only an achievement of molecular biology but a huge computational effort.
Sequencing technologies are based on the \emph{shotgun} approach that chops DNA fragments up into smaller segments and then generates \emph{reads} of these short nucleotide sequences.
It’s left up to bioinformatics to reassemble these reads and produce a holistic representation of the donor genome.
In order to produce an assembly of high fidelity, DNA must be sequenced multiple times and  produce a manyfold coverage.
The HGP used the \emph{hierarchical shotgun}.
In this approach, genomic DNA is cut into fragments of about 150 Mb, whose order is determined (the so called Golden Tiling Path); then each fragment is independently sequenced by shotgun.
Celera Genomics instead adopted \emph{whole genome shotgun}, a technique which had been used only for small prokaryotic organisms.
This sequencing method is even more computationally involved, but much cheaper.

The knowledge of the complete genomes of model organisms opened the door to a myriad of new computational studies and scientific discoveries.
For instance, the first unexpected finding was that the human genome contains about twenty-four thousand protein-coding genes, which is significantly fewer than what had been previously estimated and a remarkably small number if compared to the nineteen thousand genes of C.~elegans \citep{Hodgkin2001}.
Nowadays, annotated collections of model genomes are hosted by public databases such as GenBank \citep{genbank} and can be consulted interactively \eg via the UCSC genome browser \citep{ucscgb}.

% -----------------------------------------------------------------------------

\section{High-throughput sequencing}

The success of these huge sequencing projects did not mark the end of the sequencing era, but its beginning.
The early sequencing projects used \emph{capillary electrophoresis} based on the \emph{Sanger sequencing} method~\ref{sanger}.
This sequencing technology reads DNA with high fidelity but at low speed: it produces \emph{reads} of an average length of 700~bp at a throughput of about 150~Kbp/h.
At this throughput, the sequencing of the human genome took years and many million dollars.
Since then, sequencing technology steadily improved and evolved into what is now called \emph{high-throughput sequencing} (HTS), or \emph{next-generation sequencing} (NGS).

In 2004, \emph{454 Life Science} commercialized the \emph{Genome Sequencer FLX}, an instrument based on large-scale parallel \emph{pyrosequencing}, capable of sequencing DNA under the form of 400~bp reads at a throughput of 20--30~Mbp/h.
High-throughput sequencing was born.
In 2006, Solexa released its \emph{1G Genetic Analyzer}, based on a massively parallel technique of \emph{reversible terminator-based sequencing}.
The instrument produced reads as short as 30~bp, at lower accuracy than Sanger sequencing, but at very high-throughput: it allowed \emph{resequencing} a human genome in three months for about \$100,000.
Following this success, Solexa was acquired by \emph{Illumina}, which is nowadays the market leader.
At the beginning of 2014, Illumina announced the \emph{HiSeq X Ten}, allowing the sequencing of many human whole-genomes at \$1,000 each, in less than three days.

Over the last decade, numerous other sequencing instruments hit the market with mixed success, \eg the \emph{SOLiD} system of \emph{Applied Biosystems}, as well as the \emph{third-generation sequencing} \emph{Ion Torrent} instruments by \emph{Life Technologies} and the \emph{RS II} by \emph{Pacific Bioscience}.
I will return to sequencing technologies in chapter~\ref{sec:background:sequencing}, to give more insights on the kind of data produced and understand the problematics linked with their analysis.
In the following, I introduce most relevant \emph{applications} of high-throughput sequencing.
Why should we resequence an individual, after all?

\subsection{Sequencing protocols and applications}

%HTS impact ranges from biology to medicine.
%Resequencing applications include...

\subsubsection{DNA-seq}

Whole genome sequencing.

Whole exome sequencing.

Targeted sequencing.

\subsubsection{RNA-seq}
%RNA sequencing (RNA-seq) is a method of investigating the transcriptome of an organism using deep-sequencing techniques.
%The RNA content of a sample is directly sequenced after appropriate library construction, providing a rich data set for analysis.
%The high level of sensitivity and resolution provided by this technique makes it a valuable tool for investigating the entire transcriptional landscape.
%The quantitative nature of the data and the high dynamic range of the sequencing technology enables gene expression analysis with a high sensitivity.
%The single-base resolution of the data provides information on single nucleotide polymorphisms (SNPs), alternative splicing, exon/intron boundaries, untranslated regions, and other elements.
%Additionally, prior knowledge of the reference sequence is not required to perform RNA-seq, allowing for de novo transcriptome analysis and detection of novel variants and mutations.
%RNA-seq is an extremely powerful and revolutionary way to investigate transcriptomes, but requires care in order to achieve the highest quality of data.

\subsubsection{ChIP-seq}
%Chromatin immunoprecipitation (ChIP) is a powerful and versatile method for understanding the mechanisms of gene regulation by transcription factors and modified histones.
%It is used to identify chromatin regions which are bound by transcription factors, co-regulators, modified histones, chromatin remodeling proteins, or other nuclear factors from live cells.
%For example, ChIP-seq can be used to analyze protein-DNA interactions, and to determine global methylation pattern in a genome.

%The procedure is time-consuming and involves many steps and variables, each of which must be optimized by the investigator in their model system.
%After cross-linking cells with formaldehyde, chromatin containing covalent complexes between genomic DNA and all nuclear factors is isolated and sheared by sonication into manageable sizes. Immunoprecipitation with an antibody specific for the target nuclear protein of interest also pulls down any specifical genomic DNA sequences bound by this factor.
%Reversal of the chemical cross-linking and nucleic acid purification prepare the DNA for detection by sequencing, hybridization-based microarrays, or PCR.
%ChIP with subsequent next-generation sequencing (ChIP-Seq) is used to study the genome-wide distribution of loci bound by a protein of interest.
%Compared to microarray analysis (ChIP-Chip) ChIP-Seq offers higher spatial resolution, dynamic range, and genome coverage.
%This results in a superior sensitivity and specificity for the detection of DNA binding sites.
%Furthermore, ChIP-Seq generally requires less input material and is more flexible, since no hybridization probes are required and therefore, any species can be studied with sequenced genomes.

\subsection{Analysis stages}

The analysis of high-throughput sequencing data consists of numerous steps, ranging from the initial instrument specific data processing to the final application specific interpretation of the results.
It is conventional wisdom to subdivide such data analysis pipelines in three stages: primary, secondary, and tertiary analysis.
The primary stage of analysis consists of instrument specific processes for generation, collection, and processing of raw sequencing data.
The secondary stage mixes instrument and application specific sequence analysis methods to reconstruct the complete genome of the donor.
The tertiary stage aggregates multiple donor samples and performs data mining to provide interpretations specific to a biomedical application.

\subsubsection{Primary analysis}

Primary analysis consists of instrument specific steps to call base pairs and compute quality metrics.
The base caller processes instrumental signals, then assigns a base to each peak, \ie A, C, G, T, or N.
The software assigns a quality value to each called base, estimating the probability of a base calling error.
On early generation instruments, users could provide their own base calling tool.
Now this process happens automatically on special hardware (\eg FPGAs or GPUs) bundled within the instruments.
The result of primary analysis is a standard FASTQ file containing DNA read sequences and their associated quality scores in Phred format (see \ref{sec:background:phred}).

\subsubsection{Secondary analysis}

Quality assessment, assembly, and genotyping.

Correction, mapping reads to a reference and variant calling.
Correction, de novo assembly.

Correction.

%Before further processing, the raw reads from the machine are often assessed and filtered for quality to produce the best results.
%Reassembly differs if the sequencing was done on an organism with a polished reference genome or if the genome is to be assembled from scratch, also referred to as de novo assembly.

Three plans of assembly.
%With a reference genome available, the process is much simpler, as the reads simply need to be aligned to the reference, often with some tolerance for a few base-pair errors in the sequences of the reference itself.


%The next step in the sequence analysis process is detection of variants.
%While more customizable, and sometimes considered part of tertiary analysis, variant calling lends itself to being pipelined in the same manner as secondary analysis.
%Variant calling is the process of accurately determining the variations (or differences) between a sample and the reference genome.
%These may be in the form of single nucleotide variants (SNVs), small insertions or deletions (indels), or large structural variants such as transversions, trans-locations, and copy number variants (CNVs).
The result of secondary analysis is a VFC file containing variant calls.

%In both the case of de novo assembly or reference sequence alignment, you will be shooting for a desired average depth and coverage of sequence reads over the entire genome or targeted regions of interest.
%Depth is a measure of how many reads cover a given locus of the genome.
%If you were to pile up the reads to where they were assembled or mapped (pictured above), the depth would be the height of this pileup at each locus.
%For de novo assembly, a higher average depth is usually needed, so that large contigs can be formed that are then the building blocks for a draft genome.
%In the case of sequence alignment, higher average depth means more certainty in the “consensus” sequence of the sample and more accuracy in detecting variants from the reference.

\subsubsection{Tertiary analysis}

%Tertiary analysis diverges into a spectrum of various study specific downstream investigations.
%Though the research or building of draft genomes for novel variants will have its own specialized tertiary analysis after de novo assembly, I will focus on the more mainstream practice of “resequencing” studies where sequence alignment to a reference genome was used.
%Out of the secondary analysis step of variant calling, you now have a more manageable set of differences between the sequenced samples and the reference, but there is still an enormous amount of data to make sense of.
%This is the realm of tertiary analysis.

%The typical SNPs analysis pipeline~\ref{fig:ngs-pipeline} consists of...
%\begin{figure}[h]
%\caption{NGS pipeline.}
%\label{fig:ngs-pipeline}
%\end{figure}

\subsection{Secondary analysis plans}

Given a set of reads, two approaches are possible: assembly and mapping.
Assembly methods are based on overlaps, de brujin graphs, or...
Read mapping methods work on a previously assembled reference genome.

%\begin{itemize}
%\item Plan A: de-novo assembly
%\item Plan B: reference mapping
%\item Plan C: reference guided de-novo assembly
%\end{itemize}

\subsection{Read mapping}

% -----------------------------------------------------------------------------

\section{Outline}

The recent advances of sequencing technologies motivate this work.

%In this work we focus on read mapping, although many core algorithms considered are also applicable to assembly, as well as to later pipeline stages.
I develop methods for the mapping of DNA and ChIP-seq reads.

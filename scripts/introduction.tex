\chapter{Introduction}

The sequencing of the whole human genome, consisting of three billion base pairs of DNA, has been one of the major scientific achievements of the last decades.
Very surprisingly, this milestone has been achieved twice, by two independent groups.
The three billion dollars publicly funded \emph{Human Genome Project} (HGP) in February 2001 published the first draft covering more than 96\~\% of the euchromatic part of the human genome \citep{hgp}, and finally the full sequence in April 2003.
The privately funded company \emph{Celera Genomics} concurrently published a 2.91 billion base pair consensus sequence of the euchromatic portion of the human genome \citep{celerahuman}.
These two groups also contributed to characterize the entire genomes of other model organisms, such as mice \citep{mouse}, fruit flies \citep{celerafly}, nematode worms and yeasts.

These achievements are not only due to the efforts of molecular biologists but also of computer scientists.
Indeed, both groups used sequencing technologies based on the \emph{shotgun} approach.
Shotgun sequencing consists in chopping long DNA fragments up into smaller segments and then generating \emph{reads} of these short nucleotide sequences.
Then, it is left up to \emph{bioinformatics} to reassemble these reads and produce a holistic representation of the original genome.

Those early sequencing projects used \emph{capillary electrophoresis} based on the \emph{Sanger sequencing} method~\ref{sanger}.
This sequencing technology is capable of reading DNA with high fidelity but at low speed: it produces reads of an average length of 700~bp at a throughput of about 150~Kbp/h.
Moreover, in order to produce an assembly with high fidelity, DNA must be sequenced multiple times to obtain a manyfold \emph{coverage}.
At this pace, the sequencing of the human genome took years and many million dollars.
Nonetheless, the success of those huge sequencing projects did not mark the end of the sequencing era, but its beginning.

% -----------------------------------------------------------------------------

\section{High-throughput sequencing}

Since then, sequencing technology steadily improved and evolved into what is now called \emph{high-throughput sequencing} (HTS), or \emph{next-generation sequencing} (NGS).
In 2004, \emph{454 Life Science} commercialized the \emph{Genome Sequencer FLX}, an instrument based on large-scale parallel \emph{pyrosequencing}, capable of sequencing DNA under the form of 400~bp reads at a throughput of 20--30~Mbp/h.
High-throughput sequencing was born.

In 2006, Solexa released its \emph{1G Genetic Analyzer}, based on a massively parallel technique of \emph{reversible terminator-based sequencing}.
The instrument produced reads as short as 30~bp, at lower accuracy than Sanger sequencing, but at very high-throughput: it allowed \emph{resequencing} a human genome in three months for about \$100,000.
Following this success, Solexa was acquired by \emph{Illumina}, which is nowadays the market leader.
At the beginning of 2014, Illumina announced the \emph{HiSeq X Ten}, allowing the sequencing of many human whole-genomes at \$1,000 each, in less than three days.

Over the last decade, numerous other sequencing instruments hit the market with mixed success, \eg the \emph{SOLiD} system of \emph{Applied Biosystems}, as well as the \emph{third-generation sequencing} \emph{Ion Torrent} instruments by \emph{Life Technologies} and the \emph{RS II} by \emph{Pacific Bioscience}.
I will return to sequencing technologies in chapter~\ref{sec:background:sequencing}, to give more insights on the kind of data produced and understand the problematics linked with their analysis.
In the following, I introduce most relevant \emph{applications} of HTS.
Afterwards, I give an overview of state of the art HTS analysis pipelines.

\subsection{Protocols and applications}

The knowledge of the complete genomes of model organisms opened the door to a myriad of new computational studies and scientific discoveries.
For instance, the first unexpected finding was that the human genome contains about twenty-four thousand protein-coding genes, which is significantly fewer than what had been previously estimated and a remarkably small number if compared to the nineteen thousand genes of C.~elegans \citep{Hodgkin2001}.
Nowadays, model genomes along with extensive annotations are hosted by public databases such as GenBank \citep{genbank} and can be consulted interactively \eg via the UCSC genome browser \citep{ucscgb}.
Why should we resequence an individual, after all?

%HTS impact ranges from biology to medicine.
%Resequencing applications include...

\subsubsection{DNA-reseq}

\emph{Whole genome sequencing} (DNA-reseq).

Alternatively, \emph{whole exome sequencing} (Exome-seq).

%Targeted sequencing?

\subsubsection{RNA-seq}
\emph{RNA sequencing} (RNA-seq) is a method of investigating the \emph{transcriptome} of an organism using deep-sequencing techniques.
The RNA content of a sample is directly sequenced after appropriate library construction, providing a rich data set for gene expression analysis.
RNA-seq provides information on genetic variation, alternative splicing, intron/exon boundaries, untranslated regions (UTRs), and other transcription elements.

\subsubsection{ChIP-seq}
\emph{Chromatin immunoprecipitation} with next-generation sequencing (ChIP-seq) is a method to study the mechanisms of gene regulation by transcription factors and modified histones.
%study the genome-wide distribution of loci bound by a protein of interest.
%can be used to analyze protein-DNA interactions, and to determine global methylation pattern in a genome.
%Detection of DNA binding sites.
It is used to identify chromatin regions which are bound by transcription factors, co-regulators, modified histones, chromatin remodeling proteins, or other nuclear factors from live cells.

%After cross-linking cells with formaldehyde, chromatin containing covalent complexes between genomic DNA and all nuclear factors is isolated and sheared by sonication into manageable sizes.
%Immunoprecipitation with an antibody specific for the target nuclear protein of interest also pulls down any specifical genomic DNA sequences bound by this factor.
%Reversal of the chemical cross-linking and nucleic acid purification prepare the DNA for detection by sequencing, hybridization-based microarrays, or PCR.

\subsection{Analysis pipelines}

The analysis of HTS data consists of numerous steps, ranging from the initial, instrument specific, data processing to the final, application specific, interpretation of the results.
It is conventional wisdom to subdivide such data analysis pipelines in three stages of \emph{primary}, \emph{secondary}, and \emph{tertiary} analysis.
The primary stage of analysis consists of instrument specific processes for generation, collection, and processing of raw sequencing data.
The secondary stage mixes instrument and application specific sequence analysis methods to reconstruct the original sequence of the donor genome.
The tertiary stage characterizes features specific to the sequencing application \eg genetic variations in exome-seq, then provides interpretations, eventually by aggregating multiple samples and performing data mining.

%The typical SNPs analysis pipeline~\ref{fig:ngs-pipeline} consists of...
%\begin{figure}[h]
%\caption{NGS pipeline.}
%\label{fig:ngs-pipeline}
%\end{figure}

\subsubsection{Primary analysis}

Primary analysis consists of instrument specific steps to call base pairs and compute quality metrics.
The base caller processes instrumental signals and assigns a base to each peak, \ie A, C, G, T, or N.
The software assigns a quality value to each called base, estimating the probability of a base calling error.
On early generation instruments, users could provide their own base calling tool.
Now this process happens automatically on special hardware (\eg FPGAs or GPUs) bundled within the instrument.
The result of primary analysis is a standard \emph{FASTQ} file containing DNA read sequences and their associated quality scores in \emph{Phred} format (see section \ref{sec:background:phred}).

\subsubsection{Secondary analysis}

Secondary analysis aims at reconstructing the original sequence of the donor genome.
There are two main \emph{plans} to reassemble the original genome: \begin{inparaenum}[(A)]
\item \emph{de novo} assembly, and
\item \emph{reference-guided} assembly (commonly called \emph{read mapping}).
\end{inparaenum}
\emph{De novo} methods essentially scaffold the reads by performing \emph{overlap alignments} \citep{?} or equivalently by constructing \emph{de Bruijn graphs} \citep{?}.
Computational methods in plan A are very involved, as they require finding a \emph{shortest common superstring}, which is a NP-complete problem \citep{?}.
The knowledge of a \emph{reference genome}, highly similar to the donor, simplifies the problem and opens the door to plan B.
The reads are simply aligned to the reference, tolerating a few base pair errors.
It is worth mentioning that some combinations of plans A and B have been proposed in \citep{fermi,?}.

Plan B is always preferred in resequencing projects, as it is more computationally viable than plan A and directly provides a way to assess genetic variation \wrt a reference genome.
In this manuscript, I follow only plan B and present methods that work within this specific plan.
Nonetheless, many of the algorithmic components that I expose in the first part of this manuscript are ubiquitous in bioinformatics, thus applicable, if not already applied, to plan A.
According to plan B, the secondary analysis step consists of three tasks: \emph{quality control}, \emph{read mapping}, and \emph{pile up}.

\emph{Quality control} (Q/C) performs specific computations to remove artifacts introduced by the sequencer that could potentially bias final results.
Q/C ranges from the simple \emph{trimming} of low quality read stretches to sophisticated methods of \emph{read error correction} \citep{fiona}.

\emph{Read mapping} takes care of aligning the reads to the reference genome.
This has been the most compute intensive task.
Tools using state-of-the-art \emph{approximate string matching} methods.

Mapped reads, often stored in de-facto standard BAM files, are \emph{piled up} in order to construct a \emph{consensus sequence} of the donor genome.
The height of the pileup denotes the sequencing depth at each locus in the donor genome, thus the average height corresponds to the average sequencing coverage;
higher depth implies more confidence in the consensus sequence of the donor and thus higher genotyping accuracy.

\subsubsection{Tertiary analysis}

Tertiary analysis aims at interpreting the deluge of information provided by secondary analysis.
This stage groups a wide range of analyses specific to the sequencing application.
In many pipelines, downstream analysis performs data mining over aggregated data coming from multiple samples.

Within DNA-reseq, \emph{genotyping} consists of determining the variations between the donor and the reference genome.
These may be in the form of single nucleotide variants (SNVs), small insertions or deletions (indels), or large structural variants such as transversions, trans-locations, and copy number variants (CNVs).
The result of \emph{variant calling} is usually a VFC file annotating the variations of the donor genome.
Subsequently, \emph{genome-wide association studies} (GWAS) associate genetic variants with phenotypic traits by examining \emph{single-nucleotide polymorphisms} (SNPs), variants relatively common among individuals of the same population.

In RNA-seq, tertiary analysis consists of computing transcripts abundance by measuring \emph{reads per kilobase per million mapped reads} (RPKM) \citep{rnaseq}.
Subsequently, relative gene expression is determined by comparing multiple RNA samples.
In ChIP-seq, this stage consists of calling peaks in correspondence of mapped reads, determining which peaks identify feasibile transcription factor binding sites, then interesting peaks denoting regulation of genes.

% -----------------------------------------------------------------------------

\section{Outline}

The recent advances of HTS technologies motivate this work.

%In this work we focus on read mapping, although many core algorithms considered are also applicable to assembly, as well as to later pipeline stages.
I develop methods for the mapping of DNA and ChIP-seq reads.

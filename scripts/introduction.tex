\chapter{Introduction}

The sequencing of the whole human genome, consisting of three billion base pairs of DNA, has been one of the major scientific achievements of the last decades.
Very surprisingly, this milestone has been achieved twice, by two independent groups.
The three billion dollars publicly funded \emph{Human Genome Project} (HGP) in February 2001 published the first draft covering more than 96\~\% of the euchromatic part of the human genome \citep{hgp}, and finally the full sequence in April 2003.
The privately funded company \emph{Celera Genomics} concurrently published a 2.91 billion base pair consensus sequence of the euchromatic portion of the human genome \citep{celerahuman}.
These two groups also contributed to characterize the entire genomes of other model organisms, such as mice \citep{mouse}, fruit flies \citep{celerafly}, nematode worms and yeasts.

These achievements are not only due to the efforts of molecular biologists but also of computer scientists.
Indeed, both groups used sequencing technologies based on the \emph{shotgun} approach.
Shotgun sequencing consists in chopping long DNA fragments up into smaller segments and then generating \emph{reads} of these short nucleotide sequences.
Then, it is left up to \emph{bioinformatics} to reassemble these reads and produce a holistic representation of the original genome.

%The HGP used the \emph{hierarchical shotgun}.
%In this approach, genomic DNA is cut into fragments of about 150 Mb, whose order is determined (the so called Golden Tiling Path); then each fragment is independently sequenced by shotgun.
%Celera Genomics instead adopted \emph{whole genome shotgun}, a technique which had been used only for small prokaryotic organisms.
%This sequencing method requires a more computationally involved, but it is much quicker and cheaper.

Those early sequencing projects used \emph{capillary electrophoresis} based on the \emph{Sanger sequencing} method~\ref{sanger}.
This sequencing technology is capable of reading DNA with high fidelity but at low speed: it produces reads of an average length of 700~bp at a throughput of about 150~Kbp/h.
Moreover, in order to produce an assembly with high fidelity, DNA must be sequenced multiple times to obtain a manyfold \emph{coverage}.
At this pace, the sequencing of the human genome took years and many million dollars.
Nonetheless, the success of those huge sequencing projects did not mark the end of the sequencing era, but its beginning.

% -----------------------------------------------------------------------------

\section{High-throughput sequencing}

Since then, sequencing technology steadily improved and evolved into what is now called \emph{high-throughput sequencing} (HTS), or \emph{next-generation sequencing} (NGS).
In 2004, \emph{454 Life Science} commercialized the \emph{Genome Sequencer FLX}, an instrument based on large-scale parallel \emph{pyrosequencing}, capable of sequencing DNA under the form of 400~bp reads at a throughput of 20--30~Mbp/h.
High-throughput sequencing was born.
In 2006, Solexa released its \emph{1G Genetic Analyzer}, based on a massively parallel technique of \emph{reversible terminator-based sequencing}.
The instrument produced reads as short as 30~bp, at lower accuracy than Sanger sequencing, but at very high-throughput: it allowed \emph{resequencing} a human genome in three months for about \$100,000.
Following this success, Solexa was acquired by \emph{Illumina}, which is nowadays the market leader.
At the beginning of 2014, Illumina announced the \emph{HiSeq X Ten}, allowing the sequencing of many human whole-genomes at \$1,000 each, in less than three days.

Over the last decade, numerous other sequencing instruments hit the market with mixed success, \eg the \emph{SOLiD} system of \emph{Applied Biosystems}, as well as the \emph{third-generation sequencing} \emph{Ion Torrent} instruments by \emph{Life Technologies} and the \emph{RS II} by \emph{Pacific Bioscience}.
I will return to sequencing technologies in chapter~\ref{sec:background:sequencing}, to give more insights on the kind of data produced and understand the problematics linked with their analysis.
In the following, I introduce most relevant \emph{applications} of HTS.
Afterwards, I give an overview of state of the art HTS analysis pipelines.

\subsection{Protocols and applications}

The knowledge of the complete genomes of model organisms opened the door to a myriad of new computational studies and scientific discoveries.
For instance, the first unexpected finding was that the human genome contains about twenty-four thousand protein-coding genes, which is significantly fewer than what had been previously estimated and a remarkably small number if compared to the nineteen thousand genes of C.~elegans \citep{Hodgkin2001}.
Nowadays, model genomes along with extensive annotations are hosted by public databases such as GenBank \citep{genbank} and can be consulted interactively \eg via the UCSC genome browser \citep{ucscgb}.
Why should we resequence an individual, after all?

%HTS impact ranges from biology to medicine.
%Resequencing applications include...

\subsubsection{DNA-seq}

Whole genome sequencing.

Whole exome sequencing.

Targeted sequencing.

\subsubsection{RNA-seq}
%RNA sequencing (RNA-seq) is a method of investigating the transcriptome of an organism using deep-sequencing techniques.
%The RNA content of a sample is directly sequenced after appropriate library construction, providing a rich data set for analysis.
%The high level of sensitivity and resolution provided by this technique makes it a valuable tool for investigating the entire transcriptional landscape.
%The quantitative nature of the data and the high dynamic range of the sequencing technology enables gene expression analysis with a high sensitivity.
%The single-base resolution of the data provides information on single nucleotide polymorphisms (SNPs), alternative splicing, exon/intron boundaries, untranslated regions, and other elements.
%Additionally, prior knowledge of the reference sequence is not required to perform RNA-seq, allowing for de novo transcriptome analysis and detection of novel variants and mutations.
%RNA-seq is an extremely powerful and revolutionary way to investigate transcriptomes, but requires care in order to achieve the highest quality of data.

\subsubsection{ChIP-seq}
%Chromatin immunoprecipitation (ChIP) is a powerful and versatile method for understanding the mechanisms of gene regulation by transcription factors and modified histones.
%It is used to identify chromatin regions which are bound by transcription factors, co-regulators, modified histones, chromatin remodeling proteins, or other nuclear factors from live cells.
%For example, ChIP-seq can be used to analyze protein-DNA interactions, and to determine global methylation pattern in a genome.

%The procedure is time-consuming and involves many steps and variables, each of which must be optimized by the investigator in their model system.
%After cross-linking cells with formaldehyde, chromatin containing covalent complexes between genomic DNA and all nuclear factors is isolated and sheared by sonication into manageable sizes. Immunoprecipitation with an antibody specific for the target nuclear protein of interest also pulls down any specifical genomic DNA sequences bound by this factor.
%Reversal of the chemical cross-linking and nucleic acid purification prepare the DNA for detection by sequencing, hybridization-based microarrays, or PCR.
%ChIP with subsequent next-generation sequencing (ChIP-Seq) is used to study the genome-wide distribution of loci bound by a protein of interest.
%Compared to microarray analysis (ChIP-Chip) ChIP-Seq offers higher spatial resolution, dynamic range, and genome coverage.
%This results in a superior sensitivity and specificity for the detection of DNA binding sites.
%Furthermore, ChIP-Seq generally requires less input material and is more flexible, since no hybridization probes are required and therefore, any species can be studied with sequenced genomes.

\subsection{Analysis pipelines}

The analysis of HTS data consists of numerous steps, ranging from the initial instrument specific data processing to the final application specific interpretation of the results.
It is conventional wisdom to subdivide such data analysis pipelines in three stages of \emph{primary}, \emph{secondary}, and \emph{tertiary} analysis.
The primary stage of analysis consists of instrument specific processes for generation, collection, and processing of raw sequencing data.
The secondary stage mixes instrument and application specific sequence analysis methods to reconstruct the original sequence of the donor genome and characterize its variations.
The tertiary stage aggregates multiple samples and performs data mining to provide interpretations specific to some biomedical application.

%The typical SNPs analysis pipeline~\ref{fig:ngs-pipeline} consists of...
%\begin{figure}[h]
%\caption{NGS pipeline.}
%\label{fig:ngs-pipeline}
%\end{figure}

\subsubsection{Primary analysis}

Primary analysis consists of instrument specific steps to call base pairs and compute quality metrics.
The base caller processes instrumental signals and assigns a base to each peak, \ie A, C, G, T, or N.
The software assigns a quality value to each called base, estimating the probability of a base calling error.
On early generation instruments, users could provide their own base calling tool.
Now this process happens automatically on special hardware (\eg FPGAs or GPUs) bundled within the instrument.
The result of primary analysis is a standard \emph{FASTQ} file containing DNA read sequences and their associated quality scores in \emph{Phred} format (see section \ref{sec:background:phred}).

\subsubsection{Secondary analysis}

Secondary analysis aims at reconstructing the original sequence of the donor genome and then characterizing its variations.
There are two main \emph{plans} to reassemble the original genome: \begin{inparaenum}[(A)]
\item \emph{de novo} assembly, and
\item \emph{reference-guided} assembly (commonly called \emph{read mapping}).
\end{inparaenum}
\emph{De novo} methods essentially scaffold the reads by performing \emph{overlap alignments} \citep{?} or equivalently by constructing \emph{de Bruijn graphs} \citep{?}.
Computational methods in plan A are very involved, as they require finding a \emph{shortest common superstring}, which is a NP-complete problem \citep{?}.
The knowledge of a \emph{reference genome}, highly similar to the donor, simplifies the problem and opens the door to plan B.
The reads are simply aligned to the reference, tolerating a few base pair errors.
It is worth mentioning that some combinations of plans A and B have been proposed in \citep{fermi,?}.

Plan B is always preferred in resequencing projects, as it is more computationally viable than plan A and directly provides a way to assess genetic variation \wrt a reference genome.
In this manuscript, I follow only plan B and present methods that work within this specific plan.
Nonetheless, many of the algorithmic components that I expose in the first part of this manuscript are ubiquitous in bioinformatics, thus applicable, if not already applied, to plan A.
According to plan B, the secondary analysis step consists of three tasks: \emph{quality control}, \emph{read mapping}, and \emph{genotyping}.

\emph{Quality control} (Q/C) performs specific computations to remove artefacts introduced by the sequencer that could potentially bias final results.
Q/C ranges from the simple \emph{trimming} of low quality read stretches to sophisticated methods of \emph{read error correction} \citep{fiona}.

\emph{Read mapping} takes care of aligning the reads to the reference genome.
%using state-of-the-art \emph{approximate string matching} methods.
Mapped reads, often stored in de-facto standard BAM files, are \emph{piled up} in order to construct a \emph{consensus sequence} of the donor genome.
The height of the pileup denotes the sequencing depth at each locus in the donor genome, thus the average height corresponds to the average sequencing coverage;
higher depth implies more confidence in the consensus sequence of the donor and thus higher genotyping accuracy.

Within DNA resequencing, \emph{genotyping} consists of determining the variations between the donor and the reference genome.
These may be in the form of single nucleotide variants (SNVs), small insertions or deletions (indels), or large structural variants such as transversions, trans-locations, and copy number variants (CNVs).
The result of \emph{variant calling}, and hence of the secondary analysis stage, is usually a VFC file annotating the features of the donor genome.
%ChIP-seq: calling peaks.
%RNA-seq: transcript abundance or alternative splicing.

\subsubsection{Tertiary analysis}

Tertiary analysis diverges into a range of various application specific downstream analysis.
The secondary analysis step of variant calling, provides a set of differences between the donor and the reference.
Still, these variations lack a meaning.
%GWAS.
%Tertiary analysis is still in its infancy 

%In RNA-Seq, the process of determining relative gene expression means that sequence data from multiple samples must go through the entire process of primary, secondary, and tertiary analysis. 

%Population cataloging efforts such as HapMap and the 1000 Genomes Project allow for studies to be able to determine the relative frequencies of variants in their samples compared to common populations. Once variants have been filtered and ranked, new methods are gaining momentum to handle the analysis of rare variant burden as a model of disease association, as well as other methods specific to the filtering and studying of sequence data.

% -----------------------------------------------------------------------------

\section{Outline}

The recent advances of HTS technologies motivate this work.

%In this work we focus on read mapping, although many core algorithms considered are also applicable to assembly, as well as to later pipeline stages.
I develop methods for the mapping of DNA and ChIP-seq reads.

\chapter{Introduction}

The sequencing of the whole human genome, consisting of three billion base pairs of DNA, has been one of the major scientific achievements of the last decades.
Very surprisingly, this milestone has been achieved twice, by two independent groups.
The three billion dollars publicly funded \emph{Human Genome Project} published \citep{hgp} in February 2001 the first draft covering more than 96\~\% of the euchromatic part of the human genome, and finally the full sequence in April 2003.
The privately funded company \emph{Celera Genomics} concurrently published \citep{celerahuman} a 2.91 billion base pair consensus sequence of the euchromatic portion of the human genome.
These two groups also contributed to characterize the entire genomes of other model organisms, such as mice~\citep{mouse}, fruit flies~\citep{celerafly}, nematode worms and yeasts.

The knowledge of the complete genomes of model organisms opened the door to a myriad of new scientific studies and discoveries.
The first unexpected finding was that the human genome contains about twenty-four thousand protein-coding genes, which is significantly fewer than what had been estimated, and a remarkably low quantity if compared to the nineteen thousand genes of C.elegans~\citep{Hodgkin2001}.
Nowadays, annotated collections of model genomes are hosted by public databases such as GenBank~\citep{genbank} and can be consulted interactively \eg via the UCSC genome browser~\citep{ucscgb}.

%This has been a huge computational effort.
%I was fifteen years old by that time. I begun to be passionate about computers and. I wondered what would be the contributions of computers behind that project. I did not imagine working in this field ten years later.

% -----------------------------------------------------------------------------

\section{High-throughput sequencing}

The success of these huge sequencing projects did not mark the end of the sequencing era, but its beginning.
The early sequencing projects used \emph{capillary electrophoresis} based on the \emph{Sanger sequencing} method~\ref{sanger}.
This sequencing technology reads DNA with high fidelity but at low speed: it produces \emph{reads} of an average length of 700~bp at a throughput of about 150~Kbp/h.
At this throughput, the sequencing of the human genome took years and many million dollars.
Since then, sequencing technology steadily improved and evolved into what is now called \emph{high-throughput sequencing} (HTS), or \emph{next-generation sequencing} (NGS).

In 2004, \emph{454 Life Science} commercialized the \emph{Genome Sequencer FLX}, an instrument based on large-scale parallel \emph{pyrosequencing}, capable of sequencing DNA under the form of 400~bp reads at a throughput of 20--30~Mbp/h.
High-throughput sequencing was born.
In 2006, Solexa released its \emph{1G Genetic Analyzer}, based on a massively parallel technique of \emph{reversible terminator-based sequencing}.
The instrument produced reads as short as 30~bp, at lower accuracy than Sanger sequencing, but at very high-throughput: it allowed \emph{resequencing} a human genome in three months for about \$100,000.
Following this success, Solexa was acquired by \emph{Illumina}, which is nowadays the market leader.
At the beginning of 2014, Illumina announced the \emph{HiSeq X Ten}, allowing the sequencing of many human whole-genomes at \$1,000 each, in less than three days.

Over the last decade, numerous other sequencing instruments hit the market with mixed success, \eg the \emph{SOLiD} system of \emph{Applied Biosystems}, as well as the \emph{third-generation sequencing} \emph{Ion Torrent} instruments by \emph{Life Technologies} and the \emph{RS II} by \emph{Pacific Bioscience}.
I will return to sequencing technologies in chapter~\ref{sec:background:sequencing}, to give more insights on the kind of data produced and understand the problematics linked with their analysis.
In the following, I introduce most relevant \emph{applications} of high-throughput sequencing.
Why should we resequence an individual, after all?

%At the same time, \emph{Applied Biosystems} entered the market with its \emph{SOLiD} system, based on parallel sequencing by \emph{stepwise ligation}, reading two bases at a time with a florescent label in order to improve sequencing accuracy.

%Next-generation sequencing has been the second revolution.
%NGS produces billions of reads for 1000\$ dollars.
%Why should one re-sequence a known genome?
%Resequencing applications include...
%NGS impacts biomedicine.

\subsection{Sequencing protocols and applications}

\subsubsection{DNA-seq}

Whole genome sequencing.

Whole exome sequencing.

Targeted sequencing.

\subsubsection{RNA-seq}
%RNA sequencing (RNA-seq) is a method of investigating the transcriptome of an organism using deep-sequencing techniques. The RNA content of a sample is directly sequenced after appropriate library construction, providing a rich data set for analysis. The high level of sensitivity and resolution provided by this technique makes it a valuable tool for investigating the entire transcriptional landscape. The quantitative nature of the data and the high dynamic range of the sequencing technology enables gene expression analysis with a high sensitivity. The single-base resolution of the data provides information on single nucleotide polymorphisms (SNPs), alternative splicing, exon/intron boundaries, untranslated regions, and other elements. Additionally, prior knowledge of the reference sequence is not required to perform RNA-seq, allowing for de novo transcriptome analysis and detection of novel variants and mutations. RNA-seq is an extremely powerful and revolutionary way to investigate transcriptomes, but requires care in order to achieve the highest quality of data.

\subsubsection{ChIP-seq}
%Chromatin immunoprecipitation (ChIP) is a powerful and versatile method for understanding the mechanisms of gene regulation by transcription factors and modified histones. It is used to identify chromatin regions which are bound by transcription factors, co-regulators, modified histones, chromatin remodeling proteins, or other nuclear factors from live cells.

%Next-generation sequencing has also proved to be a powerful tool for studies of gene regulation networks. For example, ChIP-seq (or chromatin immunoprecipitation sequencing), can be used to analyze protein-DNA interactions. NGS can also be used to determine global methylation pattern in a genome.

%The procedure is time-consuming and involves many steps and variables, each of which must be optimized by the investigator in their model system. After cross-linking cells with formaldehyde, chromatin containing covalent complexes between genomic DNA and all nuclear factors is isolated and sheared by sonication into manageable sizes. Immunoprecipitation with an antibody specific for the target nuclear protein of interest also pulls down any specifical genomic DNA sequences bound by this factor. Reversal of the chemical cross-linking and nucleic acid purification prepare the DNA for detection by sequencing, hybridization-based microarrays, or PCR. ChIP with subsequent next-generation sequencing (ChIP-Seq) is used to study the genome-wide distribution of loci bound by a protein of interest. Compared to microarray analysis (ChIP-Chip) ChIP-Seq offers higher spatial resolution, dynamic range, and genome coverage. This results in a superior sensitivity and specificity for the detection of DNA binding sites. Furthermore, ChIP-Seq generally requires less input material and is more flexible, since no hybridization probes are required and therefore, any species can be studied with sequenced genomes.

\subsection{Analysis stages}

The analysis of high-throughput sequencing data proceeds in three stages.

%Data analysis may be segmented into primary, secondary, and tertiary analysis. Primary analysis includes universal processes for data generation, collection, and raw processing. Both secondary and tertiary analyses are application-specific. Secondary analysis processes application-specific data at the sequence level, while tertiary analysis generates biological interpretation specific to an application.

%The bioinformatics of sequence analysis ranges from instrument specific processing of data to the final aggregation of multiple samples into data mining and analysis tools. The software of sequence analysis can be categorized into the three stages of the data’s lifecycle: primary, secondary, and tertiary analysis.

\subsubsection{Primary analysis}

%can be defined as the machine specific steps needed to call base pairs and compute quality scores for those calls. This often results in a FASTQ file, which is just a combination of the sequence data as a string of A, C, G and T characters and an associated Phred quality score for each of those bases. This is the absolute bare minimum “raw” format you would ever expect to see for sequence data. While the first generation of high throughput sequencing machines, such as the Illumina G1, allowed for users to provide their own alternatives to the standard primary analysis solution, called “the Illumina pipeline”, current generation machines often do this work on bundled computational hardware. The effective output of the machine is the result of the primary analysis. This output is ready for processing in a secondary analysis pipeline.

\subsubsection{Secondary analysis}

%Because current sequencing technologies are generally based on the “shotgun” approach of chopping all the DNA up into smaller molecules and then generating what are referred to as “reads” of these small nucleotide sequences, it’s left up to secondary analysis to reassemble these reads to get a representation of the underlying biology. Before this reassembly, the “raw” reads from the machine are often assessed and filtered for quality to produce the best results. Reassembly differs if the sequencing was done on an organism with a polished reference genome or if the genome is to be assembled from scratch, also referred to as de novo assembly. With a reference genome available, the process is much simpler, as the reads simply need to be aligned to the reference, often with some tolerance for a few base-pair errors in the sequences of the reference itself.

%In both the case of de novo assembly or reference sequence alignment, you will be shooting for a desired average depth and coverage of sequence reads over the entire genome or targeted regions of interest. Depth is a measure of how many reads cover a given locus of the genome. If you were to pile up the reads to where they were assembled or mapped (pictured above), the depth would be the height of this pileup at each locus. For de novo assembly, a higher average depth is usually needed, so that large contigs can be formed that are then the building blocks for a draft genome. In the case of sequence alignment, higher average depth means more certainty in the “consensus” sequence of the sample and more accuracy in detecting variants from the reference.

%The next step in the sequence analysis process is detection of variants. While more customizable, and sometimes considered part of tertiary analysis, variant calling lends itself to being pipelined in the same manner as secondary analysis. Variant calling is the process of accurately determining the variations (or differences) between a sample and the reference genome. These may be in the form of single nucleotide variants, smaller insertions or deletions (called indels), or larger structural variants of categorizations such as transversions, trans-locations, and copy number variants.

\subsubsection{Tertiary analysis}

%Tertiary analysis diverges into a spectrum of various study specific downstream investigations. Though the research or building of draft genomes for novel variants will have its own specialized tertiary analysis after de novo assembly, I will focus on the more mainstream practice of “resequencing” studies where sequence alignment to a reference genome was used. Out of the secondary analysis step of variant calling, you now have a more manageable set of differences between the sequenced samples and the reference, but there is still an enormous amount of data to make sense of. This is the realm of tertiary analysis.

%The typical SNPs analysis pipeline~\ref{fig:ngs-pipeline} consists of...
%\begin{figure}[h]
%\caption{NGS pipeline.}
%\label{fig:ngs-pipeline}
%\end{figure}

\subsection{Secondary analysis plans}

Given a set of reads, two approaches are possible: assembly and mapping.
Assembly methods are based on overlaps, de brujin graphs, or...
Read mapping methods work on a previously assembled reference genome.

%\begin{itemize}
%\item Plan A: de-novo assembly
%\item Plan B: reference mapping
%\item Plan C: reference guided de-novo assembly
%\end{itemize}

\subsection{Read mapping}

% -----------------------------------------------------------------------------

\section{Outline}

The recent advances of sequencing technologies motivate this work.

%In this work we focus on read mapping, although many core algorithms considered are also applicable to assembly, as well as to later pipeline stages.
I develop methods for the mapping of DNA and ChIP-seq reads.

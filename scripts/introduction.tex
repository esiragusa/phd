\chapter{Introduction}

The sequencing of the whole human genome has been one of the major scientific achievements of the last decades.
In February 2001, the three billion dollars publicly funded \emph{Human Genome Project} (HGP) published the first draft \emph{covering more than 96~\% of the euchromatic part of the human genome} \citep{Consortium2001}.
Concurrently, the privately funded company \emph{Celera Genomics} published \emph{a 2.91 billion base pair consensus sequence of the euchromatic portion of the human genome} \citep{Venter2001}.
Celera Genomics and various international sequencing consortiums contributed to characterize the entire genomes of several other model organisms, including mice \citep{Chinwalla2002}, fruit flies \citep{Myers2000}, nematode worms \citep{Sulston1992} and yeasts.
The knowledge of these entire genomics sequences paved the way to a myriad of computational biology studies.

These projects joined the efforts of molecular biologists and computer scientists.
Whole genome sequencing projects used the \emph{Sanger method} \citep{Sanger1977} with \emph{capillary electrophoresis}, a technology producing high fidelity DNA reads long 700~bp in average, at a throughput of about 150 \emph{kilo base pairs per hour} (Kbp/h).
Because of such technological limitation, sequencing of whole genomes had to be coupled with the \emph{shotgun} approach.
Shotgun sequencing consists of chopping long DNA fragments up into smaller segments and then generating \emph{reads} of these short nucleotide sequences.
Then, it is left up to \emph{bioinformatics} to reassemble these reads and produce a holistic representation of the original genome.
%In order to produce a contiguous assembly, DNA sequences must be read with a manyfold \emph{coverage} \citep{Lander1988}.
At the pace of Sanger sequencing, it took years and many million dollars for these projects to complete.
Nonetheless, their success did not mark the end of the sequencing era but its beginning.

% -----------------------------------------------------------------------------

\section{High-throughput sequencing}

Since then, sequencing technology steadily improved and evolved into what is now called \emph{high-throughput sequencing} (HTS) or \emph{next-generation sequencing} (NGS).
In 2004, \emph{454 Life Science} commercialized the \emph{Genome Sequencer FLX}, an instrument based on large-scale parallel \emph{pyrosequencing}, capable of sequencing DNA under the form of 400~bp reads at a throughput of about 20~Mbp/h.
High-throughput sequencing was born.

In 2006, Solexa released its \emph{1G Genetic Analyzer}, based on a massively parallel technique of \emph{reversible terminator-based sequencing}.
The instrument produced reads as short as 30~bp with lower accuracy than Sanger sequencing but at very high-throughput: it allowed \emph{resequencing} a human genome in three months for about \$100,000.
Following this success, Solexa was acquired by \emph{Illumina}, which is nowadays the market leader.
At the beginning of 2014, Illumina announced the \emph{HiSeq X Ten}, allowing in less than three days the sequencing of many whole human genomes at \$1,000 each.

%Over the last decade, other sequencing instruments hit the market with mixed success, \eg the \emph{SOLiD} system of \emph{Applied Biosystems}, as well as the \emph{third-generation sequencing} \emph{Ion Torrent} instruments by \emph{Life Technologies} and the \emph{RS II} by \emph{Pacific Bioscience}.

I return to sequencing technologies in chapter~\ref{chr:background}, where I give insights on the kind of data produced by HTS instruments, in order to understand the problematics linked with their analysis.
In the following of this section, I continue to provide the context of this work.
I first introduce most relevant \emph{applications} of HTS and then explain how HTS \emph{analysis pipelines} proceed.
In the next section, I outline the structure of this work and state my contributions.

\subsection{Protocols and applications}

In the last years, HTS has become an invaluable method of investigation for computational molecular biologists.
Abundant and cost-effective production of sequencing data permits viewing not only genomic DNA but also transcripts and epigenetic features at unpreceded single-base resolution.
For instance, DNA resequencing applications include genotyping and discovery of structural variations, the sequencing of RNA transcripts allows to assess gene expression as well as to analyse non-coding RNAs, epigenetic applications predict gene regulation according to methylation patterns and transcription factor binding-sites activity.

\subsubsection{Whole genome and exome-seq}

\emph{Whole genome sequencing} (WGS, DNA-reseq) allows discovery of genetic variations across the whole genome.
These may be in the form of single nucleotide variants (SNVs), small insertions or deletions (indels), or large structural variants such as transversions, trans-locations, and copy number variants (CNVs).
\emph{Whole exome sequencing} (WES, exome-seq) is a cost-effective yet powerful alternative to WGS.
This protocol consists in the \emph{targeted sequencing} of the \emph{exome}, \ie the protein coding subset of a genome.
%WES applications are oriented towards clinical settings, \eg towards mendelian disorders and cancer diagnostic.

\subsubsection{RNA-seq}
\emph{RNA sequencing} (RNA-seq) is a protocol to sequence the \emph{transcriptome}, \ie the set of RNA molecules of an organism, including mRNAs, rRNAs, tRNAs and other non-coding RNAs.
Actual HTS technologies perform deep-sequencing of RNA molecules after they have been reverse-transcribed into cDNA.
RNA-seq provides a myriad of information on gene expression, alternative splicing, intron/exon boundaries, untranslated regions (UTRs), and genetic variation with single-base accuracy.

\subsubsection{ChIP-seq}
\emph{Chromatin immunoprecipitation} with next-generation sequencing (ChIP-seq) is a protocol that allows the selective sequencing of genomic DNA bound by a specific protein.
The process isolates chromatin-protein complexes and sheares them by sonication.
Subsequently, immunoprecipitation with a protein-specific antibody pulls down genomic DNA sequences bound by a specific protein.
These DNA sequences are finally sequenced by HTS.
ChIP-seq is used to study the mechanisms of gene regulation.
Through this sequencing protocol it is possible to determine global methylation patterns, identify transcription factor binding sites, histone modifications, chromatin remodelling proteins.

\subsection{Analysis pipelines}

The analysis of HTS data consists of numerous steps, ranging from the initial instrument specific data processing to the final application specific interpretation of the results.
It is conventional wisdom to subdivide such data analysis pipelines in three stages of \emph{primary}, \emph{secondary}, and \emph{tertiary} analysis.
The primary stage of analysis consists of instrument specific processes for generation, collection, and processing of raw sequencing data.
The secondary stage reconstructs the original sequence of the donor genome by applying sequence analysis methods to the raw sequencing data.
The tertiary stage characterizes features of the donor genome specific to the sequencing application \eg genetic variations in exome-seq, then provides interpretations \eg by aggregating multiple samples and performing data mining.

%The typical SNPs analysis pipeline~\ref{fig:ngs-pipeline} consists of...
%\begin{figure}[h]
%\caption{NGS pipeline.}
%\label{fig:ngs-pipeline}
%\end{figure}

\subsubsection{Primary analysis}

Primary analysis consists of instrument specific steps to call base pairs and compute quality metrics.
The base caller processes instrumental signals and assigns a base to each peak, \ie A, C, G, T, or N.
The software assigns a quality value to each called base, estimating the probability of a base calling error.
On early generation instruments, users could provide their own base calling tool.
Now this process happens automatically on special hardware (\eg FPGAs or GPUs) bundled within the instrument.
The result of primary analysis is a standard \emph{FASTQ} file containing DNA read sequences and their associated quality scores in \emph{Phred} format (see section \ref{sec:background:hts:phred}).

\subsubsection{Secondary analysis}

Secondary analysis aims at reconstructing the original sequence of the donor genome.
There are two main \emph{plans} to reassemble the original genome: \begin{inparaenum}[(A)]
\item \emph{de novo} assembly, and
\item \emph{reference-guided} assembly (commonly called \emph{read mapping}).
\end{inparaenum}
\emph{De novo} assembly is very involved as it essentially requires finding a \emph{shortest common superstring} (SCS) of the reads, which is a NP-complete problem \citep{Maier1977,Gallant1980,Turner1989}.
Computational methods in plan A first scaffold the reads by performing \emph{overlap alignments} \citep{Myers2005} or equivalently by constructing \emph{de Bruijn graphs} \citep{Pevzner2001}.
The knowledge of a \emph{reference genome}, highly similar to the donor, simplifies the problem and opens the door to plan B.
The reads are simply aligned to the reference, tolerating a few base pair errors.
It is worth mentioning that some combinations of plans A and B have been proposed \citep{Li2012}.

Plan B is always preferred in resequencing projects, as it is more computationally viable than plan A and directly provides a way to assess genetic variation \wrt a reference genome.
In this manuscript, I follow only plan B and present methods that work within this specific plan.
Nonetheless, many of the algorithmic components that I expose in the first part of this manuscript are ubiquitous in bioinformatics, thus applicable, if not already applied, to plan A.
According to plan B, the secondary analysis step consists of three tasks: \emph{quality control}, \emph{read mapping}, and \emph{pile up}.

\emph{Quality control} (Q/C) checks the quality of raw reads.
Reads produced by current HTS technologies contain sequencing artefacts, in form of single miscalled bases or stretches of oligonucleotides.
In order to circumvent this problem, various techniques have been developed, ranging from the simple \emph{trimming} of low quality read stretches to sophisticated methods of \emph{read error correction} \citep{Weese2013a}.
Sometimes, Q/C is simply omitted.

\emph{Read mapping} takes care of aligning the reads to the reference genome.
Read mappers adopt state of the art \emph{approximate string matching} methods to efficiently analyze the deluge of data produced by HTS instruments.
Approximate matching accounts for two kinds of errors:
residual sequencing artefacts not removed by Q/C, and small genetic variations in the donor genome to which reads belong.
Consequently, a read mapper must take into account errors when mapping reads.

Mapped reads, often stored in de-facto standard BAM files \citep{Li2009a}, are \emph{piled up} in order to construct a \emph{consensus sequence} of the donor genome.
The height of the pileup denotes the sequencing depth at each locus in the donor genome, thus the average height corresponds to the average sequencing coverage;
higher depth implies more confidence in the consensus sequence of the donor genome and thus more accuracy in tertiary analysis.

\subsubsection{Tertiary analysis}

Tertiary analysis aims at interpreting information provided by secondary analysis.
This pipeline stage groups a wide range of analyses specific to the sequencing application.
In some pipelines, downstream analysis performs data mining over aggregated data coming from multiple samples.

Within DNA resequencing, \emph{genotyping} consists of determining the variations between the donor and the reference genome.
The result of \emph{variant calling} is usually a VFC file annotating the variations of the donor genome.
Subsequently, \emph{genome-wide association studies} (GWAS) associate genetic variants with phenotypic traits by examining \emph{single-nucleotide polymorphisms} (SNPs), variants relatively common among individuals of the same population.

In RNA-seq, tertiary analysis consists of computing transcripts abundance by measuring \emph{reads per kilobase per million mapped reads} (RPKM) \citep{Mortazavi2008}.
Subsequently, relative gene expression is determined by comparing multiple RNA samples.
In ChIP-seq, this stage consists of calling peaks in correspondence of mapped reads, determining which peaks identify feasible transcription factor binding sites, then interesting sites affecting gene regulation.

% -----------------------------------------------------------------------------

\section{Outline}

This thesis presents novel methods for the \emph{efficient} and \emph{accurate} mapping of high-throughput sequencing DNA reads, based on state of the art \emph{approximate string matching} algorithms and data structures.
Read mapping is a non-trivial, ubiquitous task in all resequencing applications.
Efficiency is mandatory to keep the pace of sequencing technologies, exponentially increasing in throughput.
Accuracy is required to enable downstream data analysis at single-base resolution.
The ingenuity of state of the art approximate string matching methods is crucial for the design and implementation of efficient and accurate read mapping programs.

The contributions of this work are of purely practical interest, nonetheless I follow a practical yet rigorous approach.
I subdivide this work in two parts: the former part exposes \emph{practical} approximate string matching methods, while the latter one describes the engineering and evaluation of two read mapping programs developed by myself.

\subsection{Approximate string matching}

In chapter 2, I introduce basic stringology concepts.
I give an overview of the basic online, indexed and filtering methods for exact and approximate string matching.
In particular, in section \ref{sub:introindex}, I introduce the concept of full-text index and define a set of \emph{generic} top-down traversal operations.

In chapter 3, I cover \emph{indexed methods} for exact and approximate string matching.
First, I describe some classic full-text indices (suffix arrays and $q$-gram indices) and succinct full-text indices (uncompressed FM-index variants).
Afterwards, I expose generic string matching algorithms, working on any of these data structures, and provide their experimental evaluation.
To the best of my knowledge, this is the first work providing a comprehensive exposition of these methods together with their experimental evaluation.
In addition, my implementation of all these algorithms and data structures is publicly available in source form within the \CC library SeqAn \citep{Doering2008}.

In chapter 4, I cover \emph{filtering methods} for approximate string matching.
I consider two classes of full-sensitive filtering methods: those based on \emph{seeds} and those based on \emph{$q$-grams}.
From the former class, I cover:
\emph{exact seeds} \citep{Baeza1992},
\emph{approximate seeds} \citep{Myers1994,Navarro2000},
\emph{suffix filters} \citep{Kaerkkaeinen2007}.
From the latter one:
\emph{contiguous $q$-grams} \citep{Jokinen1991},
\emph{gapped $q$-grams} \citep{Burkhardt2001},
\emph{multiple gapped $q$-grams} (also called \emph{$q$-gram families}) \citep{Kucherov2005}.
Again, to the best of my knowledge, this is the first work providing a comprehensive exposition of these methods together with their experimental evaluation.
In addition, I introduce a formal framework for (multiple) gapped $q$-grams, which leads to the formulation of APX and FPRAS (fully polynomial-time randomized approximation scheme) algorithms  answering some combinatorially hard filter design questions.

\subsection{Read mapping}

In chapter 5, I finally return to read mapping. Before diving into the plethora of tools proposed during the last years, I give a quick overview of market-leading HTS technologies and expose the \emph{de facto} standard analysis paradigms adopted to analyze the deluge of data produced by such instruments.
By reviewing some recent works \citep{Derrien2012,Lee2012}, I try to delineate which are the limits of these HTS technologies and data analysis paradigms.
Finally, I give an overview of the most popular read mappers.
In the last two chapters, I consider these popular programs in the evaluation of my own tools, Masai \citep{Siragusa2013} and Yara \citep{Siragusa2014}.

In chapter 6, I present my first attempt at engineering a read mapping tool.
My method is packaged in a \CC tool nicknamed \emph{Masai}, which stands for \emph{m}ultiple backtracking of \emph{a}pproximate seeds on a \emph{s}uffix \emph{a}rray \emph{i}ndex.
Masai is part of the SeqAn library \citep{Doering2008}, it is distributed under the BSD license and can be downloaded from \url{http://www.seqan.de/projects/masai}.
The result of this work has been published \citep{Siragusa2013} in the peer-reviewed journal \emph{Nucleic Acids Research}.

In chapter 7, I present \emph{Yara}, a state of the art read mapping tool.
Yara is capable of quickly reporting all stratified mapping locations within a given error rate.
The tool works with Illumina or Ion Torrent reads, supports paired-end and mate-pair protocols, computes accurate mapping qualities, offers parallelization via multi-threading, has a low memory footprint thanks to the FM-index, and does not require ad-hoc parameterization.
%The result of this work has been published \citep{Siragusa2014} in ??.
Yara is the foremost contribution of this work.


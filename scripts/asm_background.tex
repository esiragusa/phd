
\chapter{Preliminaries}

In this chapter, I introduce fundamental definitions and problems of stringology.
%At the end of the manuscript, I provide a summary of notations.
The reader familiar with basic stringology can skip this chapter and proceed to chapter~\ref{chr:index}.

\section{Definitions}

I start with fundamental objects of stringology: alphabets and strings.
An \emph{alphabet} $\Sigma= \{ a_1, a_2, \dots , a_{\sigma} \}$ is a finite ordered set of symbols (or characters) $a_i$.
A \emph{string} (or word) $s=s_1 s_2 \dots s_n$ over an alphabet $\Sigma$ is a finite sequence of symbols $s_i \in \Sigma$.
I denote by $s_{i \dots j}$ the \emph{concatenated} symbols $s_i \dots s_j$, and by $|s|=n$ the length of $s$.
%I often call \emph{$q$-gram} a string of length $q$.
%I denote by $\epsilon$ the \emph{empty string}, \st $|\epsilon|=0$.
Given an alphabet $\Sigma$, the set $\Sigma^0=\{ \epsilon \}$ contains only the \emph{empty string} \st $|\epsilon|=0$, $\Sigma^n$ contains all strings of length $n$ over $\Sigma$, and $\Sigma^* = \cup_{n=0}^{\infty}{\Sigma^n}$ all strings over $\Sigma$.

The definitions of prefix, suffix and substring are immediately derived from concatenation.
A string $x$ is a \emph{prefix} of another string $y$ iff $x = y_{1 \dots i}$ for some $1 \leq i \leq |y|$, a \emph{suffix} iff $x = y_{i \dots n}$ for some $1 \leq i \leq |y|$, 
and a \emph{substring} iff $x = y_{i \dots j}$ for some $1 \leq i \leq j \leq |y|$.
For convenience, often I denote the suffix of $y$ beginning at position $i$ simply by $y_{i \dots}$.

The definition of \emph{occurrence} is fundamental in string matching.
A string $x$ occurs in a string $y$ iff $x$ is a substring of $y$.
Given $x = y_{i \dots j}$, the occurrence of $x$ starts at position $i$ and ends at position $j$ in $y$.
Furthermore, definition \ref{def:delta} is mandatory for approximate string matching problems. 
\begin{definition}
\label{def:delta}
Given an alphabet $\Sigma$, I define the \emph{distance function} $\delta : \Sigma \times \Sigma \rightarrow \{0, 1\}$ \st $\delta(a,b) = 1$ for any two distinct $a,b \in \Sigma$ and 0 otherwise.
\end{definition}

Lexicographical ordering is fundamental in respect to \emph{full-text indexing}.
\begin{definition}
\label{def:rho}
Given an alphabet $\Sigma$ of size $\sigma$, I denote the \emph{lexicographic rank} of any alphabet symbol by the function $\rho : \Sigma \rightarrow [1 \dots \sigma]$, \st $\rho(a) < \rho(b) \iff a < b$ for any distinct $a,b \in \Sigma$.
\end{definition}
The \emph{lexicographical order} $<_{lex}$ between two non-empty strings $x,y$ is defined as $x <_{lex} y \iff x_1 < y_1, \text{ or } x_1 = y_1 \text{ and } x_{2 \dots} <_{lex} y_{2 \dots}$.

\emph{Generalized} full-text indices work on string collections.
A \emph{string collection} is an ordered multiset $\Strings = \{ s^1, s^2, \dots, s^c \}$ of non necessarily distinct strings over a common alphabet $\Sigma$.
I denote by $\norm{\Strings}=\sum_{i=1}^{c}{|s^i|}$ the total length of the string collection.
I extend the above definitions of prefix, suffix and substring also to multisets, \eg $\Strings_{(d,i) \dots (d,j)}$ denotes the substring $s^d_{i \dots j}$.

To present full-text indices, I use the definitions of padded string (\ref{def:strd}) and padded string collection (\ref{def:coltd}).
I first introduce special terminator symbols to perform padding.
I call \emph{terminator} a symbol $\$ \not \in \Sigma$ \st $\rho(\$) < \rho(a)$ for any $a \in \Sigma$.
\begin{definition}
\label{def:strd}
Given a string $s$ over $\Sigma$, I call \emph{padded string} the concatenation of $s$ with a terminator symbol \$.
\end{definition}
\begin{definition}
\label{def:coltd}
Given a string collection $\Strings$ over $\Sigma$, I call \emph{padded string collection} the collection consisting of strings $s^i \in \Strings$ padded with terminator symbols $\$^i$ \st $\rho(\$^i) < \rho(\$^j) \iff i < j$.
\end{definition}

In this manuscript, in addition to strings, I use numerical arrays and matrices.
An \emph{array} (or table, or vector) $V=v_1 v_2 \dots v_n$ is a finite sequence of numbers $v_i \in \N_0$ (or any subset of $\N_0$).
I denote by $V[i]$ the subscript element $v_i$.
A \emph{matrix} $X$, is a rectangular array of numbers $x_{i,j} \in \N_0$ (or any subset of $\N_0$), arranged \st
$$X=
\begin{pmatrix}
  x_{1,1} & x_{1,2} & \cdots & x_{1,n} \\
  x_{2,1} & x_{2,2} & \cdots & x_{2,n} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  x_{m,1} & x_{m,2} & \cdots & x_{m,n}
\end{pmatrix}$$
I denote by $X[i,j]$ the subscript element $x_{i,j}$.

\section{Transcripts, alignments and distances}

I now define the basic edit operations to transform one string into another.
Given two strings $x,y$ of equal length $n$, the string $x$ is easily transformed into the string $y$ by substituting (or replacing) all symbols $x_i$ \st $x_i \neq y_i$ into $y_i$, for $1 \leq i \leq n$.
However, if the two strings have different lengths, some symbols from $x$ must be necessarily inserted or deleted in order to obtain $y$.
This fact motivates the following definition of \emph{edit transcript}.

\begin{definition}
\label{def:transcript}
\citep{Gusfield1997} An edit transcript for any two given strings $x,y$ is a finite sequence of substitutions, insertions and deletions transforming $x$ into $y$.
\end{definition}

An \emph{alignment} is an alternative way of visualizing a transformation between strings.
While an edit transcript is an explicit sequence of edit operations that transform one string into another, an alignment is an explicit relationship between pairs of symbols from the two strings.
Nonetheless, when some symbols are inserted or removed, some symbols in one string are not related to any symbol in the other string.
For this reason, it is necessary to introduce an additional gap symbol $-$, not being part of the string alphabet $\Sigma$.
The definition of alignment follows.
Figure~\ref{fig:edit-transcript} shows an example of edit transcript with its associated alignment.

\begin{definition}
\label{def:alignment}
An alignment of two strings of length $m,n$ over $\Sigma$ is a string of length between $\min\{m,n\}$ and $m+n$ over the pair alphabet $(\Sigma \cup \{ - \}) \times (\Sigma \cup \{ - \}) \setminus \{ (-,-) \} $.
\end{definition}

\begin{figure}[h]
\begin{center}
\caption[Example of edit transcript and alignment]{Example of edit transcript and alignment. The string $x$ is transformed into $y$. The transcript character M indicates a match, R a replacement, I an insertion, and D a deletion.
%The alignment of the strings $x=...$ and $y=...$ is given by the string $z={\text{A} \choose \text{A}}{\text{A} \choose \text{-}}{\text{A} \choose \text{C}}{\text{G} \choose \text{G}}$
}
\label{fig:edit-transcript}
\input{figures/transcript.tikz}
\end{center}
\end{figure}

%A \emph{dotplot} is yet another way to visualize any alignment between two strings and highlight their similarities.
%\begin{definition}
%\label{def:dotplot}
%Given two string $x,y$ of length $m,n$, a dotplot is a $m \times n$ matrix containing a dot at position $(i,j)$ iff the symbol $x_i$ matches symbol $y_j$.
%\end{definition}
%A dotplot trace is a monotonical path in the matrix connecting non-decreasing positions of the matrix.
%Any dotplot trace corresponds to an alignment and vice versa.
%In a trace, match and mismatch columns of the corresponding alignment appear as diagonal stretches, while insertions and deletions are horizontal or vertical stretches.
%See figure~\ref{fig:dotplot}.
%
%\begin{figure}[h]
%\begin{center}
%\caption[Example of dotplot]{Example of dotplot of the strings $x=AAAA$ and $y=CCCC$. The highlighted trace corresponds to the transcript and alignment of figure~\ref{fig:edit-transcript}.}
%\label{fig:dotplot}
%\input{figures/dotplot.tikz}
%\end{center}
%\end{figure}

%\section{Distance functions}

%A cost is assigned to any alignment, and to its associated edit transformation, by defining a weight function $\delta : (\Sigma \cup \{ - \}) \times (\Sigma \cup \{ - \}) \rightarrow \R_0^{+}$, where:
%\begin{itemize}
%\item $\delta(\alpha,\beta)$ for all $(\alpha,\beta) \in \Sigma \times \Sigma$ defines the cost of substituting $\alpha$ with $\beta$,
%\item $\delta(\alpha,-)$ for all $\alpha \in \Sigma$ defines the cost of deleting the symbol $\alpha$,
%\item $\delta(-,\beta)$ for all $\beta \in \Sigma$ defines the cost of inserting the symbol $\beta$,
%\end{itemize}
%
%The total cost $C(z)$ of an alignment $z$ between two strings is the sum of the weights of all its alignment symbols:
%\begin{eqnarray}
%C(z) = \sum_{i=0}^{|z|}{\delta(z_i)}
%\end{eqnarray}
%Consequently, any distance function $d : \Sigma^{*} \times \Sigma^{*} \rightarrow \R_0^{+}$ takes the minimum cost over all possible alignments of $x,y$:
%\begin{eqnarray}
%d(x,y) = \min_{z \in \mathbb{A}(x,y)}{C(z)}
%\end{eqnarray}

At this point, I give the definition of two fundamental distance functions between strings.

\begin{definition}
\label{def:hamming}
\citep{Hamming1950} The \emph{Hamming distance} $d_H : \Sigma^{n} \times \Sigma^{n} \rightarrow \N_0$ between two strings $x,y \in \Sigma^{n}$ counts the number of substitutions necessary to transform $x$ into $y$.
\end{definition}
%I denote the Hamming distance by the function $d_H : \Sigma^{n} \times \Sigma^{n} \rightarrow \N_0$.
%The function $d_H$ is obtained by defining for all $(\alpha,\beta) \in \Sigma \times \Sigma$, $\delta(\alpha,\beta) = 1$ iff $\alpha \neq \beta$ and 0 otherwise, and $\delta(-,\beta)$ to be $+ \infty$ in order to disallow insertions and deletions.

\begin{definition}
\label{def:edit}
\citep{Levenshtein1966} The \emph{Levenshtein} or \emph{edit distance} $d_E : \Sigma^{*} \times \Sigma^{*} \rightarrow \N_0$ between two strings $x,y \in \Sigma^{*}$ counts the \emph{minimum} number of edit operations necessary to transform $x$ into $y$.
\end{definition}
%I denote the edit distance by the function $d_E : \Sigma^{*} \times \Sigma^{*} \rightarrow \N_0$.
%The function $d_E$ is obtained by defining $\delta(\alpha,\beta)$ as for the Hamming distance, and $\delta(\alpha,-) = \delta(-,\beta) = 1$.

The problem of finding an optimal alignment between two strings is equivalent to the problem of finding their minimum distance \citep{Gusfield1997}.
While the Hamming distance between any two strings of length $n$ is easily computed in time $\Oh(n)$, computing the edit distance involves solving a non-trivial optimization problem.

\section{Edit distance computation}

The edit distance between two strings is efficiently computed via \emph{dynamic programming} (DP) \citep{Needleman1970}.
%Below, I describe the three essential components of the DP approach: the recurrence relation, the DP table, and the traceback.
Let $x,y$ be two strings of length $n \geq m$.
The edit distance $d_E(x_{1 \dots i},y_{1 \dots j})$ between any their prefixes $x_{1 \dots i}$ and $y_{1 \dots j}$ is defined recursively.
%for all $1 \leq i \leq m$ and $1 \leq j \leq n$,
The base conditions of the recurrence relation are:
\begin{eqnarray}
d_E(\epsilon,\epsilon)&=&0\label{eq:dp-ee}\\
d_E(x_{1 \dots i},\epsilon)&=&i \; \text{ for all } 1 \leq i \leq n\label{eq:dp-row}\\
d_E(\epsilon, y_{1 \dots j})&=&j \; \text{ for all } 1 \leq j \leq m\label{eq:dp-col}
\end{eqnarray}
and the recursive case for all $1 < i \leq n$ and $1 < j \leq m$ is as follows:
\begin{eqnarray}
d_E(x_{1\dots i},y_{1 \dots j}) = \min \left\{
\begin{array}{lcl}
d_E(x_{1 \dots i-1},y_{1 \dots j})&+&1\\
d_E(x_{1 \dots i},y_{1 \dots j-1})&+&1\\
d_E(x_{1 \dots i-1},y_{1 \dots j-1})&+&\delta(x_i, y_j)
\end{array}
\right.\label{eq:dp-min}
\end{eqnarray}
where the function $\delta$ indicates whether the characters $x_i$, $x_j$ match or mismatch (see definition \ref{def:delta}).

\begin{figure*}[t]
\begin{center}
\begin{minipage}[t]{.9\textwidth}
\begin{algorithm}[H]
\Algorithm{EditDistance}{$x,y$}
\begin{tabular}{ll}
\textbf{Input}  & $x$ : string of length $n$\\
				& $y$ : string of length $m$\\
\textbf{Output} & the edit distance $d_E(x,y)$ between $x$ and $y$\\
\end{tabular}
\begin{algorithmic}[1]
\State {$D[0,0] \gets 0$}
\For {$j \gets 1 \, \To m$}
	\State {$D[j,0] \gets D[j-1,0] + 1$}
\EndFor
\For {$i \gets 1 \, \To n$}
	\State {$D[0,i] \gets D[0,i-1] + 1$}
	\For {$j \gets 1 \, \To m$}
		\State {$D[i,j] \gets \min \, \{ \, D[i-1,j] + 1, D[i,j-1] + 1, D[i-1,j-1] + \delta(t_i, p_j) \, \}$}
	\EndFor
\EndFor
\State \Return D[m,n]
\end{algorithmic}
\label{alg:dp-edit}
\end{algorithm}
\end{minipage}
\end{center}
\end{figure*}

Algorithm \ref{alg:dp-edit} computes the above recurrence relation in time $\Oh(nm)$ using a dynamic programming table $D$ of $(n+1) \times (m+1)$ cells, where cell $D[i,j]$ stores the value of $d_E(x_{1 \dots i},y_{1 \dots j})$.
The sole edit distance without any alignment can be computed in space $\Oh(m)$; indeed, only column $j-1$ is required to compute column $j$.
An optimal alignment can be computed in time $\Oh(m + n)$ via \emph{traceback} on the table $D$;
the traceback starts in the cell $D[m,n]$ and goes backwards (either left, up-left, or up) to the previous cell by deciding which condition of equation~\ref{eq:dp-min} yielded the value of $D[m,n]$.

If no alignment at all is needed, there is a more efficient solution to the edit distance problem.
Myers' algorithm \citep{Myers1999}, instead of computing DP cells one after another, encodes one whole DP column in two bit-vectors and computes the adjacent column in a constant number of 12 logical and 3 arithmetical operations.
The time complexity of Myers' algorithm is $\Oh(nm/b)$, where $b$ is the processor's bit-width, \eg 64 on modern processors.

% === Overview of existing methods ===

\section{String matching}

\emph{Exact string matching} is one of the most fundamental problems in stringology.
\begin{definition}
\citep{Gusfield1997}
Given a string $p$ of length $m$, called the \emph{pattern}, and a string $t$ of length $n$, called the \emph{text}, the exact string matching problem is to find all occurrences of $p$ into $t$.
\end{definition}

This problem has been extensively studied from the theoretical standpoint and is well solved in practice \citep{Faro2013}.
Nonetheless, the definition of distance functions between strings lends to a more challenging problem: \emph{approximate string matching}.

\begin{definition}
\citep{Galil1988}
Given a text $t$, a pattern $p$, and a \emph{distance threshold} $k \in \N$, the approximate string matching problem is to find all occurrences of $p$ into $t$ within distance $k$.
\end{definition}
The approximate string matching problem under the Hamming distance is commonly called the \emph{$k$-mismatches} problem, while under the edit distance is called the \emph{$k$-differences} problem.
Figure \ref{fig:edit-occurrence} shows an example of occurrence for $k$-differences.
For $k$-mismatches and $k$-differences, it must hold $k > 0$ as the case $k = 0$ corresponds to exact string matching, and $k < m$ as a pattern trivially occurs at any position in the text if all its $m$ characters are substituted.
Frequently, the problem's input respects the condition $k \ll m \ll n$.
\begin{definition}
Under the edit or Hamming distance, the \emph{error rate} is defined as $\epsilon = k/m$, with $0 < \epsilon < 1$ given the above conditions.
\end{definition}

\begin{figure}[t]
\begin{center}
\caption[Example of occurrence for $k$-differences]{Example of occurrence for $k$-differences. Pattern $p$ occurs in text $t$ at edit distance 3, \ie with a 19~\% error rate.
The alignment between $p$ and any substring of $t$ is called semi-global, as opposed to the global alignment of two complete strings.
}
\label{fig:edit-occurrence}
\input{figures/occurrence.tikz}
\end{center}
\end{figure}

String matching problems are subdivided in two categories, \emph{online} and \emph{offline}, depending on which string, the pattern or the text, is given first.
Algorithms for online string matching work by preprocessing the pattern and scanning the text from left to right (or right to left).
Algorithms for offline string matching are instead allowed to preprocess the text,
hence they build an index of the text beforehand to speed up subsequent searches.
In practice, if the text is long, static and searched frequently, offline methods outperform online methods in terms of runtime, provided the necessary amount of memory for text indexing.

It goes without saying that offline string matching algorithms are tightly bound to text indexing data structures.
Almost all of these algorithms require a \emph{full-text index}, \ie a data structure representing all substrings of the text.
Very often, such full-text index is realized as the \emph{suffix tree} \citep{Weiner1973}, a fundamental data structure in stringology.
Among its virtues \citep{Apostolico1985}, the suffix tree natively provides exact string matching in optimal time and approximate string matching via backtracking \citep{Ukkonen1993}.
Often, the suffix tree finds its use within hybrid \emph{filtering methods} rather than on its own.

Filtering methods first discard uninteresting portions of the text and subsequently verify only narrower areas.
These methods work either online or offline.
Online filtering methods try to jump over the text while scanning it; instead, offline filtering methods use an index to place anchors in the text.
Both classes of filtering methods then require a native online method to verify the anchors.

Filtering methods outperform \emph{native} online and indexed methods for a vast range of inputs, \ie when the error rate is low, and are thus very appealing from a practical standpoint.
Nonetheless, filtering methods are just opportunistic combinations of native online and indexed methods.

In this manuscript, I often consider \emph{multiple} string matching problems, \ie variants in which many patterns are given at once, instead of \emph{single} problems where patterns are given one by one in an online fashion.
Obviously, any method for the single case can solve the multiple case and vice versa.
However, it is clear that methods for multiple string matching have advantages over methods for single string matching.
For instance, online methods for multiple string matching are allowed to preprocess all patterns and then scan the text only once, while online methods for single string matching have to scan the text every time a pattern is given.
Thus, in general, methods for multiple string matching are more appealing than methods for single string matching.

In the remainder of this section, I give a quick overview of the fundamental algorithms and data structures adopted by classic string matching methods.
This overview serves as an introduction to the indexed and filtering methods presented in the next two chapters.
For an extensive treatment of this subject, the reader is referred to complete surveys on online \citep{Navarro2001a} and indexed \citep{Navarro2001} approximate string matching methods.

% --- Online methods ---

\subsection{Online methods}
\label{sub:introonline}

%\subsubsection{Dynamic programming}

The DP algorithm~\ref{alg:dp-edit} to compute the edit distance between two strings is easily turned into an online $k$-differences algorithm.
Since an approximate occurrence of the pattern can start (and end) anywhere in the text, the problem involves computing the edit distance between the pattern and \emph{any substring} of the text.
Algorithm ~\ref{alg:dp-kdiff} efficiently solves this problem by computing the edit distance between the text and the pattern without penalizing leading and trailing deletions in the text.

\begin{figure*}[b]
\begin{center}
\begin{minipage}[t]{.9\textwidth}
\begin{algorithm}[H]
\Algorithm{KDifferences}{$t,p,k$}
\begin{tabular}{ll}
\textbf{Input}  & $t$ : text string of length $n$\\
				& $p$ : pattern string of length $m$\\
				& $k$ : integer bounding the number of errors\\
\textbf{Output} & all end positions of $k$-differences occurrences of $p$ in $t$\\
\end{tabular}
\begin{algorithmic}[1]
\State {$D[0,0] \gets 0$}
\For {$j \gets 1 \, \To m$}
	\State {$D[j,0] \gets j$}
\EndFor
\For {$i \gets 1 \, \To n$}
	\State {$D[0,i] \gets 0$}
	\For {$j \gets 1 \, \To m$}
		\State {$D[i,j] \gets \min \, \{ \, D[i-1,j] + 1, D[i,j-1] + 1, D[i-1,j-1] + \delta(t_i, p_j) \, \}$}
	\EndFor
	\If {$D[i,m] \leq k$}
		\State \Report $i$
	\EndIf
\EndFor
\end{algorithmic}
\label{alg:dp-kdiff}
\end{algorithm}
\end{minipage}
\end{center}
\end{figure*}

Consider the recurrence relation described by equations~\ref{eq:dp-ee}--\ref{eq:dp-min} and pose $x=t$ and $y=p$.
Because an occurrence of the pattern can start anywhere in the text, the base condition \ref{eq:dp-row} of the edit distance recurrence relation becomes:
\begin{eqnarray}
d(x_{1 \dots i}, \epsilon) = 0 \; \text{ for all } 1 \leq i \leq n\label{eq:dp-row-kdiff}
\end{eqnarray}
and algorithm~\ref{alg:dp-kdiff} initializes the top row of the DP matrix $D$ accordingly.
Then, as an occurrence of the pattern can end anywhere in the text, algorithm~\ref{alg:dp-kdiff} checks any cell in the bottom row for the condition $D[i,m] \leq k$.

%\begin{figure}[h]
%\begin{center}
%\caption[Example of approximate string matching via DP]{DP table representing the match of $p=...$ in $t=...$.}
%\label{fig:asm-dp}
%\input{figures/dp_asm.tikz}
%\end{center}
%\end{figure}

%\subsubsection{Myers' algorithm}

%DP bit-parallelism.
%Other algorithms for approximate string matching are based on automata \citep{Navarro2001b}.

% --- Indexed methods ---

\subsection{Indexed methods}
\label{sub:introindex}

%Indexed methods are more attractive than online methods when several patterns are to be searched in the same static text.
%These methods work by building an index of the text beforehand to speed up subsequent searches.
%To this intent,
I now introduce \emph{suffix tries}, idealized data structures to index full texts.
I define a set of generic operations to traverse them in a top-down fashion.
In chapter~\ref{chr:index}, I introduce various data structures replacing suffix tries in practical implementations.
Thanks to these generic traversal operations, I later formulate generic string matching algorithms that are independent of the practical suffix trie implementations.

\subsubsection{Trie}

Consider a padded string collection $\Strings$ (definition~\ref{def:coltd}) consisting of $c$ strings.
Note that padding is necessary to ensure that no string $s^i \in \Strings$ is a prefix of another string $s^j \in \Strings$.
\begin{definition}
The \emph{trie} $\Si$ of $\Strings$ is a lexicographically ordered tree data structure having one node designated as the root and $c$ leaves, denoted as $\Ln_1 \dots \Ln_c$, where leaf $\Ln_i$ points to string $s^i$.
Any edge entering a node of $\Si$ is labeled with a symbol in $\Sigma$, while any edge entering a leaf of $\Si$ is labeled with a terminator symbol.
Any path from the root to a leaf $\Ln_i$ spells the string $s^i$, including its terminator symbol $\$^i$.
%Figure~\ref{fig:trie} illustrates.
\end{definition}

\subsubsection{Suffix trie}

\begin{definition}
Given a padded string $s$ (definition~\ref{def:strd}) of length $n$, the suffix trie $\Si$ is the trie of all suffixes of $s$.
The \emph{suffix trie} $\Si$ has $n$ leaves, where leaf $\Ln_i$ points to suffix $s_{i..n}$.
\end{definition}
%The suffix trie generalizes to index string collections.
\begin{definition}
Given a padded string collection $\Strings$ (definition~\ref{def:coltd}), any leaf of the \emph{generalized suffix trie} is labeled with a pair $(i,j)$ where $i$ points to the string $s^i \in \Strings$ and $1 \leq j \leq n_i$ points to one of the $n_i$ suffixes of $s^i$.
Thus any path from the root to a leaf $(i,j)$ spells the suffix $\Strings_{(i,j) \dots}$.
\end{definition}
%Figure~\ref{fig:stree} illustrates.

\begin{figure}[b]
\caption[Example of suffix trie and suffix tree]{Suffix trie and suffix tree of the string {\ttfamily ANANAS\$} .}
\label{fig:stree}
\begin{subfigure}{.5\textwidth}
\begin{center}
\input{figures/strie_banana.tikz}
\end{center}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\begin{center}
\input{figures/stree_banana.tikz}
\end{center}
\end{subfigure}
\end{figure}

Note that the (generalized) suffix trie here defined contains $\Oh(n^2)$ nodes, yet I only consider the suffix trie as an idealized data structure to formulate generic algorithms.
The optimal data structure to represent in linear space all suffixes of a given string is the \emph{suffix tree} \citep{Morrison1968}.
However, the suffix tree comes with the restriction that internal nodes are branching, \ie they must have more than one child, and with the property that edges can labeled by strings of arbitrary length (see figure~\ref{fig:stree}).
This latter property slightly complicates the exposition of string matching algorithms but it does not affect their runtime complexity nor their result.
For this reason, in the remainder of this manuscript, I consider \wlogs always suffix tries instead of suffix trees.
%I remark that all given algorithms can be generalized to work on trees.

\begin{figure}[t]
\caption[Example of generalized suffix trie]{Generalized suffix trie of the string collection $\Strings = \{$ {\ttfamily ANANAS$\$_1$}, {\ttfamily CACAO$\$_2$} $\}$.}
\label{fig:gstrie}
\begin{center}
\input{figures/gstrie.tikz}
\end{center}
\end{figure}

\subsubsection{Top-down traversal}

I define a set of generic operations to traverse tries in a top-down fashion.
Note that the following traversal operations do not walk over edges labeled terminator symbols, as these symbols are not necessary in string matching applications.
Given a trie $\Ti$, I define the following operations inspecting any pointed node $\Tn$ of $\Ti$:
\begin{itemize}
\item \textsc{isRoot}($\Tn$) returns true iff the pointed node is the root;
\item \textsc{isLeaf}($\Tn$) returns true iff all outgoing edges are labeled by terminator symbols;
\item \textsc{label}($\Tn$) returns the symbol labeling the edge entering $\Tn$;
\item \textsc{occurrences}($\Tn$) returns the list of positions pointed by leaves below $\Tn$;
\end{itemize}
Moreover, I define the following operations moving from pointed node $\Tn$, and returning true on success and false otherwise:
\begin{itemize}
\item \textsc{goDown}($\Tn$) moves to the lexicographically smallest child of $\Tn$;
\item \textsc{goDown}($\Tn, c$) moves to the child of $\Tn$ whose entering edge is labeled by $c$;
\item \textsc{goRight}($\Tn$) moves to the lexicographically next child of $\Tn$;
\item \textsc{goUp}($\Tn$) moves to the parent node of $\Tn$.
\end{itemize}

Time complexities of the above operations depend on the data structure implementing the trie.
Usually \textsc{label} is $\Oh(1)$, both variants of \textsc{goDown} and \textsc{goRight} can be $\Oh(1)$ or logarithmic in $n$, \textsc{occurrences} can be linear in the number of occurrences, \textsc{goUp} is $\Oh(1)$ but with an additional $\Oh(n)$ space complexity to stack all parent nodes.
Algorithms \ref{alg:st-godown} and \ref{alg:st-goright} show how to implement respectively \textsc{goDown} and \textsc{goRight} using \textsc{goDown} a symbol and \textsc{goUp}, although with a worst-case time complexity of $\Oh(\sigma)$.
In chapter~\ref{chr:index}, I consider various data structures to implement suffix tries, I show how to implement these operations and give their complexities.

\begin{figure*}[t]
\begin{minipage}[t]{.5\textwidth}
\begin{algorithm}[H]
\Algorithm{goDown}{$\Tn$}
\begin{tabular}{ll}
\textbf{Input}  & $\Tn$ : pointer to a suffix trie node\\
\textbf{Output} & boolean indicating success\\
\end{tabular}
\begin{algorithmic}[1]
\If {\Call{isLeaf}{$\Tn$}}
	\State \Return \False
\EndIf
\If {\Call{goDown}{$\Tn, \min_{lex}{\Sigma}$}}
	\State \Return \True
\Else
	\State \Return $\Call{goRight}{$\Tn$}$		
\EndIf
\end{algorithmic}
\label{alg:st-godown}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{.5\textwidth}
\begin{algorithm}[H]
\Algorithm{goRight}{$\Tn$}
\begin{tabular}{ll}
\textbf{Input}  & $\Tn$ : pointer to a suffix trie node\\
\textbf{Output} & boolean indicating success\\
\end{tabular}
\begin{algorithmic}[1]
\If {\NotB \Call{isRoot}{$\Tn$}}
	\While {$c \gets \text{next}_{lex}{\Call{label}{\Tn}}$}
		\State {$\Call{goUp}{x}$}
		\If {$\Call{goDown}{x, c}$}
			\State \Return \True
		\EndIf
	\EndWhile
\EndIf
\State \Return \False $\phantom{()}$
\end{algorithmic}
\label{alg:st-goright}
\end{algorithm}
\end{minipage}
\end{figure*}


% --- Filtering methods ---

\subsection{Filtering methods}
\label{sec:intro:filtering}

Filtering methods work in two stages: the \emph{filtration stage} discards portions of the text unlikely or unable to contain an occurrence of the pattern; subsequently the \emph{verification stage} checks the remaining portions.
The filtration stage proceeds online by scanning the text or alternatively offline using an index of the text.
The verification stage uses a conventional method, \eg the online dynamic programming method or some variation of it.
The crux of filtering methods is thus to accurately and efficiently classify text portions as containing or not some occurrence of the pattern.

\subsubsection{Specificity and sensitivity}
\label{intro:filtering:spec-sens}

Any filtering method is thus a binary classification method.
Any text location is \emph{true} if it coincides with the beginning of an occurrence of the pattern and \emph{false} if it does not.
The outcome of the classification method is \emph{positive} for text locations filtered in and \emph{negative} for locations filtered out.
Therefore, as shown in table \ref{tab:filter:accuracy}, any text location belongs either to the set of \emph{true positives} (TP), \emph{false positives} (FP), \emph{true negatives} (TN), or \emph{false negatives} (FN).
Thus, standard \emph{specificity} and \emph{sensitivity} measure the accuracy of filtering methods.
The specificity of a filter $f$ on a text $t$ is defined as:
\begin{eqnarray}
\frac{|TN_f(t)|}{|TN_f(t)| + |FP_f(t)|}
\end{eqnarray}
and the sensitivity as:
\begin{eqnarray}
\frac{|TP_f(t)|}{|TP_f(t)| + |FN_f(t)|}
\end{eqnarray}

%An important case is its sensitivity is 1.
\begin{definition}
A filter is \emph{lossless} or \emph{full-sensitive} if its sensitivity is 1, \ie it produces no false negatives, otherwise it is \emph{lossy}.
\end{definition}
%A full-sensitive filter for approximate string matching solves the problem exactly.
Many practical applications, \eg read mapping, do not require strictly lossless filtration.
Nonetheless, a predictable or controlled lossy filter helps to interpret the results and insure their quality.
For this reason, I focus on criteria yielding filters which are lossless or lossy in a predictable fashion.

\begin{table}[b]
\begin{center}
\caption[Classification of text locations by filtering methods]{Classification of text locations by filtering methods.}
\begin{tabular}{ccc}
\toprule
Occurrence & Filtered in & Filtered out\\
\midrule
Yes & True positive (TP) & False negative (FN) \\
No & False positive (FP) & True negative (TN) \\
\bottomrule
\end{tabular}
\label{tab:filter:accuracy}
\end{center}
\end{table}

\subsubsection{Efficiency}

The total runtime of a filtering method is given by the sum of the runtimes of its filtration and verification stages.
Filtration specificity determines how much time is spent in the verification stage.
Clearly, to keep verification time small, the number of false positives must be low.
Nonetheless, the filtration stage must also run in a reasonable amount of time.
Hence, the runtime of an efficient filtering method is usually balanced between filtration and verification time.
In any way, no filtering method can be efficient when the number of true positive locations approaches the text length.
%No filtering method is effective unless $|TP_f(t)| \ll |t|$.

Filtering methods for string matching work under the assumption that patterns occur in the text with a \emph{low average probability}.
For $k$-differences, the occurrence probability is a function of the error rate $\epsilon$ and the alphabet size $\sigma$, and can be computed or estimated under the assumption of the text being generated by a specific random source.
Under the uniform Bernoulli model, where each symbol of $\Sigma$ occurs with probability $1/\sigma$, \cite{Navarro2001a} experimentally finds that $\epsilon < 1 - 1 / \sqrt{\sigma}$ is a tight upper bound on the error rate which ensures few occurrences and for which filtering algorithms are effective.
For higher error rates, non-filtering methods, either online or offline, work better.

On the one hand, 50 \% error rate marks the filtration efficiency bound for the DNA alphabet, while on the other hand HTS read mapping applications require error rates in a range from 3 to 10 \%.
According to the above considerations, filtering methods are expected to be very efficient in HTS applications.

\subsubsection{Seeds versus $q$-grams}

Filtering methods apply combinatorial criteria to determine which portions of the text might contain some occurrence of the pattern.
These criteria are in general valid for both online and offline variants of the problem.
In practice, one specific criterion might be more convenient for one variant of the problem rather than the other.
The combinatorial criterion underlying a filter is of paramount importance as it provides guarantees on filtration sensitivity.

In chapter \ref{sec:filter}, I consider two classes of combinatorial filtering methods: those based on \emph{seeds} and those based on \emph{$q$-grams}.
Filters in the former class partition the pattern into \emph{non-overlapping} factors called seeds;
application of the pigeonhole principle yields full-sensitive partitioning strategies.
Instead, filters in the latter class consider all \emph{overlapping} substrings of the pattern having length $q$, the so-called $q$-grams;
simple counting lemmata give lower bounds on the number of $q$-grams that must be present in a narrow window of the text, as necessary conditions for an approximate occurrence of the pattern.


\chapter{Preliminaries}

In this chapter, I introduce fundamental definitions and problems of stringology.
%At the end of the manuscript, I provide a summary of notations.
The reader familiar with basic stringology can skip this chapter and proceed to chapter~\ref{chr:index}.

\section{Definitions}

%I start with fundamental objects of stringology: alphabets and strings.
An \emph{alphabet} $\Sigma= \{ a_{\At{1}}, \dots, a_{\AtEnd{\sigma}} \}$ is a finite ordered set of characters (or symbols) $a_i$.
A \emph{string} (or word) $\String$ over $\Sigma$ is a finite sequence of symbols from $\Sigma$.
I denote with $\Subscript{\String}{i}$ the $i$-th symbol of $\String$ counting from $\Ibase$, with $\Substring{\String}{i}{j}$ the sequence of symbols $\Subscript{\String}{i} \ldots \Subscript{\String}{j}$ for any $\At{1} \leq i \leq j \LeqThan{\Slen}$, and with $|\String|=\Slen$ the length of $\String$.
The set $\Sigma^0=\{ \epsilon \}$ contains only the \emph{empty string} \st $|\epsilon|=0$, $\Sigma^\Slen$ contains all strings of length $\Slen$ over $\Sigma$, and $\Sigma^* = \cup_{n=0}^{\infty}{\Sigma^n}$ all strings over $\Sigma$.

A string $\XSeq$ is a \emph{prefix} of another string $\YSeq$ iff $\XSeq = \Prefix{\YSeq}{i}$ for some $\At{1} \leq i \LeqThan{|\YSeq|$}, a \emph{suffix} iff $\XSeq = \Substring{\YSeq}{\At{1}}{i}$ for some $\At{1} \leq i \LeqThan{|\YSeq|}$, 
and a \emph{substring} iff $\XSeq = \Substring{\YSeq}{i}{j}$ for some $\At{1} \leq i \leq j \LeqThan{|\YSeq|}$.
For convenience, I denote the suffix of $\YSeq$ beginning at position $i$ simply by $\Suffix{\YSeq}{i}$.
Moreover, I denote the concatenation of two strings $\XSeq$ and $\YSeq$ by $\XSeq \cdot \YSeq$ or simply $\XSeq \YSeq$.

%The definition of \emph{occurrence} is fundamental in string matching.
\begin{definition}[Occurrence]
\label{def:occurrence}
A string $\XSeq$ of length $\Xlen$ \emph{occurs} in a longer string $\YSeq$ iff $\XSeq$ is a substring of $\YSeq$.
Given $\XSeq = \Substring{\YSeq}{i}{i+\Xlen}$, the occurrence of $\XSeq$ \emph{starts} at position $i$ and \emph{ends} at position $i+\Xlen$ in $\YSeq$.
\end{definition}

%Furthermore, definition \ref{def:delta} is mandatory for approximate string matching problems. 
\begin{definition}
\label{def:delta}
Given an alphabet $\Sigma$, I define the \emph{distance function} $\delta : \Sigma \times \Sigma \rightarrow \{0, 1\}$ \st $\delta(a,b) = 1$ for any distinct $a,b \in \Sigma$ and 0 otherwise.
\end{definition}

%Lexicographical ordering is fundamental in respect to \emph{full-text indexing}.
\begin{definition}[Lexicographic rank]
\label{def:rho}
Given an alphabet $\Sigma$ of size $\sigma$, I denote the \emph{lexicographic rank} of any alphabet symbol by the function $\lexrank{} : \Sigma \rightarrow \OInterval{\Ibase}{\sigma}$, \st $\lexrank{a} < \lexrank{b} \iff a < b$ for any distinct $a,b \in \Sigma$.
In addition, I denote by $\lexmin{\Sigma}$ the lexicographically smallest symbol $a \in \Sigma$ \st $\lexrank{a}=0$, and by $\lexnext{a}$ the lexicographical successor $b$ of $a$ in $\Sigma$ \st $\lexrank{b} = \lexrank{a}+1$.
\end{definition}

\begin{definition}[Lexicographical order]
\label{def:lexorder}
The \emph{lexicographical order} $\lexlt$ between two non-empty strings $\XSeq,\YSeq$ is defined as $\XSeq \lexlt \YSeq \iff \Subscript{\XSeq}{\At{1}} < \Subscript{\YSeq}{\At{1}}, \text{ or } \Subscript{\XSeq}{\At{1}} = \Subscript{\YSeq}{\At{1}} \text{ and } \Suffix{\XSeq}{\At{2}} \lexlt \Suffix{\YSeq}{\At{2}}$.
\end{definition}

%\emph{Generalized} full-text indices work on string collections.
\begin{definition}[String collection]
\label{def:col}
A \emph{string collection} is an ordered multiset $\Strings = \{ \AtCollection{\String}{\At{1}}, \dots, \AtCollection{\String}{\AtEnd{\Scard}} \}$ of non necessarily distinct strings over a common alphabet $\Sigma$.
I denote by $\norm{\Strings}=\sum_{i=\At{1}}^{\AtEnd{c}}{|\AtCollection{\String}{i}|}$ the total length of the string collection.
The above definitions of prefix, suffix and substring generalize to multisets, \eg $\Substring{\Strings}{(d,i)}{(d,j)}$ denotes the substring $\Substring{\String^d}{i}{j}$.
\end{definition}

To well-define full-text indices, I use the definitions of padded string (\ref{def:strd}) and padded string collection (\ref{def:coltd}).
To this end, I introduce the special \emph{terminator} symbol $\$ \not \in \Sigma$, \st $\lexrank{\$} < \lexrank{a}$ for any $a \in \Sigma$.

\begin{definition}[Padded string]
\label{def:strd}
Given a string $\String$ over $\Sigma$, I call \emph{padded string} the string $\String\$$ over $\Sigma_{\$}$, where $\Sigma_{\$} =\Sigma \cup \{ \$ \}$.
\end{definition}

\begin{definition}[Padded string collection]
\label{def:coltd}
Given a string collection $\Strings = \{ \AtCollection{\String}{\At{1}}, \ldots, \AtCollection{\String}{\AtEnd{c}} \}$ over $\Sigma$, I call \emph{padded string collection} the collection $\{ \AtCollection{\String}{\At{1}}\AtCollection{\$}{\At{1}}, \ldots, \AtCollection{\String}{\AtEnd{\Scard}}\AtCollection{\$}{\AtEnd{\Scard}} \} $ over $\Sigma_{\$}$, where  $\Sigma_{\$} = \Sigma \cup \{ \AtCollection{\$}{\At{1}}, \ldots, \AtCollection{\$}{\AtEnd{\Scard}} \}$ and $\lexrank{\AtCollection{\$}{i}} < \lexrank{\AtCollection{\$}{j}} \iff i < j$.
\end{definition}

\begin{definition}[Reverse string]
\label{def:revstr}
For a string $\String$, $\Rev{\String}$ denotes its reversal, \ie $\Subscript{\Rev{\String}}{i} = \Subscript{\String}{\AtEnd{\Slen} - i}$ for any $\At{1} \leq i \LeqThan{\Slen}$.
\end{definition}

\begin{definition}[Reverse string collection]
\label{def:revcol}
For a string collection $\Strings = \{ \AtCollection{\String}{\At{1}}, \ldots, \AtCollection{\String}{\AtEnd{\Scard}} \}$, $\Rev{\Strings}$ denotes the reversed collection $\{ \AtCollection{\Rev{\String}}{\At{1}}, \ldots, \AtCollection{\Rev{\String}}{\AtEnd{\Scard}} \}$.
\end{definition}

In this manuscript, I use numerical intervals, arrays and matrices.
Numerical intervals are sequences over $\N_{\Ibase}$, \ie $\Interval{i}{j} = \{i,i+1,\ldots,j\}$ and $\OInterval{i}{j} = \Interval{i}{j-1}$.
An \emph{array} (or table, or vector) $V=\Subscript{V}{\At{1}} \dots \Subscript{V}{\AtEnd{\Slen}}$ is a finite sequence of numbers $\Subscript{V}{i} \in \N_0$.
I denote by $\SubscriptNV{V}{i}$ the subscript element $\Subscript{V}{i}$.
A \emph{matrix} $X$ is a rectangular array of numbers $\SubscriptM{X}{i}{j} \in \N_0$ arranged \st
$$X=
\begin{pmatrix}
  \SubscriptM{X}{\At{1}}{\At{1}} & \cdots & \SubscriptM{X}{\At{1}}{\AtEnd{n}} \\
  \vdots & \ddots & \vdots  \\
  \SubscriptM{X}{\AtEnd{m}}{\At{1}} & \cdots & \SubscriptM{X}{\AtEnd{m}}{\AtEnd{n}}
\end{pmatrix}$$
I denote by $\SubscriptNM{X}{i}{j}$ the subscript element $\SubscriptM{X}{i}{j}$.

\section{Transcripts, alignments and distances}

I now define the basic edit operations to transform one string into another.
Given two strings $\XSeq,\YSeq$ of equal length $\Xlen$, the string $\XSeq$ is easily transformed into the string $\YSeq$ by substituting (or replacing) all symbols $\Subscript{X}{i}$ \st $\Subscript{X}{i} \neq \Subscript{Y}{i}$ into $\Subscript{Y}{i}$, for $\At{1} \leq i \LeqThan{\Xlen}$.
However, if the two strings have different lengths, some symbols must be necessarily inserted or deleted from $\XSeq$ in order to obtain $\YSeq$.
This fact motivates the following definition of \emph{edit transcript}.

\begin{definition}
\label{def:transcript}[Edit transcript]
An edit transcript for any two given strings $\XSeq,\YSeq$ is a finite sequence of substitutions, insertions and deletions transforming $\XSeq$ into $\YSeq$ \citep{Gusfield1997}.
\end{definition}

An \emph{alignment} is an alternative way of visualizing a transformation between strings.
While an edit transcript is an explicit sequence of edit operations that transform one string into another, an alignment is an explicit relationship between pairs of symbols from the two strings.
Nonetheless, when some symbols are inserted or removed, some symbols in one string are not related to any symbol in the other string.
For this reason, it is necessary to introduce an additional gap symbol $-$, not being part of the string alphabet $\Sigma$.
The definition of alignment follows.
Figure~\ref{fig:edit-transcript} shows an example of edit transcript with its associated alignment.

\begin{definition}[Alignment]
\label{def:alignment}
An alignment of two strings of length $\Ylen,\Xlen$ over $\Sigma$ is a string of length between $\min\{\Ylen,\Xlen\}$ and $\Ylen+\Xlen$ over the pair alphabet $(\Sigma \cup \{ - \}) \times (\Sigma \cup \{ - \}) \setminus \{ (-,-) \} $.
\end{definition}

\begin{figure}[h]
\begin{center}
\caption[Example of edit transcript and alignment]{Example of edit transcript and alignment. The string $\XSeq$ is transformed into $\YSeq$. The transcript character \texttt{M} indicates a match, \texttt{R} a replacement, \texttt{I} an insertion, and \texttt{D} a deletion.
}
\label{fig:edit-transcript}
\input{figures/transcript.tikz}
\end{center}
\end{figure}

At this point, I give the definition of two fundamental distance functions between strings.

\begin{definition}[Hamming distance]
\label{def:hamming}
The \emph{Hamming distance} $d_H : \Sigma^{\Xlen} \times \Sigma^{\Xlen} \rightarrow \N_0$ between two strings $\XSeq,\YSeq \in \Sigma^{\Xlen}$ counts the number of substitutions necessary to transform $\XSeq$ into $\YSeq$ \citep{Hamming1950}.
\end{definition}

\begin{definition}[Edit distance]
\label{def:edit}
The \emph{Levenshtein} or \emph{edit distance} $d_E : \Sigma^{*} \times \Sigma^{*} \rightarrow \N_0$ between two strings $\XSeq,\YSeq \in \Sigma^{*}$ counts the \emph{minimum} number of edit operations necessary to transform $\XSeq$ into $\YSeq$ \citep{Levenshtein1966}.
\end{definition}

The problem of finding an optimal alignment between two strings is equivalent to the problem of finding their minimum distance \citep{Gusfield1997}.
While the Hamming distance between any two strings of length $\Xlen$ is easily computed in time $\Oh(\Xlen)$, computing the edit distance involves solving a non-trivial optimization problem.

\section{Edit distance computation}

The edit distance between two strings is efficiently computed via \emph{dynamic programming} (DP) \citep{Needleman1970}.
%Below, I describe the three essential components of the DP approach: the recurrence relation, the DP table, and the traceback.
Let $\XSeq,\YSeq$ be two strings of length $\Xlen \geq \Ylen$.
The edit distance $d_E(\Prefix{\XSeq}{i},\Prefix{\YSeq}{j})$ between any their prefixes $\Prefix{\XSeq}{i}$ and $\Prefix{\YSeq}{j}$ is defined recursively.
The base conditions of the recurrence relation are:
\begin{eqnarray}
d_E(\epsilon,\epsilon)&=&0\label{eq:dp-ee}\\
d_E(\Prefix{\XSeq}{i},\epsilon)&=&i \; \text{ for all } \At{1} < i \LeqThan{\Xlen}\label{eq:dp-row}\\
d_E(\epsilon, \Prefix{\YSeq}{j})&=&j \; \text{ for all } \At{1} < j \LeqThan{\Ylen}\label{eq:dp-col}
\end{eqnarray}
and the recursive case for all $\At{1} < i \LeqThan{\Xlen}$ and $\At{1} < j \LeqThan{\Ylen}$ is as follows:
\begin{eqnarray}
d_E(\Prefix{\XSeq}{i},\Prefix{\YSeq}{j}) = \min \left\{
\begin{array}{lcl}
d_E(\Prefix{\XSeq}{i-1},\Prefix{\YSeq}{j})&+&1\\
d_E(\Prefix{\XSeq}{i},\Prefix{\YSeq}{j-1})&+&1\\
d_E(\Prefix{\XSeq}{i-1},\Prefix{\YSeq}{j-1})&+&\delta(\Subscript{X}{i}, \Subscript{Y}{j})
\end{array}
\right.\label{eq:dp-min}
\end{eqnarray}
where the function $\delta$ indicates whether the characters $\Subscript{X}{i}$, $\Subscript{Y}{j}$ match or mismatch (see definition \ref{def:delta}).

\begin{figure*}[t]
\begin{center}
\begin{minipage}[t]{.9\textwidth}
\begin{algorithm}[H]
\Algorithm{EditDistance}{$\XSeq,\YSeq$}
\begin{tabular}{ll}
\textbf{Input}  & $\XSeq$ : string of length $\Xlen$\\
				& $\YSeq$ : string of length $\Ylen$\\
\textbf{Output} & the edit distance $d_E(\XSeq,\YSeq)$ between $\XSeq$ and $\YSeq$\\
\end{tabular}
\begin{algorithmic}[1]
\State {$\SubscriptNM{D}{0}{0} \gets 0$}
\For {$j \gets 1 \, \To \Ylen$}
	\State {$\SubscriptNM{D}{j}{0} \gets \SubscriptNM{D}{j-1}{0} + 1$}
\EndFor
\For {$i \gets 1 \, \To \Xlen$}
	\State {$\SubscriptNM{D}{0}{i} \gets \SubscriptNM{D}{0}{i-1} + 1$}
	\For {$j \gets 1 \, \To \Ylen$}
		\State {$\SubscriptNM{D}{i}{j} \gets \min \, \{ \, \SubscriptNM{D}{i-1}{j} + 1, \SubscriptNM{D}{i}{j-1} + 1, \SubscriptNM{D}{i-1}{j-1} + \delta(\Subscript{X}{i}, \Subscript{Y}{j}) \, \}$}
	\EndFor
\EndFor
\State \Return \SubscriptNM{D}{\Ylen}{\Xlen}
\end{algorithmic}
\label{alg:dp-edit}
\end{algorithm}
\end{minipage}
\end{center}
\end{figure*}

Algorithm \ref{alg:dp-edit} computes the above recurrence relation in time $\Oh(\Xlen\Ylen)$ using a dynamic programming table $D$ of $(\Xlen+1) \times (\Ylen+1)$ cells, where cell $\SubscriptNM{D}{i}{j}$ stores the value of $d_E(\Prefix{\XSeq}{i},\Prefix{\YSeq}{j})$.
The sole edit distance without any alignment can be computed in space $\Oh(\Ylen)$; indeed, only column $j-1$ is required to compute column $j$.
An optimal alignment can be computed in time $\Oh(\Xlen + \Ylen)$ via \emph{traceback} on the table $D$;
the traceback starts in the cell $D[\Ylen,\Xlen]$ and goes backwards (either left, up-left, or up) to the previous cell by deciding which condition of equation~\ref{eq:dp-min} yielded the value of $D[\Ylen,\Xlen]$.

If no alignment at all is needed, there is a more efficient solution to the edit distance problem.
Myers' algorithm \citep{Myers1999}, instead of computing DP cells one after another, encodes one whole DP column in two bit-vectors and computes the adjacent column in a constant number of 12 logical and 3 arithmetical operations.
The time complexity of Myers' algorithm is $\Oh(\Xlen\Ylen/b)$, where $b$ is the processor's bit-width, \eg 64 on modern processors.

% === Overview of existing methods ===

\section{String matching}

\begin{definition}[Exact string matching]
Given a string $\Pattern$ of length $\Plen$, called the \emph{pattern}, and a string $\Text$ of length $\Tlen$, called the \emph{text}, the \emph{exact string matching} problem is to find all occurrences of $\Pattern$ into $\Text$ \citep{Gusfield1997}.
\end{definition}

Exact string matching is one of the most fundamental problems in stringology.
This problem has been extensively studied from the theoretical standpoint and is well solved in practice \citep{Faro2013}.
Nonetheless, the definition of distance functions between strings lends to a more challenging problem: \emph{approximate string matching}.

\begin{definition}[Approximate string matching]
Given a text $\Text$, a pattern $\Pattern$, and a \emph{distance threshold} $k \in \N$, the approximate string matching problem is to find all occurrences of $\Pattern$ into $\Text$ within distance $k$ \citep{Galil1988}.
\end{definition}

The approximate string matching problem under the Hamming distance is commonly called the \emph{$k$-mismatches} problem, while under the edit distance is called the \emph{$k$-differences} problem.
Figure \ref{fig:edit-occurrence} shows an example of occurrence for $k$-differences.
For $k$-mismatches and $k$-differences, it must hold $k > 0$ as the case $k = 0$ corresponds to exact string matching, and $k < \Plen$ as a pattern trivially occurs at any position in the text if all its $\Plen$ characters are substituted.
Frequently, the problem's input respects the condition $k \ll \Plen \ll \Tlen$.
\begin{definition}[Error rate]
Under the edit or Hamming distance, the \emph{error rate} is defined as $\epsilon = k/\Plen$, with $0 < \epsilon < 1$ given the above conditions.
\end{definition}

\begin{figure}[t]
\begin{center}
\caption[Example of occurrence for $k$-differences]{Example of occurrence for $k$-differences. Pattern $\Pattern$ occurs in text $\Text$ at edit distance 3, \ie with a 19~\% error rate.
The alignment between $\Pattern$ and any substring of $\Text$ is called semi-global, as opposed to the global alignment of two complete strings.
}
\label{fig:edit-occurrence}
\input{figures/occurrence.tikz}
\end{center}
\end{figure}

String matching problems are subdivided in two categories, \emph{online} and \emph{offline}, depending on which string, the pattern or the text, is given first.
Algorithms for online string matching work by preprocessing the pattern and scanning the text from left to right (or right to left).
Algorithms for offline string matching are instead allowed to preprocess the text,
hence they build an index of the text beforehand to speed up subsequent searches.
In practice, if the text is long, static and searched frequently, offline methods outperform online methods in terms of runtime, provided the necessary amount of memory for text indexing.

It goes without saying that offline string matching algorithms are tightly bound to text indexing data structures.
Almost all of these algorithms require a \emph{full-text index}, \ie a data structure representing all substrings of the text.
Very often, such full-text index is realized as the \emph{suffix tree} \citep{Weiner1973}, a fundamental data structure in stringology.
Among its virtues \citep{Apostolico1985}, the suffix tree natively provides exact string matching in optimal time and approximate string matching via backtracking \citep{Ukkonen1993}.
Often, the suffix tree finds its use within hybrid \emph{filtering methods} rather than on its own.

Filtering methods first discard uninteresting portions of the text and subsequently verify only narrower areas.
These methods work either online or offline.
Online filtering methods try to jump over the text while scanning it; instead, offline filtering methods use an index to place anchors in the text.
Both classes of filtering methods then require a native online method to verify the anchors.

Filtering methods outperform \emph{native} online and indexed methods for a vast range of inputs, \ie when the error rate is low, and are thus very appealing from a practical standpoint.
Nonetheless, filtering methods are just opportunistic combinations of native online and indexed methods.

In this manuscript, I often consider \emph{multiple} string matching problems, \ie variants in which many patterns are given at once, instead of \emph{single} problems where patterns are given one by one in an online fashion.
Obviously, any method for the single case can solve the multiple case and vice versa.
However, it is clear that methods for multiple string matching have advantages over methods for single string matching.
For instance, online methods for multiple string matching are allowed to preprocess all patterns and then scan the text only once, while online methods for single string matching have to scan the text every time a pattern is given.
Thus, in general, methods for multiple string matching are more appealing than methods for single string matching.

In the remainder of this section, I give a quick overview of the fundamental algorithms and data structures adopted by classic string matching methods.
This overview serves as an introduction to the indexed and filtering methods presented in the next two chapters.
For an extensive treatment of this subject, the reader is referred to complete surveys on online \citep{Navarro2001a} and indexed \citep{Navarro2001} approximate string matching methods.

% --- Online methods ---

\subsection{Online methods}
\label{sub:introonline}

%\subsubsection{Dynamic programming}

The DP algorithm~\ref{alg:dp-edit} to compute the edit distance between two strings is easily turned into an online $k$-differences algorithm.
Since an approximate occurrence of the pattern can start (and end) anywhere in the text, the problem involves computing the edit distance between the pattern and \emph{any substring} of the text.
Algorithm ~\ref{alg:dp-kdiff} efficiently solves this problem by computing the edit distance between the text and the pattern without penalizing leading and trailing deletions in the text.

\begin{figure*}[b]
\begin{center}
\begin{minipage}[t]{.9\textwidth}
\begin{algorithm}[H]
\Algorithm{KDifferences}{$\Text,\Pattern,k$}
\begin{tabular}{ll}
\textbf{Input}  & $\Text$ : text string of length $\Tlen$\\
				& $\Pattern$ : pattern string of length $\Plen$\\
				& $k$ : integer bounding the number of errors\\
\textbf{Output} & all end positions of $k$-differences occurrences of $\Pattern$ in $\Text$\\
\end{tabular}
\begin{algorithmic}[1]
\State {$\SubscriptNM{D}{0}{0} \gets 0$}
\For {$j \gets 1 \, \To \Plen$}
	\State {$\SubscriptNM{D}{j}{0} \gets j$}
\EndFor
\For {$i \gets 1 \, \To \Tlen$}
	\State {$\SubscriptNM{D}{0}{i} \gets 0$}
	\For {$j \gets 1 \, \To \Plen$}
		\State {$\SubscriptNM{D}{i}{j} \gets \min \, \{ \, \SubscriptNM{D}{i-1}{j} + 1, \SubscriptNM{D}{i}{j-1} + 1, \SubscriptNM{D}{i-1}{j-1} + \delta(\Subscript{\Text}{i}, \Subscript{\Pattern}{j}) \, \}$}
	\EndFor
	\If {$\SubscriptNM{D}{i}{\Plen} \leq k$}
		\State \Report $i$
	\EndIf
\EndFor
\end{algorithmic}
\label{alg:dp-kdiff}
\end{algorithm}
\end{minipage}
\end{center}
\end{figure*}

Consider the recurrence relation described by equations~\ref{eq:dp-ee}--\ref{eq:dp-min} and pose $\XSeq=\Text$ and $\YSeq=\Pattern$.
Because an occurrence of the pattern can start anywhere in the text, the base condition \ref{eq:dp-row} of the edit distance recurrence relation becomes:
\begin{eqnarray}
d(\Prefix{\XSeq}{i}, \epsilon) = 0 \; \text{ for all } \At{1} < i \LeqThan{\Xlen} \label{eq:dp-row-kdiff}
\end{eqnarray}
and algorithm~\ref{alg:dp-kdiff} initializes the top row of the DP matrix $D$ accordingly.
Then, as an occurrence of the pattern can end anywhere in the text, algorithm~\ref{alg:dp-kdiff} checks any cell in the bottom row for the condition $\SubscriptNM{D}{i}{\Plen} \leq k$.


% --- Indexed methods ---

\subsection{Indexed methods}
\label{sub:introindex}

I now introduce \emph{suffix tries}, idealized data structures to index full texts.
I define a set of generic operations to traverse them in a top-down fashion.
In chapter~\ref{chr:index}, I introduce various data structures replacing suffix tries in practical implementations.
Thanks to these generic traversal operations, I later formulate generic string matching algorithms that are independent of the practical suffix trie implementations.

%\subsubsection{Trie}

Consider a padded string collection $\Strings$ (definition~\ref{def:coltd}) consisting of $\Scard$ strings.
Note that padding is necessary to ensure that no string $\AtCollection{\String}{i} \in \Strings$ is a prefix of another string $\AtCollection{\String}{j} \in \Strings$.

\begin{definition}[Trie]
The \emph{trie} $\RtrieOf{\Strings}$ of $\Strings$ is a lexicographically ordered tree data structure having one node designated as the root and $\Scard$ leaves, denoted as $\StrieLeaf{\At{1}}, \dots, \StrieLeaf{\AtEnd{\Scard}}$, where leaf $\StrieLeaf{i}$ points to string $\AtCollection{\String}{i}$. In addition,
\begin{enumerate}
\item the incoming edge of any inner node of $\RtrieOf{\Strings}$ is labeled with a symbol in $\Sigma$;
\item the incoming edge of any leaf of $\RtrieOf{\Strings}$ is labeled with a terminator symbol;
\item the path from the root to leaf $\StrieLeaf{i}$ spells the string $\AtCollection{\String}{i}\AtCollection{\$}{i}$.
%Figure~\ref{fig:trie} illustrates.
\end{enumerate}
\end{definition}

%\subsubsection{Suffix trie}

\begin{definition}[Suffix trie]
Given a padded string $\String$ (definition~\ref{def:strd}) of length $\Slen$, the suffix trie $\StrieOf{\String}$ is the trie of all suffixes of $\String$.
$\StrieOf{\String}$ has $\Slen$ leaves, where leaf $\StrieLeaf{i}$ points to suffix $\Suffix{\String}{i}$.
\end{definition}
%The suffix trie generalizes to index string collections.

\begin{definition}[Generalized suffix trie]
Given a padded string collection $\Strings$ (definition~\ref{def:coltd}), its generalized suffix tree is the suffix tree of all strings in $\Strings$.
Each of the $||\Strings||$ leaves is labeled by a pair $\StrieLeaf{(i,j)}$ and points to suffix $\Suffix{\Strings}{(i,j)}$.

% the string $\AtCollection{\String}{i} \in \Strings$ and $\At{1} \leq j \LeqThan{\AtCollection{\Slen}{i}}$ points to suffix $\AtCollection{\Slen}{i}\AtCollection{\$}{i}$ of $\AtCollection{\String}{i}$.
%The path from the root to leaf $(i,j)$ spells the suffix $\Suffix{\Strings}{(i,j)}\AtCollection{\$}{i}$.
\end{definition}
%Figure~\ref{fig:stree} illustrates.

\begin{figure}[b]
\caption[Example of suffix trie and suffix tree]{Suffix trie and suffix tree of the string {\ttfamily ANANAS\$} .}
\label{fig:stree}
\begin{subfigure}[b]{.5\textwidth}
\begin{center}
\input{figures/strie_banana.tikz}
\end{center}
\end{subfigure}%
\begin{subfigure}[b]{.5\textwidth}
\begin{center}
\input{figures/stree_banana.tikz}
\end{center}
\end{subfigure}
\end{figure}

Note that the (generalized) suffix trie here defined contains $\Oh(\Slen^2)$ nodes, yet I only consider the suffix trie as an idealized data structure to formulate generic algorithms.
The optimal data structure to represent in linear space all suffixes of a given string is the \emph{suffix tree} \citep{Morrison1968}.
However, the suffix tree comes with the restriction that internal nodes are branching, \ie they must have more than one child, and with the property that edges can labeled by strings of arbitrary length (see figure~\ref{fig:stree}).
This latter property slightly complicates the exposition of string matching algorithms but it does not affect their runtime complexity nor their result.
For this reason, in the remainder of this manuscript, I consider \wlogs always suffix tries instead of suffix trees.
%I remark that all given algorithms can be generalized to work on trees.

\begin{figure}[t]
\caption[Example of generalized suffix trie]{Generalized suffix trie of the string collection $\Strings = \{$ {\ttfamily ANANAS$\AtCollection{\$}{\At{1}}$}, {\ttfamily CACAO$\AtCollection{\$}{\At{2}}$} $\}$.}
\label{fig:gstrie}
\begin{center}
\input{figures/gstrie.tikz}
\end{center}
\end{figure}

\subsubsection{Top-down traversal}

I define a set of generic operations to traverse tries in a top-down fashion.
Note that the following traversal operations do not walk over edges labeled terminator symbols, as these symbols are not necessary in string matching applications.
I define the following operations on the node $\StrieNode$ pointed during the traversal of $\StrieOf{\Strings}$:
\begin{itemize}
\item \FunctionCall{isRoot}{\StrieNode} returns true iff the pointed node is the root;
\item \FunctionCall{isLeaf}{\StrieNode} returns true iff all outgoing edges are labeled by terminator symbols;
\item \FunctionCall{label}{\StrieNode} returns the symbol labeling the edge entering $\StrieNode$;
\item \FunctionCall{occurrences}{\StrieNode} returns the list of positions pointed by leaves below $\StrieNode$;
\item \FunctionCall{concat}{\StrieNode} returns the concatenation of edge labels on the path from the root to $\StrieNode$.
\end{itemize}
Moreover, I define the following operations moving from pointed node $\StrieNode$, and returning \emph{true} on success and \emph{false} otherwise:
\begin{itemize}
\item \FunctionCall{goDown}{\StrieNode} moves to the lexicographically smallest child of $\StrieNode$;
\item \FunctionCall{goDown}{\StrieNode, c} moves to the child of $\StrieNode$ whose entering edge is labeled by $c$;
\item \FunctionCall{goRight}{\StrieNode} moves to the lexicographically next child of $\StrieNode$;
\item \FunctionCall{goUp}{\StrieNode} moves to the parent node of $\StrieNode$.
\end{itemize}

Time complexities of the above operations depend on the data structure implementing the trie.
Usually \textsc{label} is $\Oh(1)$, both variants of \textsc{goDown} and \textsc{goRight} can be $\Oh(1)$ or logarithmic in $\Slen$, \textsc{occurrences} can be linear in the number of occurrences, \textsc{goUp} is $\Oh(1)$ but with an additional $\Oh(\Slen)$ space complexity to stack all parent nodes.
Algorithms \ref{alg:st-godown} and \ref{alg:st-goright} show how to implement respectively \textsc{goDown} and \textsc{goRight} using \textsc{goDown} a symbol and \textsc{goUp}, although with a worst-case time complexity of $\Oh(\sigma)$.
In chapter~\ref{chr:index}, I consider various data structures to implement suffix tries, I show how to implement these operations and give their complexities.

\begin{figure*}[t]
\begin{minipage}[t]{.5\textwidth}
\begin{algorithm}[H]
\Algorithm{goDown}{$\StrieNode$}
\begin{tabular}{ll}
\textbf{Input}  & $\StrieNode$ : iterator of $\StrieOf{\String}$\\
\textbf{Output} & boolean indicating success\\
\end{tabular}
\begin{algorithmic}[1]
\If {\Call{isLeaf}{$\StrieNode$}}
	\State \Return \False
\EndIf
\If {\Call{goDown}{$\StrieNode, \lexmin{\Sigma}$}}
	\State \Return \True
\Else
	\State \Return $\Call{goRight}{\StrieNode}$		
\EndIf
\end{algorithmic}
\label{alg:st-godown}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{.5\textwidth}
\begin{algorithm}[H]
\Algorithm{goRight}{$\StrieNode$}
\begin{tabular}{ll}
\textbf{Input}  & $\StrieNode$ : iterator of $\StrieOf{\String}$\\
\textbf{Output} & boolean indicating success\\
\end{tabular}
\begin{algorithmic}[1]
\If {\NotB \Call{isRoot}{$\StrieNode$}}
	\While {$\StrieChar \gets \lexnext{\Call{label}{\StrieNode}}$}
		\State {$\Call{goUp}{\StrieNode}$}
		\If {$\Call{goDown}{\StrieNode, \StrieChar}$}
			\State \Return \True
		\EndIf
	\EndWhile
\EndIf
\State \Return \False $\phantom{()}$
\end{algorithmic}
\label{alg:st-goright}
\end{algorithm}
\end{minipage}
\end{figure*}


% --- Filtering methods ---

\subsection{Filtering methods}
\label{sec:intro:filtering}

Filtering methods work in two stages: the \emph{filtration stage} discards portions of the text unlikely or unable to contain an occurrence of the pattern; subsequently the \emph{verification stage} checks the remaining portions.
The filtration stage proceeds online by scanning the text or alternatively offline using an index of the text.
The verification stage uses a conventional method, \eg the online dynamic programming method or some variation of it.
The crux of filtering methods is thus to accurately and efficiently classify text portions as containing or not some occurrence of the pattern.

\subsubsection{Specificity and sensitivity}
\label{intro:filtering:spec-sens}

Any filtering method is thus a binary classification method.
Any text location is \emph{true} if it coincides with the beginning of an occurrence of the pattern and \emph{false} if it does not.
The outcome of the classification method is \emph{positive} for text locations filtered in and \emph{negative} for locations filtered out.
Therefore, as shown in table \ref{tab:filter:accuracy}, any text location belongs either to the set of \emph{true positives} (TP), \emph{false positives} (FP), \emph{true negatives} (TN), or \emph{false negatives} (FN).
Thus, standard \emph{specificity} and \emph{sensitivity} measure the accuracy of filtering methods.
The specificity of a filter $f$ on a text $\Text$ is defined as:
\begin{eqnarray}
\frac{|TN_f(t)|}{|TN_f(t)| + |FP_f(t)|}
\end{eqnarray}
and the sensitivity as:
\begin{eqnarray}
\frac{|TP_f(t)|}{|TP_f(t)| + |FN_f(t)|}
\end{eqnarray}

%An important case is its sensitivity is 1.
\begin{definition}
A filter is \emph{lossless} or \emph{full-sensitive} if its sensitivity is 1, \ie it produces no false negatives, otherwise it is \emph{lossy}.
\end{definition}
%A full-sensitive filter for approximate string matching solves the problem exactly.
Many practical applications, \eg read mapping, do not require strictly lossless filtration.
Nonetheless, a predictable or controlled lossy filter helps to interpret the results and insure their quality.
For this reason, I focus on criteria yielding filters which are lossless or lossy in a predictable fashion.

\begin{table}[b]
\begin{center}
\caption[Classification of text locations by filtering methods]{Classification of text locations by filtering methods.}
\begin{tabular}{ccc}
\toprule
Occurrence & Filtered in & Filtered out\\
\midrule
Yes & True positive (TP) & False negative (FN) \\
No & False positive (FP) & True negative (TN) \\
\bottomrule
\end{tabular}
\label{tab:filter:accuracy}
\end{center}
\end{table}

\subsubsection{Efficiency}

The total runtime of a filtering method is given by the sum of the runtimes of its filtration and verification stages.
Filtration specificity determines how much time is spent in the verification stage.
Clearly, to keep verification time small, the number of false positives must be low.
Nonetheless, the filtration stage must also run in a reasonable amount of time.
Hence, the runtime of an efficient filtering method is usually balanced between filtration and verification time.
In any way, no filtering method can be efficient when the number of true positive locations approaches the text length.
%No filtering method is effective unless $|TP_f(t)| \ll |t|$.

Filtering methods for string matching work under the assumption that patterns occur in the text with a \emph{low average probability}.
For $k$-differences, the occurrence probability is a function of the error rate $\epsilon$ and the alphabet size $\sigma$, and can be computed or estimated under the assumption of the text being generated by a specific random source.
Under the uniform Bernoulli model, where each symbol of $\Sigma$ occurs with probability $1/\sigma$, \cite{Navarro2001a} experimentally finds that $\epsilon < 1 - 1 / \sqrt{\sigma}$ is a tight upper bound on the error rate which ensures few occurrences and for which filtering algorithms are effective.
For higher error rates, non-filtering methods, either online or offline, work better.

On the one hand, 50 \% error rate marks the filtration efficiency bound for the DNA alphabet, while on the other hand HTS read mapping applications require error rates in a range from 3 to 10 \%.
According to the above considerations, filtering methods are expected to be very efficient in HTS applications.

\subsubsection{Seeds versus $q$-grams}

Filtering methods apply combinatorial criteria to determine which portions of the text might contain some occurrence of the pattern.
These criteria are in general valid for both online and offline variants of the problem.
In practice, one specific criterion might be more convenient for one variant of the problem rather than the other.
The combinatorial criterion underlying a filter is of paramount importance as it provides guarantees on filtration sensitivity.

In chapter \ref{sec:filter}, I consider two classes of combinatorial filtering methods: those based on \emph{seeds} and those based on \emph{$q$-grams}.
Filters in the former class partition the pattern into \emph{non-overlapping} factors called seeds;
application of the pigeonhole principle yields full-sensitive partitioning strategies.
Instead, filters in the latter class consider all \emph{overlapping} substrings of the pattern having length $q$, the so-called $q$-grams;
simple counting lemmata give lower bounds on the number of $q$-grams that must be present in a narrow window of the text, as necessary conditions for an approximate occurrence of the pattern.

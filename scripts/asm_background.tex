
\chapter{Stringology preliminaries}

We now introduce fundamental definitions and problems of stringology, in order to keep the manuscript self-contained.
The reader familiar with basic stringology can skip this section and proceed to section~\ref{?}.

\section{Definitions}

Let us start by defining primitive objects of stringology: alphabets and strings.
An alphabet is a finite set of symbols (or characters); a string (or word) over an alphabet is a finite sequence of symbols from that alphabet.
We denote the length of a string $s$ by $|s|$, and by $\epsilon$ the empty string s.t. $|\epsilon|=0$.
Given an alphabet $\Sigma$, we define $\Sigma^0=\{ \epsilon \}$ as the set containing the empty string, $\Sigma^n$ as the set of all strings over $\Sigma$ of length $n$, and $\Sigma^* = \cup_{n=0}^{\infty}{\Sigma^n}$ as the set of all strings over $\Sigma$.
Finally, we call any subset of $\Sigma^*$ a language over $\Sigma$.

We now define concatenation, the most fundamental operation on strings.
The concatenation operator of two strings is denoted with $\cdot$ and defined as $\cdot : \Sigma^* \times \Sigma^* \rightarrow \Sigma^*$.
Given two strings, $x \in \Sigma^m$ with $x=x_1 x_2 \dots x_m$, and $y \in \Sigma^n$ with $y=y_1 y_2 \dots y_n$, their concatenation $x \cdot y$ (or simply denoted $xy$) is the string $z \in \Sigma^{m+n}$ consisting of the symbols $x_1 x_2 \dots x_m y_1 y_2 \dots y_n$.

From concatenation we can derive the notion of prefix, suffix, and substring.
A string $x$ is a prefix of $y$ iff there is some string $z$ s.t. $y=x\cdot z$.
Analogously, $x$ is a suffix of $y$ iff there is some string $z$ s.t. $y=z\cdot x$.
Moreover, $x$ is a substring of $y$ iff there is some string $w,z$ s.t. $y=w\cdot x \cdot z$, and then we say that $x$ occurs within $y$ at position $|w|$.

\begin{example}
These definitions allow us to model basic biological sequences.
Let us consider the alphabet consisting of DNA bases: $\Sigma = \{\text{A},\text{C},\text{G},\text{T}\}$.
Examples of strings over $\Sigma$ are $x=$A, $y=$AGGTAC, $z=$TA.
For instance, $y \in \Sigma^6$ and $| y | = 6$.
Moreover, the concatenation $x \cdot z$ produces ATA.
The string $x$ is a prefix of $y$, and the string $z$ is a substring of $y$  occurring at position 4 in $y$.
\end{example}

\section{Alignments}

The next step is to define the minimal set of edit operations to transform one string into another: substitutions, insertions and deletions.
Given two strings $x,y$ of equal length $n$, the string $x$ can be transformed into the string $y$ by substituting (or replacing) all symbols $x_i$ s.t. $x_i \neq y_i$ into $y_i$, for $1 \leq i \leq n$.
If the given strings have different lengths, insertion and deletion of symbols from $x$ become necessary to transform it into $y$.
Therefore, given any two strings $x,y$, we define as edit transcript for $x,y$ any finite sequence of substitutions, insertions and deletions transforming $x$ into $y$.
See Figure~\ref{fig:edit-transcript} for an example.

\begin{figure}[h]
\begin{center}
\caption[Example of edit transcript.]{Example of edit transcript transforming the string $x=AAAA$ into $y=CCCC$. The transcript character M indicates a match, R a replacement, I an insertion, and D a deletion.}
\label{fig:edit-transcript}
\input{figures/transcript.tikz}
\end{center}
\end{figure}

An alignment is an alternative yet equivalent way of visualizing a transformation between strings.
While an edit transcript provides an explicit sequence of edit operations transforming one string into another, an alignment relates pairs of corresponding symbols between two strings.
Because some symbols in one string are not related to any symbol in the other string, \ie some symbols are inserted or removed, we first need to introduce a gap symbol $-$, which is not part of the string alphabet $\Sigma$.
Subsequently, we can define the alignment of two strings of length $m,n$ over $\Sigma$ to be a string of length between $\min\{m,n\}$ and $m+n$ over the pair alphabet $(\Sigma \cup \{ - \}) \times (\Sigma \cup \{ - \})$.

\begin{example}
\label{ex:alignment}
An alignment of the strings $x=AAAA$ and $y=CCCC$ is given by the string
$z={\text{A} \choose \text{A}}{\text{A} \choose \text{-}}{\text{A} \choose \text{C}}{\text{G} \choose \text{G}}$
\end{example}

A dotplot is a way to visualize any alignment between two strings and highlight their similarities.
Given two string $x,y$ of length $m,n$, a dotplot is a $m \times n$ matrix containing a dot at position $(i,j)$ iff the symbol $x_i$ matches symbol $y_j$.
We define a dotplot trace to be a monotonical path in the matrix connecting non-decreasing positions of the matrix.
A dotplot trace corresponds to an alignment and vice versa.
In a trace, match and mismatch columns of the corresponding alignment appear as diagonal stretches, while insertions and deletions are horizontal or vertical stretches.
See Figure~\ref{?}.

\begin{figure}[h]
\begin{center}
\caption[Example of dotplot.]{Example of dotplot of the strings $x=AAAA$ and $y=CCCC$. The highlighted trace corresponds to the alignment of Example~\ref{ex:alignment}.}
\label{fig:dotplot}
\input{figures/dotplot.tikz}
\end{center}
\end{figure}

\section{Distance functions}

We can assign a cost to any alignment and to its associated edit transformation by defining a weight function $\omega : (\Sigma \cup \{ - \}) \times (\Sigma \cup \{ - \}) \rightarrow \R_0^{+}$, where:
\begin{itemize}
\item $\omega(\alpha,\beta)$ for all $(\alpha,\beta) \in \Sigma \times \Sigma$ defines the cost of substituting $\alpha$ with $\beta$,
\item $\omega(\alpha,-)$ for all $\alpha \in \Sigma$ defines the cost of deleting the symbol $\alpha$,
\item $\omega(-,\beta)$ for all $\beta \in \Sigma$ defines the cost of inserting the symbol $\beta$,
\end{itemize}
and by defining the total cost $C(z)$ of an alignment $z$ between two strings as the sum of the weights of all its alignment symbols:
\begin{eqnarray}
C(z) = \sum_{i=0}^{|z|}{\omega(z_i)}
\end{eqnarray}
Consequently, we can define the distance function $d : \Sigma^{*} \times \Sigma^{*} \rightarrow \R_0^{+}$ by taking the minimum cost over all possible alignments of $x,y$:
\begin{eqnarray}
d(x,y) = \sum_{z \in \mathbb{A}(x,y)}{C(z)}
\end{eqnarray}

In particular, the edit or \emph{Levenshtein distance} between two strings $x,y \in \Sigma^{*}$ is defined as the function $d_E : \Sigma^{*} \times \Sigma^{*} \rightarrow \N_0$ counting the \emph{minimum} number of edit operation necessary to transform $x$ into $y$.
It is obtained by defining for all $(\alpha,\beta) \in \Sigma \times \Sigma$, $\omega(\alpha,\beta) = 1$ iff $\alpha \neq \beta$ and 0 otherwise, and $\omega(\alpha,-)$ and $\omega(-,\beta)$ as 1.
The \emph{Hamming distance} between two strings $x,y \in \Sigma^{n}$ is defined as the function $d_H : \Sigma^{n} \times \Sigma^{n} \rightarrow \N_0$ counting the number of substitutions necessary to transform $x$ into $y$.
We obtain it by defining $\omega(\alpha,\beta)$ as in the edit distance, and by setting all $\omega(\alpha,-)$ and $\omega(-,\beta)$ to be $\infty$ in order to disallow indels.

\begin{example}
TODO: example of edit and hamming distance.
\end{example}

\section{Optimal alignments}

The problem of finding an optimal alignment between two strings is equivalent to the problem of finding their minimum distance \citep{Gusfield1997}.
A solution to this optimization problem can be efficiently computed via dynamic programming (DP).
Below we describe the three essential components of the DP approach: the recurrence relation, the DP table, and the traceback.

Given two strings $x,y$ of length $m,n$, for all $1 \leq i \leq m$ and $1 \leq j \leq n$ we define with $d(x_{1..i},y_{1..j})$ the distance between their prefixes $x_{1..i}$ and $y_{1..j}$.
The base conditions of the recurrence relation are:
\begin{eqnarray}
d(\epsilon,\epsilon)&=&0\\
d(x_{1..i},\epsilon)&=&\sum_{l=1}^{i}{\omega(x_l, -)} \text{ for all } 1 \leq i \leq m\\
d(\epsilon, y_{1..j})&=&\sum_{l=1}^{j}{\omega(-, y_l)} \text{ for all } 1 \leq j \leq n
\end{eqnarray}
and the recursive case is defined as follows:
\begin{eqnarray}
d(x_{1..i},y_{1..j}) = \min \left\{
\begin{array}{lcl}
d(x_{1..i-1},y_{1..j})&+&\omega(x_i, -)\\
d(x_{1..i},y_{1..j-1})&+&\omega(-, y_j)\\
d(x_{1..i-1},y_{1..j-1})&+&\omega(x_i, y_j)
\end{array}
\right.\label{eq:dp-min}
\end{eqnarray}

We can compute the above recurrence relation in time $\Oh(nm)$ using a dynamic programming table $D$ of $(m+1) \times (n+1)$ cells, where cell $D[i,j]$ stores the value of $d(x_{1..i},y_{1..j})$.
The sole distance without any alignment can be computed in space $\Oh(\min\{ n, m \})$, as we only need column $D[:j-1]$ to compute column $D[:j]$ (or row $D[i-1:]$ to compute $D[i:]$) and we can fill the table $D$ either column-wise or row-wise\footnote{Note that $D$ can be filled also diagonal-wise or antidiagonal-wise.}.
An optimal alignment can be computed in time $\Oh(m + n)$ via \emph{traceback} on the table $D$:
We start in the cell $D[m,n]$ and go backwards (either left, up-left, or up) to the previous cell by deciding which condition of Equation~\ref{eq:dp-min} yielded the value of $D[m,n]$.

\begin{figure}[h]
\begin{center}
\caption[Example of DP table.]{DP table representing the computation of the edit distance $d_E(x_{1..5}, y_{1..4})$.}
\label{fig:edit-dp}
\input{figures/dp_edit.tikz}
\end{center}
\end{figure}


% === Overview of existing methods ===

\section{String matching}

%\subsection{Problem definition}

We can now define \emph{exact string matching}, perhaps the most fundamental problem in stringology.
Given a string $p$ (with $|p|=m$) called the \emph{pattern} and a longer string $t$ (with $|t|=n$) called the \emph{text}, the exact string matching problem is to find all occurrences, if any, of pattern $p$ into text $t$ \citep{Gusfield1997}.
This problem has been extensively studied from the theoretical standpoint and is well solved in practice.

The definition of distance functions between strings let us generalize exact string matching into a more challenging problem: \emph{approximate string matching}.
Given a text $t$, a pattern $p$, and a \emph{distance threshold} $k \in \N$, the approximate string matching (a.s.m.) problem is to find all occurrences of $p$ into $t$ within distance $k$.
The a.s.m. problem under the Hamming distance is commonly referred as the \emph{$k$-mismatches} problem and under the edit distance as the \emph{$k$-differences} problem.
For $k$-mismatches and $k$-differences, it must hold $k > 0$ as the case $k = 0$ corresponds to exact string matching, and $k < m$ as a pattern occurs at any position in the text if we substitute all its $m$ characters.
Under these distances, we define the \emph{error rate} as $\epsilon = \frac{k}{m}$, with $0 < \epsilon < 1$ given the above conditions, and we alternatively refer to these a.s.m. problems as $\epsilon$-mismatches and $\epsilon$-differences.

We can classify string matching problems in two categories, \emph{online} and \emph{offline}, depending on which string, the pattern or the text, is given first.
Algorithms for online string matching work by preprocessing the pattern and scanning the text from left to right (or right to left); their worst-case runtime complexity ranges from $\Oh(nm)$ to $\Oh(n)$ while their worst-case memory complexity ranges from $\Oh(\sigma^k m^k)$ to $\Oh(m)$.
Algorithms for offline string matching are instead allowed to preprocess the text; their worst-case runtime complexity ranges from $\Oh(m)$ to $\Oh(\sigma^k m^k)$ while their worst-case memory complexity is usually $\Oh(n)$.
In practice, if the text is long, static and searched frequently, offline methods largely outperform online methods in terms of runtime, provided the necessary amount of memory. Therefore, we concentrate on offline algorithms.

We can subdivide algorithms for offline string matching in two categories: \emph{fully-indexed} and \emph{filtering}.
Fully-indexed algorithms work solely on the index of the text, while filtering methods first use the index to discard uninteresting portions of the text and subsequently use an online method to verify narrow areas of the text.
Filtering methods outperform fully-indexed methods for a vast range of inputs\footnote{When the error rate is low.} and are thus very interesting from a practical standpoint. Nonetheless, filtering methods are just opportunistic combinations of online and fully-indexed methods.

In the following of this section we thus give a brief and non-exhaustive overview of the fundamental techniques for online and (both fully-indexed and filtering) offline string matching.
This overview serves as an introduction to the more involved algorithms presented in chapter~\ref{?} and directly used in applications of part~\ref{part:apps}.
For an extensive treatment of the subject, we refer the reader to complete surveys on exact \citep{Faro2013} and approximate \citep{Navarro1999} online string matching methods, as well as to a succint survey on indexed methods \citep{Navarro2001}.

% --- Online methods ---

\subsection{Online methods}

We consider two classes of algorithms for online string matching, those based on automata and those based on dynamic programming.

\subsubsection{Automata}

Exact search of one pattern. Knuth-Morris-Pratt automaton.

Exact search of multiple patterns. Aho-corasick automaton.

Approximate search of one pattern. Ukkonen automaton.

%\subsubsection{NFA bit-parallelism}

\subsubsection{Dynamic programming}

The dynamic programming algorithm~\ref{?} to compute the distance of two strings can be easily turned into a string matching algorithm.
Since an occurrence of the pattern can start and end anywhere in the text, a.s.m. consists of computing the edit distance between the pattern and all substrings of the text.
The problem can be thus solved by computing the edit distance between the text and the pattern without penalizing leading and trailing deletions in the text.

We pose $x=p$ and $y=t$ and consider Equation~\ref{eq:dp-min}.
Since an occurrence of the pattern can start anywhere in the text, we change the initialization of the top row $D[0:]$ of the DP matrix according to the condition:
\begin{eqnarray}
d(\epsilon, y_{1..j}) = 0 \text{ for all } 1 \leq j \leq n
\end{eqnarray}
and since an occurrence of the pattern can end anywhere in the text, we check all cells $D[m,j]$ for all $1 \leq j \leq n$ in the bottom row of $D$ for the condition $D[m,j] \leq k$.

\begin{figure}[h]
\begin{center}
\caption[Example of approximate string matching via DP.]{DP table representing the match of $p=...$ in $t=...$.}
\label{fig:asm-dp}
\input{figures/dp_asm.tikz}
\end{center}
\end{figure}


%\subsection{DP bit-parallelism}


% --- Indexed methods ---

\subsection{Indexed methods}
\label{sub:introindex}

No matter how efficient online methods can be, these approaches quickly become impractical when the text is long and searched frequently.
If the text is static and given in advance, we are allowed to preprocess it.
Therefore we build an index of the text beforehand and use it to speed up subsequent searches.
To this intent we introduce the \emph{suffix tree}, an optimal data structure to index all substrings of a text.
We take the suffix tree as an idealized data structure to elegantly expose our indexed algorithms solving string matching problems.
In chapter~\ref{chr:index} we will consider other substring indices to replace the suffix tree in practice.

\subsubsection{Trie}

\subsubsection{Suffix tree and suffix trie}

The suffix tree \citep{Morrison1968} is a lexicographically ordered tree data structure representing all suffixes of a string.
Assume w.l.o.g. a string $s$ of length $n$, padded with a \emph{terminator symbol} $\$$ not being part of the string alphabet $\Sigma$\footnote{The terminator symbol is necessary to ensure that no suffix $s_{i..n}$ is a prefix of another suffix $s_{j..n}$.}.
The suffix tree $\Si$ of the string $s$ has one node designated as the root and $n$ leaves, each one pointing to a distinct suffix of $s$, denoted as $\Ln_1 \dots \Ln_n$.
Each internal node has more than one child, and each edge is labeled with a non-empty substring of $s$.
Each path from the root to a leaf $\Ln_i$ spells the suffix $s_{i..n}$.
Figure~\ref{fig:stree} illustrates.

%The suffix tree can be built in time and space linear in the length of the string.

%The definition of suffix tree can be generalized to index set of strings.
%Given a set of strings $\Dc = \{ s^1, s^2, \dots, s^z \}$, assume w.l.o.g. that each string $s^i \in \Dc$ is padded with distinct terminator symbol $\$_i$.
%Leaves of the \emph{generalized suffix tree} are labeled with with pairs $(i,j)$ such that $1 \leq i \leq z$ and $1 \leq j \leq | s^i |$.
%Each path from the root to a leaf $(i,j)$ spells the suffix $s_{j..n}^i$.

In the following of this manuscript we consider w.l.o.g. \emph{suffix tries} instead of suffix trees.
On suffix tries, internal nodes can have only one child and each edge is labeled by one single character (see Figure~\ref{fig:st}).
This fact simplifies the exposition of all given algorithms without affecting their runtime complexity nor their result.
However, we remark that all given algorithms can be generalized to work on trees.

\begin{figure}[h]
\caption{Suffix tree and suffix trie for the string ANANAS.}
\label{fig:stree}
\begin{subfigure}{.5\textwidth}
\begin{center}
\input{figures/stree_banana.tikz}
\end{center}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\begin{center}
\input{figures/strie_banana.tikz}
\end{center}
\end{subfigure}
\end{figure}

Therefore, from now on we assume the text $t$ to be indexed using a suffix trie $\Ti$.
Given a node $\Tn$ of $\Ti$, we denote with $label(\Tn)$ the label of the edge entering into $\Tn$, with $\Ci(\Tn)$ the set of children of $\Tn$ being internal nodes, with $\Ei(\Tn)$ the set of children of $\Tn$ being leaves, and with $\Li(\Tn)$ the set of all leaves of the subtree rooted in $\Tn$. We remark that entering edges of internal nodes in $\Ci(\Tn)$ are always labeled with symbols in $\Sigma$, while entering edges of leaves in $\Li(\Tn)$ and $\Ei(\Tn)$ are always labeled with terminator symbols.

\subsubsection{Exact string matching}

Using the suffix trie $\Ti$ of the text $t$, we can find all occurrences of a pattern $p$ into $t$ in optimal time $\Oh(m)$, thus independently of $n$.
Algorithm~\ref{alg:st-exact} searches a pattern $p$ by starting in the root node of $\Ti$ and following the path spelling the pattern.
If the search ends up in a node $\Tn$, each leaf $\Ln_i \in \Li(\Tn)$ points to a distinct suffix $t_{i..n}$ such that $t_{i..i+m} = p$.
Algorithm~\ref{alg:st-exact} is correct since each path from the root to any internal node of the suffix trie $\Ti$ spells a different unique substring of $t$; consequently all equal substrings of $t$ are represented by a single common path.

\begin{algorithm}[h]
\caption{Exact string matching on a suffix trie.}
\label{alg:st-exact}
\begin{algorithmic}[1]
\algnotext{EndFor}
\Procedure{ExactSearch}{$\Tn,p$}
	\If {$p = \epsilon$}
		\State \Report $\Li(\Tn)$
	\ElsIf {$\exists\,{\Cn \in \Ci(\Tn)}:\  label(\Cn) = p_1$\label{alg:st-exact:comp}}
		\State \Call{ExactSearch}{$\Cn,p_{2..|p|}$}
	\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{figure}[h]
\begin{center}
\caption[Exact string matching on a suffix tree.]{Exact string matching on a suffix tree. The pattern NA is searched exactly in the text ANANAS.}
\label{fig:st-exact}
\input{figures/stree.tikz}
\end{center}
\end{figure}

\subsubsection{Backtracking $k$-mismatches}

We can solve $k$-mismatches by backtracking \citep{Ukkonen1993, Baeza1999} on the suffix trie $\Ti$, in average time sublinear in $n$ \citep{Navarro2000}.
A top-down traversal on $\Ti$ spells incrementally all distinct substrings of $t$.
While traversing each branch of the trie, we incrementally compute the distance between the query and the spelled string.
If the computed distance exceeds $k$, we stop the traversal and proceed on the next branch.
Conversely, if we completely spelled the pattern $p$ and we ended up in a node $\Tn$, each leaf $\Ln_i \in \Li(\Tn)$ points to a distinct suffix $t_{i..n}$ such that $d_H(t_{i..i+m}, p) \leq k$.
See algorithm~\ref{alg:st-hamming}.

\begin{algorithm}[h]
\caption{$k$-mismatches on a suffix trie.}
\label{alg:st-hamming}
\begin{algorithmic}[1]
\algnotext{EndFor}
\Procedure{KMismatches}{$\Tn,p,e$}
	\If {$e = 0$}
		\State \Call{ExactSearch}{$\Tn,p$}
	\Else 
		\ForAll {$\Cn \in \Ci(\Tn)$}
			\If {$label(\Cn) = p_1$}
				\State \Call{KMismatches}{$\Cn,p_{2..|p|},e$}
			\Else
				\State \Call{KMismatches}{$\Cn,p_{2..|p|},e-1$}
			\EndIf
		\EndFor
	\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{figure}[h]
\begin{center}
\caption{Approximate string matching on a suffix tree.}
\label{fig:st-hamming}
\input{figures/stree.tikz}
\end{center}
\end{figure}

\subsubsection{Backtracking $k$-differences}

We can compute $k$-differences on a suffix tree in two different ways. Algorithm~\ref{alg:st-edit-explicit} explicitly enumerates errors by recursing on the suffix trie. Algorithm~\ref{alg:st-edit} computes the edit distance on the suffix trie.

\begin{algorithm}[h]
\caption{$k$-differences on a suffix trie.}
\label{alg:st-edit-explicit}
\begin{algorithmic}[1]
\algnotext{EndFor}
\Procedure{KDifferences}{$\Tn,p,e$}
	\If {$e = 0$}
		\State \Call{ExactSearch}{$\Tn,p$}
	\Else 
		\State \Call{KDifferences}{$\Tn,p_{2..|p|},e-1$}
		\ForAll {$\Cn \in \Ci(\Tn)$}
			\State \Call{KDifferences}{$\Cn,p,e-1$}
			\If {$label(\Cn) = p_1$}
				\State \Call{KDifferences}{$\Cn,p_{2..|p|},e$}
			\Else
				\State \Call{KDifferences}{$\Cn,p_{2..|p|},e-1$}
			\EndIf
		\EndFor
	\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{$k$-difference on a suffix trie.}
\label{alg:st-edit}
\begin{algorithmic}[1]
\algnotext{EndFor}
\Procedure{KDifferences}{$\Tn,p,e$}
	\ForAll {$\Cn \in \Ci(\Tn)$}
		\State \Call{KDifferences}{$\Cn,p_{2..|p|},e - d_E(repr(\Tn), p)$}
	\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

In algorithm~\ref{alg:st-edit}, any online method can be used to compute the edit distance.
However, for theoretical considerations, it is important to consider an algorithm which computes in $\Oh(1)$ per node.
Note that it is sufficient to have an algorithm capable of checking whether the current edit distance is within the imposed threshold $k$.

Algorithm~\ref{alg:st-edit-explicit} reports more occurrences than algorithm~\ref{alg:st-edit}.
Discuss neighborhood, condensed neighborhood, and super-condensed neighborhood.

% --- Filtering methods ---

\subsection{Filtering methods}
\label{sec:intro:filtering}

The goal of filtering methods is to obtain algorithms that have favorable average running times.
The principle under which they work is that large and uninteresting portions of the text can be quickly discarded, while narrow and highly similar portions can be verified with a conventional online method.
We remark that any filtering method can either work in a online fashion or take advantage of an index of the text to speed up the filtration phase.
Here we always consider the filtration phase to be indexed.

Filtering methods work under the assumption that given patterns occur in the text with a \emph{low average probability}.
Such probability is a function of the error rate $\epsilon$, in addition to the alphabet size $\sigma$, and can be computed or estimated under the assumption of the text being generated by a specific random source.
Under the uniform Bernoulli model, where each symbol of $\Sigma$ occurs with probability $\frac{1}{\sigma}$, \citet{Navarro2000} estimates that $\epsilon < 1 - \frac{1}{\sigma}$ is a conservative bound on the error rate which ensures few matches, and for which filtering algorithms are effective.
For higher error rates, non-filtering online and indexed methods work better.

We call a filter \emph{lossless} or \emph{full-sensitive} if it guarantees not to discard any occurrence of the pattern, otherwise we call it \emph{lossy}.
Lossy filters can be designed to solve approximately $\epsilon$-differences.
We focus our attention on lossless filters. 

We now consider two classes of filtering methods: those based on \emph{seeds} and those based on \emph{$q$-grams}.
Filters based on seeds partition the pattern into \emph{non-overlapping} factors called seeds.
We can derive full-sensitive partitioning strategies by application of the pigeonhole principle.
Instead, filters based on $q$-grams consider all \emph{overlapping} substrings of the pattern having length $q$, the so-called $q$-grams.
Eventually, simple lemmas gives us lower bounds on the number of $q$-grams that must be present in a narrow window of the text as necessary condition for an occurrence of the pattern.

\subsubsection{Seed filters}

We start by considering the case of two arbitrary strings $x,y$ \st $d_E(x,y) \leq k$.
If we partition w.l.o.g. $y$ into $k+1$ non-overlapping seeds, then at least one seed will occur as a factor of $x$.
\begin{lemma}
\label{lemma:exact-seeds}
\citep{Baeza1999b}
Let $x,y$ be two strings \st $d_E(x,y) = k$ for some $k \in \N_0$.
If $y=y^1 y^2 \dots y^{k+1}$ then $x=ay^ib$ for some $a, b$.
\end{lemma}
\begin{proof}
We proceed by induction on $k$.
For $k=0$, the string $y$ is partitioned into only one factor, $y$ itself.
The condition $d_E(x,y) = 0$ implies $x=y$, which is true for $a=\epsilon$ and $b=\epsilon$.
We suppose the case $k=j-1$ to be true, thus since $d_E(x,y) = j-1$ and $y=y^1 y^2 \dots y^{j}$ then $x=ay^ib$ for some $a, b$.
We consider the case $k=j$. The $j$-th error can be in
\begin{inparaenum}[(i)]
\item\label{lemma:exact-seeds:prefix} $y^1\dots y^{i-1}$,
\item\label{lemma:exact-seeds:infix} $y^i$, or
\item\label{lemma:exact-seeds:suffix} $y^{i+1}\dots y^{j}$.
\end{inparaenum}
In case~\ref{lemma:exact-seeds:prefix} or~\ref{lemma:exact-seeds:suffix}, $x=ay^ib$ clearly holds.
In case~\ref{lemma:exact-seeds:infix}, if we partition $y^i$ in two factors $y^{i'}$ and $y^{i''}$, then either $x=ay^{i'}b'$ or $x={a'}y^{i''}b$.
\end{proof}

Thus, we solve $k$-differences by partitioning the pattern into $k+1$ seeds and searching all seeds into the text, \eg with the help of a substring index.
Figure~\ref{fig:seeds-ext} shows an example.
Note that we are reducing one approximate search into many smaller exact searches.
As Lemma~\ref{lemma:exact-seeds} gives us a necessary but not sufficient condition, we must verify whether any candidate location induced by an occurrence of some seed corresponds to an approximate occurrence of the pattern in the text.
Thus, we verify any substring $s$ of the text of length $m - k \leq |s| \leq m + k$ containing one seed of $p$.

\begin{figure}[h]
\begin{center}
\caption{Filtration with exact seeds.}
\label{fig:seeds-ext}
\input{figures/filtration_exact.tikz}
\end{center}
\end{figure}

How many verifications we expect to have?
\begin{eqnarray}
p_\alpha = \frac{1}{\sigma} \text{ for all } \alpha \in \Sigma\\
\text{Pr}(H > 0) = \frac{1}{\sigma^q}\\
E(H) = \sum_{i=1}^{n-q+1}{\text{Pr}(H > 0)} = \frac{n - q + 1}{\sigma^q} \leq \frac{n}{\sigma^q}\\
E(V) = (k + 1) \cdot E(H) < \frac{n (k + 1)}{\sigma^q}
\end{eqnarray}

Which is the runtime of the algorithm?

How to choose the partitioning? Which length of $q$ makes filtration lossless?
$q=\left \lfloor \frac{m}{k+1} \right \rfloor$.


\subsubsection{$q$-Gram filters}

$q$-Gram filters are based on the so-called $q$-gram similarity measure $\tau_q : \Sigma^{*} \times \Sigma^{*} \rightarrow \N_0$, defined as the number of substrings of length $q$ common to two given strings.
The following lemma relates $q$-gram similarity to edit distance\footnote{Thus it relates $q$-gram similarity also to Hamming distance.}.
It gives a lower bound on the $q$-gram similarity $\tau_q(x,y)$ for any two strings $x,y$ for which $d_E(x,y) = k$.
This means that $\tau_q(x,y) \geq k$ is a necessary but not sufficient condition for $d_E(x,y) \leq k$.
\begin{lemma}
\citep{?}
Let $x,y$ be two strings with edit distance $k$ and $\min\{|x|,|y|\} = m$, then $x$ and $y$ have $q$-gram similarity $\tau_q(m,k) \geq m - q + 1 - kq$.
\end{lemma}
\begin{proof}
By induction on k.
\end{proof}

How can we use this result to solve approximate string matching?
The lemma itself does not give us the direct solution, indeed it considers the edit distance between two arbitrary strings, while in a.s.m. the pattern can match any substring of the text.
In the case of Hamming distance, if the pattern matches any substring $s$ of $t$, then $s$ must have length $m$.
In the case of edit distance, it must hold $m - k \leq |s| \leq m + k$.
The dot-plot representation helps us to visualize this concept.
Hamming distance occurrences cover one single diagonal of the dot-plot, while edit distance occurrences are enclosed inside a parallelogram of side $2k+1$.

Overlapping parallelograms?

We can design a filtration algorithm that scans the text and counts how many $q$-grams of the pattern falls into each parallelograms.
Only the parallelograms exceeding the threshold $\tau_q(m,k)$ have to be verified with an online method, \eg standard DP.
To speed up the filtration phase, the $q$-grams can be counted with the help of a substring index.

Which length of $q$ makes filtration lossless? $q=\left \lfloor \frac{m}{k+1} \right \rfloor$.

\begin{figure}[h]
\begin{center}
\caption{Filtration with $q$-grams.}
\label{fig:qgrams-ext}
\input{figures/filtration_qgrams.tikz}
\end{center}
\end{figure}


%\subsubsection{Approximate seeds}
%Filtration specificity in terms of candidate locations to verify is strongly correlated to seed length.
%Since we want to maximize the length of the shortest seed, we let the minimum seed length be $\lfloor |p|/(k+1) \rfloor$.
%If we want to improve filtration specificity by increasing seed length, we can resort to approximate seeds.
%A more involved filtering algorithm proposed in \citep{Navarro2000} reduces an approximate search into smaller approximate searches.
%We partition $p$ into $s \leq k+1$ non-overlapping seeds.
%According to the pigeonhole principle each approximate occurrence of $p$ in $t$ then contains an approximate occurrence of some seed within distance $\lfloor k/s \rfloor$.
%
%Approximate seeds can be searched via backtracking on $\Ti$.
%We search $(k \bmod{s}) + 1$ seeds within distance $\lfloor k/s \rfloor$ and the remaining seeds within distance $\lfloor k/s \rfloor - 1$.
%To prove full-sensitivity it suffices to see that, if none of the seeds occurs within its assigned distance, the total distance must be at least $s \cdot \lfloor k/s \rfloor + (k \bmod s) + 1 = k + 1$.
%Hence all approximate occurrences of $p$ in $t$ within distance $k$ will be found.


% === Online Methods ===

\chapter{Online Methods}
\section{Myers' bit-vector algorithm}
\section{Banded Myers' bit-vector algorithm}
\section{Increased bit-parallelism using SIMD instructions}

% === Indexed Methods ===

\chapter{Indexed Methods}

Suffix trees are elegant data structures but they are rarely used in practice.
Although suffix trees provides optimal construction and query time, their high space consumption prohibits practical applicability to large text collections.
A practical study on suffix trees \citep{Kurtz1999} considers efficient implementations achieving sizes between $12~n$ and $20~n$ bytes per character.
For instance, two years before completing the sequencing of the human genome, \citeauthor{Kurtz1999} conjectured the resources required for computing the suffix tree for the complete human genome (consisting of about $3 \cdot 10^9$~bp) in 45.31~GB of memory and nine hours of CPU time, and concluded that ``it seems feasible to compute the suffix tree for the entire human genome on some computers''.

We might be tempted to think that such memory requirements are not anymore a limiting factor as, at the time of writing, standard personal computers come with 32~GB of main memory.
Indeed, over the last decades, the semiconductors industry followed the exponential trends dictated by Moores' law and yielded not only exponentially faster microprocessors but also bigger memories.
Unfortunately, memory latency improvements have been more modest, leading to the so called memory wall effect \citep{?}: data access times are taking an increasingly fraction of total computation times.
Thus, if in \citeyear{Knuth1973} \citeauthor{Knuth1973} wrote that ``space optimization is closely related to time optimization in a disk memory'', forty years later we can deliberately say that space optimization is closely related to time optimization.

Over the last years, a significant effort has been devoted to the engineering of more space-efficient data structures to replace the suffix tree in practical applications.
In particular, much research has been done into designing succint or even compressed data-structure providing efficient query times using space proportional to that of the uncompressed or compressed input.
Thanks to this research, we are able to index the human genome in as little as 2.X~GB of memory and at the same time improve query time by a factor of X over classic indices!

In this chapter, we introduce some classic full-text indices (suffix arrays and $q$-gram indices) and subsequently succint full-text indices (our FM-index implementations) replacing suffix trees.
Afterwards we give approximate string matching algorithms working on any of these data structures.

\section{Classic Full-Text Indices}

\subsection{Suffix array}

The key idea of the suffix array \citep{Manber1990} is that most information explicitly encoded in a suffix tree is superfluous for pattern matching.
We can omit suffix tree's internal nodes and outgoing edges.
Indeed, leaves pointing to the sorted suffixes are sufficient to perform exact pattern matching or even trie traversals.
We can compute on the fly paths from the root to any internal node, via binary searches over the leaves.
We are thus willing to pay an additional logarithmic time complexity to reduce space by a linear factor.

\begin{definition}
The suffix array of a string $s$ of length $n$ is defined as an array $A$ containing a permutation of the interval $[1,n]$, such that $s_{A[i] \dots n} <_{lex} s_{A[i+1] \dots n}$ for all $1 \leq i < n$.
\end{definition}

\begin{figure}[h]
\caption{Suffix array for the string ANANAS.}
\label{fig:sa}
\begin{center}
\end{center}
\end{figure}

We can construct the suffix array in $\Oh(n)$ time, for instance using the \citep{Karkkainen2003} algorithm, or using non-optimal but practically faster algorithms \citep{?}.
The space consumption of the suffix array is $n \log{n}$ bits.
When $n < 2^{32}$, a 32 bit integer is sufficient to encode any value in the range $[1,n]$.
Consequently, the space consumption of suffix arrays for texts shorter than 4~GB is $4 n$ bytes.
For instance, we construct the suffix array of the human genome in about one hour on a modern computer and the suffix array itself fits in 12~GB of memory.

We now concentrate on replacing suffix tree functionalities. We replace algorithm~\ref{alg:st-exact} by algorithm~\ref{alg:sa-exact}.
The worst case runtime of algorithm~\ref{alg:sa-exact} is $\Oh(m \log{n})$, as the binary search consists of $\Oh(\log{n})$ steps, and each step is performed in $\Oh(m)$ time, as it requires in the worst case a full lexicographical comparison between the pattern and any suffix of the text.

As shown in \citep{Manber1990}, we can decrease the worst case runtime to $\Oh(m + \log{n})$ at the expense of additional $n \log{n}$ bits, by storing the precomputed longest common prefixes (LCP) between any two consecutive suffixes $s_{A[i]}$, $s_{A[i+1]}$ for all $1 \leq i < n$.
Alternatively, we can reduce the average case runtime to $\Oh(m + \log{n})$ without storing any additional information, by using the MLR heuristic \citep{Manber1990}.
In practice, the MLR heuristic outperforms the SA + LCP algorithm, due to the higher cost of fetching additional data from the LCP table.

\begin{algorithm}[h]
\caption{Exact string matching on a suffix array.}
\label{alg:sa-exact}
\begin{algorithmic}[1]
\algnotext{EndFor}
\Procedure{ExactSearch}{$\Tn,p$}
	\If {$p = \epsilon$}
		\State \Report $\Li(\Tn)$
	\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{figure}[h]
\begin{center}
\caption[Exact string matching on a suffix array.]{Exact string matching on a suffix array. The pattern NA is searched exactly in the text ANANAS.}
\label{fig:sa-exact}
%\input{figures/stree.tikz}
\end{center}
\end{figure}

\subsection{$q$-Gram index}

If we prune our idealized suffix tree to a fixed height $q$, we can improve again the query time over the suffix array.
The idea is to supplement the suffix array $A$ with an additional $q$-gram directory $D$ storing the suffix array ranges computed by algorithm~\ref{alg:sa-exact} for any possible word of length $q$.

With the aim of addressing $q$-grams in the directory $D$, we impose a canonical code on $q$-grams through a bijective function $h : \Sigma^q \rightarrow [1 \dots \sigma^q]$ defined as in \citep{Knuth19XX}:
\begin{eqnarray}
h(p) = 1 + \sum_{i=1}^{q}{\rho(p_i) \cdot \sigma^{q-i}}
\end{eqnarray}
where $p \in \Sigma^q$ is any $q$-gram and the function $\rho : \Sigma \rightarrow [0 \dots \sigma - 1]$ denotes the lexicographic rank of any symbol in the alphabet $\Sigma$.
This allows us to store in and retrieve from $D[h(p)]$, for each $q$-gram $p \in \Sigma^q$, the left suffix array interval returned by algorithm~\ref{alg:sa-lower}, \ie D[h(p)] = LowerSearch(p).
Note that the right interval returned by algorithm~\ref{alg:sa-upper} is equivalent to the left interval of the lexicographically following $q$-gram and therefore available in $D[h(p)+1]$.


\begin{figure}[h]
\caption{$q$-Gram index for the string ANANAS.}
\label{fig:qgram}
\begin{center}
\end{center}
\end{figure}

At this point, we are able to replace algorithm~\ref{alg:sa-exact} with algorithm~\ref{alg:qgram-exact}.
Algorithm~\ref{alg:qgram-exact} runs in $\Oh(q)$ time, but in practice the time to compute the function $h$ can be neglected and the lookup requires fetching only two memory locations from $D$.
The downside is that in practice this approach is applicable only for small alphabet and pattern sizes.
For instance, $|\Sigma| = 4$ and $q=14$ require a directory consisting of 268~M entries that, using a 32 bits encoding, consume 1~GB of memory.

If the patterns are shorter or equal to the fixed length $q$, we access the suffix array only to locate the occurrences, as the directory $D$ alone is sufficient to count.
In this case, the total ordering of the text suffixes in the suffix array can be relaxed to prefixes of length $q$.
This gives us a twofold advantage, as we can:
\begin{inparaenum}[(i)]
\item construct the suffix array more efficiently using bucket sorting and
\item maintain leaves in each bucket sorted by their relative text positions.
\end{inparaenum}
The latter property allows to compress the suffix array bucket-wise \eg using Elias $\delta$-coding \citep{?} or to devise cache-oblivious strategies to process the occurrences \citep{?}.

If the patterns are longer than $q$, the $q$-gram index is still useful.
We can devise an hybrid algorithm using the directory $D$ to conduct the search up to depth $q$ and later continue with binary searches.
This hybrid index cuts the most expensive binary searches and increases memory locality.
Furthermore, this hybrid index can be useful if the suffix array has to reside in external memory.

\begin{algorithm}[h]
\caption{Exact string matching on a $q$-gram index.}
\label{alg:qgram-exact}
\begin{algorithmic}[1]
\algnotext{EndFor}
\Procedure{ExactSearch}{$A,D,p$}
	\State \Report $A[D[h(p)], D[h(p)+1]]$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Succint Full-Text Indices}

The Burrows-Wheeler transform (BWT) \citep{Burrows1994} is a transformation defining a permutation of an input string.
The transformed string exposes two important properties: reversibility and compressibility.
The former property allows us to reconstruct the original string only from its BWT, the latter property makes the transformed string more amenable to compression.
Thanks to these two properties, the BWT has been recognized as a fundamental method for text compression and practically used in the bzip2 \citep{?} tool.

More recently, \citeauthor{Ferragina2000} proposed the BWT as a tool for full-text indexing.
They showed in \citep{Ferragina2000} that the BWT alone allows to perform exact pattern matching and engineered in \citep{Ferragina2001} a compressed full-text index called FM-index.
Over the last years, the FM-index has widely re-implemented and employed by many popular Bioinformatics tools \eg Bowtie \citep{Bowtie}, BWA \citep{BWA}, Soap2 \citep{Soap2}, and is now considered a fundamental method for the indexing of genomic sequences.

In the next subsections, we give the fundamental ideas behind the BWT and the FM-index.
Subsequently, we discuss our succint FM-index implementations covering texts and text collections.

\subsection{Burrows-Wheeler transform}

Let $t$ be a string of length $n$ over an alphabet $\Sigma$, terminated by a symbol $\$ \notin \Sigma$ such that $\$ <_{lex} c$ for all $c \in \Sigma$.
Consider the square matrix $M$ consisting of all cyclic shifts of the text $t$ (the $i$-th cyclic shift has the form $t_{i \dots n} t_{1 \dots i-1}$) sorted in lexicographical order.
Note how the matrix $M$ is related to the suffix array $A$ of $t$: the cyclic shift in the $i$-th row is $M[i:] = t_{A[i] \dots n} t_{1 \dots A[i-1]}$ (except when $A[i] = 1$).

\begin{definition}
The BWT of $t$ is the string $l$ obtained concatenating the symbols in the last column of the cyclic shifts matrix $M$, \ie $l = M[:n]$.
\end{definition}

%\subsubsection{Construction}

The matrix $M$ is conceptual. We do not have to construct it explicitly to derive the BWT of a text.
We can obtain the BWT in linear time by scanning the suffix array $A$ and assigning to the $i$-th BWT symbol the text character $t_{A[i]-1}$ (and when $A[i]=1$ the character $t_n$).
Various direct BWT construction algorithms have been recently proposed \citep{Bauer2013, Crochemore2013}, as constructing the suffix array is not desirable due to its space consumption of $n \log{n}$ bits.

%\subsubsection{Inversion}

We now describe how to invert the BWT to reconstruct the original text.
Inverting the BWT means being able to know where any BWT character occurs in the original text.
To this extent, we define two permutations $LF : [1,n] \rightarrow [1,n]$ and $\Psi : [1,n] \rightarrow [1,n]$, with $LF = \Psi^{-1}$, where the value of $LF(i)$ gives the position $j$ in $f$ where character $l_i$ occurs and the value $\Psi(j)$ gives back the position $i$ in $l$ where $f_j$ occurs.
We recover $t$ by starting in $f$ at the position of \$ and following the cycle defined by the permutation $\Psi$.
Or we recover the reverse text $\bar{t}$ by starting in $l$ at the position of \$ and following the cycle defined by the permutation $LF$.

We recover $t$ as follows:
\begin{eqnarray}
t_i = f_{\Psi^{i-1}(j)}
\end{eqnarray}
where 
\begin{eqnarray}
\Psi^0(j)=j\\
\Psi^{i+1}(j) = \Psi(\Psi^{i}(j))
\end{eqnarray}

\begin{example}
Recover $t$.\\
$l=$\\
$\Psi = (\dots)$
\end{example}

We recover $\bar{t}$ as follows:
\begin{eqnarray}
\bar{t}_i = t_{LF^{i-1}(j)}
\end{eqnarray}
where 
\begin{eqnarray}
LF^0(j)=j\\
LF^{i+1}(j) = LF(LF^{i}(j))
\end{eqnarray}

\begin{example}
Recover $\bar{t}$.\\
$l=$\\
$LF = (\dots)$
\end{example}

Again, the permutation $LF$ is conceptual. We do not have to explicitly store it but we can deduce it from the BWT $l$, with the help of some additional character counts.
This is possible due to two simple observations on the cyclic shifts of the matrix $M$ \citep{Burrows1994}:
\begin{itemize}
\item For all $i \in [1,n] \ I$, the character $l_i$ precedes the character $f_i$ in the original text $t$;
\item For all characters $c \in \Sigma$ the $i$-th occurrence of $c$ in $f$ corresponds to the $i$-th occurrence of $c$ in $l$.
\end{itemize}
Given the above observations, we define the permutation $LF$ as \citep{Burrows1994,Ferragina2000}:
\begin{eqnarray}
LF(i) = C(l_i) + Occ(l_i, i)
\end{eqnarray}
where we denote with $C : \Sigma \rightarrow [1,n]$ the total number of occurrences in $t$ of all characters alphabetically smaller than $c$, and with $Occ :  \Sigma \times [1,n] \rightarrow [1,n]$ the number of occurrences of character $c$ in the prefix $l_{1 \dots i}$.

The key problem of representing the permutation $LF$ is how to represent function $Occ$, as function $C$ can be easily tabulated by a small array of size $\sigma \log{n}$ bits.
In the next subsection we address the problem of representing function $Occ$ efficiently. Subsequently, in subsection~\ref{sub:fmi} we see how to build a full-text index out of function $LF$.

\subsection{Rank dictionaries}

We want to represent the function $Occ$ in succint space and at the same time answer efficiently the question: how many times a given character $c$ occurs in the prefix $l_{1 \dots i}$?
The general problem on arbitrary sequences has been tackled by several studies on the succint representation of data structures \citep{Jacobson1989, Munro1996, Clark1996, Raman2002}.
Our specific question takes the name of \emph{rank query} and a data structure answering rank queries is called \emph{rank dictionary}.

\begin{definition}
Given a sequence $s$ over an alphabet $\Sigma$ and a character $c \in \Sigma$, $rank_c(s, i)$ returns the number of occurrences of $c$ in the prefix $s_{1 \dots i}$.
\end{definition}

Rank dictionaries maintain a succint (or compressed) representation of the input sequence and attach a dictionary to it.
By doing so, it is possible to answer rank queries in constant time on the RAM model, using $n+o(n)$ bits for an input binary sequence of $n$ bits \citep{Jacobson1989}.
We start with a simple rank dictionary answering rank queries in constant time but taking $n \log{\sigma} + n \sigma$ bits of space.

First we consider the binary case $\Sigma_B = \{ 0, 1 \}$ and later we extend it to arbitrary alphabets.
Note that $rank_0(s, i) = i - rank_1(s, i)$ so we consider only $rank_1(s, i)$.
Given the binary sequence $s \in \Sigma_B$, we partition it in blocks of size $b=\log{n}$ bits.
We attach to the binary sequence $s$ an array $R$ of length $\frac{n}{b}$, where the $i$-th entry gives a summary of the number of occurrences of the bit $1$ in $s_{1 \dots i b}$, \ie $R[\frac{i}{b}] = rank_1(s, \frac{i}{b})$.
Therefore we are able to rewrite our rank query as:
\begin{eqnarray}
rank_1(s,i) = R[\frac{i}{b}] + rank_1(s_{\frac{i}{b} \dots \frac{i}{b}+b}, i \mod{b})
\end{eqnarray}
and answer it in constant time as 
\begin{inparaenum}[(i)]
\item \label{itm:fetch} we fetch in constant time the rank summary from $R$ and
\item \label{itm:count} we compute in constant time\footnote{On modern processors \citep{Intel} using the SSE~4.2 popcnt instruction, otherwise by means of the four-Russians tabulation technique \citep{Arlazarov1975}.} the number of occurrences of the bit 1 in the subsequence of length $\Oh(\log{n})$.
\end{inparaenum}
The array $R$ stores $\frac{n}{\log{n}}$ positions and each position in $s$ requires $\log{n}$ bits, so we are taking additional $n$ bits of space for $R$.

The extension to small alphabets, \eg $\Sigma_{\text{DNA}}$ is easy.
Given an input sequence $s \in \Sigma_{\text{DNA}}$ of length $n$ (and size $n \log{\sigma}$ bits), we partition it in blocks of $b=\frac{\log{n}}{\log{\sigma}}$ symbols.
We supplement each block with a summary of the occurrences of all symbols in $\Sigma$.
Thus, this time we use a matrix $R$ of $\sigma \times b$ entries.
Again, we answer $rank_c(s, i)$ in constant time by
\begin{inparaenum}[(i)]
\item \label{itm:fetch} fetching the summary from $R[rank(c)][\frac{i}{b}]$ and
\item \label{itm:count} adding the number of occurrences of the character $c$ inside the subsequence $s_{\frac{i}{b} \dots i}$,
\end{inparaenum}

%\subsubsection{Sequential dictionaries}
%\subsubsection{Wavelet tree}

\subsection{FM-index}
\label{sub:fmi}
\subsubsection{Backward search}
\subsubsection{Locating occurrences}

\section{Multiple backtracking}
\subsection{$k$-Mismatches}
\subsection{$k$-Differences}

% === Filtering Methods ===

\chapter{Filtering Methods}

\section{Gapped $q$-grams}

TODO.

%\subsection{Threshold computation}
%\subsection{Sensitivity computation}

\begin{figure}[h]
\begin{center}
\caption{Filtration with gapped $q$-grams.}
\label{fig:qgrams-gapped}
\input{figures/filtration_qgrams_gapped.tikz}
\end{center}
\end{figure}

\section{Approximate seeds}

TODO.

\begin{figure}[h]
\begin{center}
\caption{Filtration with approximate seeds.}
\label{fig:seeds-apx}
\input{figures/filtration_exact.tikz}
\end{center}
\end{figure}

\section{Suffix filters}

% === Related problems ===

\chapter{Related problems}

% --- Dictionary search ---

\section{Dictionary search}

Dictionary search is a restriction of string matching.
Given a set of database strings $\Dc$ and a query string $q$, the approximate dictionary search problem is to find all strings in $\Dc$ within distance $k$ from $q$.
Note that usually the query string $q$ has length similar to strings in $\Dc$, as $| |d| - |q| | \leq k$ is a necessary condition for $d_E(d,q) \leq k$.

\subsection{Online methods}

The problem can be solved by checking whether $d_E(d,q) \leq k$ for all $d \in \Dc$.
Answering the question whether the distance $d_E(d,q) \leq k$ is an easier problem than computing the edit distance $d_E(d,q)$: a band of size $k+1$ is sufficient.

\begin{lemma}
\label{lemma:kband}
%In contrast to the general global alignment problem, 
The $k$-differences global alignment problem can be solved by computing only a diagonal band of the DP matrix of width $k+1$, where the leftmost band diagonal is $\left\lfloor\frac{m-n+k}{2}\right\rfloor$ cells left of the main diagonal (see Figure~\ref{fig::BandedDP}).
\end{lemma}
\begin{proof}
Indirect. Assume that a cell outside the band is part of a global alignment with at most $k$ errors.
If the cell is left of the band, the traceback that starts in the top left corner would go down at least $c=\left\lfloor\frac{m-n+k}{2}\right\rfloor+1$ cells. Then it needs to go right at least $n-m+c$ cells to end in the bottom right corner.
Hence it contains at least $n-m+2c>n-m+2\frac{m-n+k}{2}=k$ errors.
The assumption that the cell is right of the band can be falsified analogously.
\end{proof}

\begin{figure}[h]
\begin{center}
\caption[Example of $k$-differences global aligment via DP.]{DP table representing the match of $p=...$ in $t=...$.}
\label{fig:dict-dp}
\input{figures/dp_banded.tikz}
\end{center}
\end{figure}

\subsection{Indexed methods}

Using a radix tree $\Di$ we can find all strings in $\Dc$ equal to a query string $q$, in optimal time $\Oh(|q|)$ and independently of $||\Dc||$.

\begin{algorithm}[h]
\caption{Exact dictionary search on a radix trie.}
\label{alg:dict-exact}
\begin{algorithmic}[1]
\algnotext{EndFor}
\Procedure{ExactSearch}{$\Tn,p$}
	\If {$p = \epsilon$}
		\State \Report $\Ei(\Tn)$
	\ElsIf {$\exists\,{\Cn \in \Ci(\Tn)}:\  label(\Cn) = p_1$\label{alg:dict-exact:comp}}
		\State \Call{ExactSearch}{$\Cn,p_{2..|p|}$}
	\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{figure}[h]
\begin{center}
\caption{Exact dictionary search on a suffix trie.}
\label{fig:dict-exact}
\input{figures/stree.tikz}
\end{center}
\end{figure}

\subsection{Filtering methods}

Filtering methods of section~\ref{sec:intro:filtering} can be directly applied to solve the dictionary search problem. Database strings satisfying the filtering condition can be verified with algorithm~\ref{?}.



% --- Local similarity search ---

\section{Local similarity search}

Define score and scoring scheme.

Define local similarity.

\subsection{Online methods}
Give dynamic programming solution.

\subsection{Indexed methods}
Backtracking over substring index. BWT-SW.

\subsection{Filtering methods}
SWIFT/Stellar is based on the $q$-gram lemma.
%Lastz resembles a suffix filter.


% --- Overlaps computation ---

\section{Overlaps computation}

Define problem.

\subsection{Online methods}

DP solution.

\subsection{Indexed methods}

Indexed solution, exact and approximate.

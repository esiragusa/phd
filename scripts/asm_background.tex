
\chapter{Stringology preliminaries}

We now introduce fundamental definitions and problems of stringology, in order to keep the manuscript self-contained.
We provide a summary of notations at the end of this manuscript.
The reader familiar with basic stringology can skip this chapter and proceed to chapter~\ref{chr:online}.

\section{Definitions}

Let us start by introducing primitive objects of stringology: alphabets and strings.
An \emph{alphabet} is a finite ordered set of symbols (or characters); a \emph{string} (or word) over an alphabet is a finite sequence of symbols from that alphabet.
We denote the length of a string $s$ by $|s|$, and by $\epsilon$ the empty string \st $|\epsilon|=0$.

\begin{definition}
Given an alphabet $\Sigma$, we define $\Sigma^0=\{ \epsilon \}$ as the set containing the empty string, $\Sigma^n$ as the set of all strings over $\Sigma$ of length $n$, and $\Sigma^* = \cup_{n=0}^{\infty}{\Sigma^n}$ as the set of all strings over $\Sigma$.
Finally, we call any subset of $\Sigma^*$ a language over $\Sigma$.
\end{definition}

\begin{definition}
We define the concatenation operator of two strings as $\cdot : \Sigma^* \times \Sigma^* \rightarrow \Sigma^*$.
Given two strings $x \in \Sigma^m$ with $x=x_1 x_2 \dots x_m$ and $y \in \Sigma^n$ with $y=y_1 y_2 \dots y_n$, their concatenation $x \cdot y$ (simply denoted $xy$) is the string $z \in \Sigma^{m+n}$ consisting of the symbols $x_1 x_2 \dots x_m y_1 y_2 \dots y_n$.
\end{definition}

%From concatenation we can derive the notion of prefix, suffix and substring.
\begin{definition}
A string $x$ is a \emph{prefix} of $y$ iff there is some string $z$ \st $y=x\cdot z$;
analogously, $x$ is a \emph{suffix} of $y$ iff there is some string $z$ \st $y=z\cdot x$;
moreover, $x$ is a \emph{substring} of $y$ iff there is some string $w,z$ \st $y=w\cdot x \cdot z$.
We denote by $y_{1 \dots i}$ the prefix of $y$ ending at position $i \in \N$, by $y_{i \dots n}$ (or simply $y_{i \dots}$) the suffix of $y$ beginning at position $i \in \N$, and by $y_{i \dots j}$ the substring of $y$ within positions $i \leq j \in \N$.
\end{definition}
% and then we say that $x$ occurs within $y$ at position $|w|$.

\begin{definition}
\label{def:rho}
Given an alphabet $\Sigma$ of size $\sigma$, we define the function $\rho : \Sigma \rightarrow [1 \dots \sigma]$ denoting the \emph{lexicographic rank} of any alphabet symbol, \st $\rho(a) < \rho(b) \iff a < b$ for any distinct $a,b \in \Sigma$.
\end{definition}

\begin{definition}
\label{def:lex}
We define the \emph{lexicographical order} $<_{lex}$ between two non-empty strings $x,y$ as $x <_{lex} y \iff x_1 < y_1, \text{ or } x_1 = y_1 \text{ and } x_{2 \dots} <_{lex} y_{2 \dots}$
\end{definition}

%Often in this manuscript we use the notion of string collection.
\begin{definition}
\label{def:colt}
We define a \emph{string collection} as an ordered multiset $\Sc = \{ s^1, s^2, \dots, s^c \}$ of non necessarily distinct strings over a common alphabet $\Sigma$.
We denote by $\norm{\Sc}=\sum_{i=1}^{c}{|s^i|}$ the total length of the string collection.
We also extend the notation of prefix, suffix and substring to multisets, \eg $\Sc_{(d,i) \dots (d,j)}$ denotes the substring $s^d_{i \dots j}$.
\end{definition}

\begin{definition}
\label{def:dollar}
We call \emph{terminator symbol} a symbol $\$ \not \in \Sigma$ \st $\rho(\$) < \rho(a)$ for any $a \in \Sigma$.
\end{definition}

\begin{definition}
\label{def:strd}
Given a string $s$ over $\Sigma$, we call \emph{padded string} the concatenation of $s$ with a terminator symbol \$.
\end{definition}

\begin{definition}
\label{def:coltd}
Given a string collection $\Sc$ over $\Sigma$, we call \emph{padded string collection} the collection consisting of strings $s^i \in \Sc$ padded with terminator symbols $\$^i$ \st $\rho(\$^i) < \rho(\$^j) \iff i < j$.
\end{definition}

%\begin{example}
%These definitions allow us to model basic biological sequences.
%Let us consider the alphabet consisting of DNA bases, $\Sigma_{DNA} = \{\text{A},\text{C},\text{G},\text{T}\}$.
%Examples of strings over $\Sigma_{DNA}$ are $x=$A, $y=$AGGTAC, $z=$TA.
%For instance, $y \in \Sigma_{DNA}^6$ and $|y| = 6$.
%Moreover, the concatenation $x \cdot z$ produces ATA.
%The string $x$ is a prefix of $y$, and the string $z$ is a substring of $y$  occurring at position 4 in $y$.
%\end{example}

\section{Alignments}

The next step is to define the minimal set of edit operations to transform one string into another: substitutions, insertions and deletions.
Given two strings $x,y$ of equal length $n$, the string $x$ can be transformed into the string $y$ by substituting (or replacing) all symbols $x_i$ \st $x_i \neq y_i$ into $y_i$, for $1 \leq i \leq n$.
If the given strings have different lengths, insertion and deletion of symbols from $x$ become necessary to transform it into $y$.
Therefore, given any two strings $x,y$, we define as edit transcript for $x,y$ any finite sequence of substitutions, insertions and deletions transforming $x$ into $y$.
See Figure~\ref{fig:edit-transcript} for an example.

\begin{figure}[h]
\begin{center}
\caption[Example of edit transcript]{Example of edit transcript transforming the string $x=AAAA$ into $y=CCCC$. The transcript character M indicates a match, R a replacement, I an insertion, and D a deletion.}
\label{fig:edit-transcript}
\input{figures/transcript.tikz}
\end{center}
\end{figure}

An alignment is an alternative yet equivalent way of visualizing a transformation between strings.
While an edit transcript provides an explicit sequence of edit operations transforming one string into another, an alignment relates pairs of corresponding symbols between two strings.
Because some symbols in one string are not related to any symbol in the other string, \ie some symbols are inserted or removed, we first need to introduce a gap symbol $-$, which is not part of the string alphabet $\Sigma$.
Subsequently, we can define the alignment of two strings of length $m,n$ over $\Sigma$ to be a string of length between $\min\{m,n\}$ and $m+n$ over the pair alphabet $(\Sigma \cup \{ - \}) \times (\Sigma \cup \{ - \})$.

\begin{example}
\label{ex:alignment}
An alignment of the strings $x=AAAA$ and $y=CCCC$ is given by the string
$z={\text{A} \choose \text{A}}{\text{A} \choose \text{-}}{\text{A} \choose \text{C}}{\text{G} \choose \text{G}}$
\end{example}

A dotplot is a way to visualize any alignment between two strings and highlight their similarities.
Given two string $x,y$ of length $m,n$, a dotplot is a $m \times n$ matrix containing a dot at position $(i,j)$ iff the symbol $x_i$ matches symbol $y_j$.
We define a dotplot trace to be a monotonical path in the matrix connecting non-decreasing positions of the matrix.
A dotplot trace corresponds to an alignment and vice versa.
In a trace, match and mismatch columns of the corresponding alignment appear as diagonal stretches, while insertions and deletions are horizontal or vertical stretches.
See Figure~\ref{fig:dotplot}.

\begin{figure}[h]
\begin{center}
\caption[Example of dotplot]{Example of dotplot of the strings $x=AAAA$ and $y=CCCC$. The highlighted trace corresponds to the alignment of example~\ref{ex:alignment}.}
\label{fig:dotplot}
\input{figures/dotplot.tikz}
\end{center}
\end{figure}

\section{Distance functions}

We can assign a cost to any alignment and to its associated edit transformation by defining a weight function $\omega : (\Sigma \cup \{ - \}) \times (\Sigma \cup \{ - \}) \rightarrow \R_0^{+}$, where:
\begin{itemize}
\item $\omega(\alpha,\beta)$ for all $(\alpha,\beta) \in \Sigma \times \Sigma$ defines the cost of substituting $\alpha$ with $\beta$,
\item $\omega(\alpha,-)$ for all $\alpha \in \Sigma$ defines the cost of deleting the symbol $\alpha$,
\item $\omega(-,\beta)$ for all $\beta \in \Sigma$ defines the cost of inserting the symbol $\beta$,
\end{itemize}
and by defining the total cost $C(z)$ of an alignment $z$ between two strings as the sum of the weights of all its alignment symbols:
\begin{eqnarray}
C(z) = \sum_{i=0}^{|z|}{\omega(z_i)}
\end{eqnarray}
Consequently, we can define the distance function $d : \Sigma^{*} \times \Sigma^{*} \rightarrow \R_0^{+}$ by taking the minimum cost over all possible alignments of $x,y$:
\begin{eqnarray}
d(x,y) = \sum_{z \in \mathbb{A}(x,y)}{C(z)}
\end{eqnarray}

In particular, the edit or \emph{Levenshtein distance} between two strings $x,y \in \Sigma^{*}$ is defined as the function $d_E : \Sigma^{*} \times \Sigma^{*} \rightarrow \N_0$ counting the \emph{minimum} number of edit operation necessary to transform $x$ into $y$.
It is obtained by defining for all $(\alpha,\beta) \in \Sigma \times \Sigma$, $\omega(\alpha,\beta) = 1$ iff $\alpha \neq \beta$ and 0 otherwise, and $\omega(\alpha,-)$ and $\omega(-,\beta)$ as 1.
The \emph{Hamming distance} between two strings $x,y \in \Sigma^{n}$ is defined as the function $d_H : \Sigma^{n} \times \Sigma^{n} \rightarrow \N_0$ counting the number of substitutions necessary to transform $x$ into $y$.
We obtain it by defining $\omega(\alpha,\beta)$ as in the edit distance, and by setting all $\omega(\alpha,-)$ and $\omega(-,\beta)$ to be $\infty$ in order to disallow indels.

\begin{example}
TODO: example of edit and hamming distance.
\end{example}

\section{Optimal alignments}

The problem of finding an optimal alignment between two strings is equivalent to the problem of finding their minimum distance \citep{Gusfield1997}.
A solution to this optimization problem can be efficiently computed via dynamic programming (DP).
Below we describe the three essential components of the DP approach: the recurrence relation, the DP table, and the traceback.

Given two strings $x,y$ of length $m,n$, for all $1 \leq i \leq m$ and $1 \leq j \leq n$ we define with $d(x_{1..i},y_{1..j})$ the distance between their prefixes $x_{1..i}$ and $y_{1..j}$.
The base conditions of the recurrence relation are:
\begin{eqnarray}
d(\epsilon,\epsilon)&=&0\\
d(x_{1..i},\epsilon)&=&\sum_{l=1}^{i}{\omega(x_l, -)} \text{ for all } 1 \leq i \leq m\\
d(\epsilon, y_{1..j})&=&\sum_{l=1}^{j}{\omega(-, y_l)} \text{ for all } 1 \leq j \leq n
\end{eqnarray}
and the recursive case is defined as follows:
\begin{eqnarray}
d(x_{1..i},y_{1..j}) = \min \left\{
\begin{array}{lcl}
d(x_{1..i-1},y_{1..j})&+&\omega(x_i, -)\\
d(x_{1..i},y_{1..j-1})&+&\omega(-, y_j)\\
d(x_{1..i-1},y_{1..j-1})&+&\omega(x_i, y_j)
\end{array}
\right.\label{eq:dp-min}
\end{eqnarray}

We can compute the above recurrence relation in time $\Oh(nm)$ using a dynamic programming table $D$ of $(m+1) \times (n+1)$ cells, where cell $D[i,j]$ stores the value of $d(x_{1..i},y_{1..j})$.
The sole distance without any alignment can be computed in space $\Oh(\min\{ n, m \})$, as we only need column $D[:j-1]$ to compute column $D[:j]$ (or row $D[i-1:]$ to compute $D[i:]$) and we can fill the table $D$ either column-wise or row-wise\footnote{Note that $D$ can be filled also diagonal-wise or antidiagonal-wise.}.
An optimal alignment can be computed in time $\Oh(m + n)$ via \emph{traceback} on the table $D$:
We start in the cell $D[m,n]$ and go backwards (either left, up-left, or up) to the previous cell by deciding which condition of equation~\ref{eq:dp-min} yielded the value of $D[m,n]$.

\begin{figure}[h]
\begin{center}
\caption[Example of DP table]{DP table representing the computation of the edit distance $d_E(x_{1..5}, y_{1..4})$.}
\label{fig:edit-dp}
\input{figures/dp_edit.tikz}
\end{center}
\end{figure}


% === Overview of existing methods ===

\section{String matching}

%\subsection{Problem definition}

We now define \emph{exact string matching}, one of the most fundamental problem in stringology.
\begin{definition}
\citep{Gusfield1997}
Given a string $p$ of length $m$, called the \emph{pattern}, and a string $t$ of length $n$, called the \emph{text}, the exact string matching problem is to find all occurrences of pattern $p$ into text $t$.
\end{definition}

This problem has been extensively studied from the theoretical standpoint and is well solved in practice \citep{Faro2013}.
Nonetheless, the definition of distance functions between strings let us generalize exact string matching into a more challenging problem: \emph{approximate string matching}.

\begin{definition}
Given a text $t$, a pattern $p$, and a \emph{distance threshold} $k \in \N$, the approximate string matching problem is to find all occurrences of $p$ into $t$ within distance $k$.
\end{definition}
The approximate string matching problem under the Hamming distance is commonly referred as the \emph{$k$-mismatches} problem and under the edit distance as the \emph{$k$-differences} problem.
For $k$-mismatches and $k$-differences, it must hold $k > 0$ as the case $k = 0$ corresponds to exact string matching, and $k < m$ as a pattern occurs at any position in the text if we substitute all its $m$ characters.
\begin{definition}
Under the edit or Hamming distance, we define the \emph{error rate} as $\epsilon = \frac{k}{m}$, with $0 < \epsilon < 1$ given the above conditions.
\end{definition}

String matching problems are subdivided in two categories, \emph{online} and \emph{offline}, depending on which string, the pattern or the text, is given first.
Algorithms for online string matching work by preprocessing the pattern and scanning the text from left to right (or right to left).
Algorithms for offline string matching are instead allowed to preprocess the text,
hence they build an index of the text beforehand to speed up subsequent searches.
In practice, if the text is long, static and searched frequently, provided the necessary amount of memory for text indexing, offline methods outperform online methods in terms of runtime.

It goes without saying that offline string matching algorithms are tightly bound to text indexing data structures.
Almost all of these algorithms require a \emph{full-text index}, \ie a data structure representing all substrings of the text.
Very often, such \emph{full-text index} is the \emph{suffix tree} \citep{Weiner1973}, a fundamental data structure in stringology.
Among its virtues \citep{Apostolico?}, the suffix tree natively provides exact string matching in optimal time and also approximate string matching via backtracking \citep{Ukkonen?}.
Often the suffix tree finds its use within hybrid \emph{filtering methods} rather than on its own.

Filtering methods first discard uninteresting portions of the text and subsequently use a conventional method to verify only narrow areas.
Filtering methods work either online or offline.
Methods of the former category try to jump over the text while scanning it, while those in the latter category use an index to place anchors in the text.
Any filtering method then requires an online verification method.
Filtering methods outperform \emph{native} online and indexed methods for a vast range of inputs, \eg when the error rate is low, and are thus very appealing from a practical standpoint.
Nonetheless, filtering methods are just opportunistic combinations of native online and indexed methods.

%A last distinction can be made between single and multiple string matching algorithms.

In the following of this section, I give a quick overview of the fundamental algorithms and data structures adopted in string matching methods.
This overview serves as an introduction to the indexed and filtering methods presented in the next chapters.
For an extensive treatment of this subject, the reader is referred to complete surveys on online \citep{Navarro2001b} and indexed \citep{Navarro2001} approximate string matching methods.

% --- Online methods ---

\subsection{Online methods}
\label{sub:introonline}

%Exact search of one pattern. Knuth-Morris-Pratt automaton.
%Exact search of multiple patterns. Aho-corasick automaton.

%The most important category of algorithms for approximate string matching are based on dynamic programming.
%DP bit-parallelism.

%Other algorithms for approximate string matching are based on automata \citep{Navarro2001b}.

%\subsubsection{Dynamic programming}

The dynamic programming algorithm~\ref{?} to compute the distance of two strings can be easily turned into a string matching algorithm.
Since an occurrence of the pattern can start and end anywhere in the text, $k$-differences consists of computing the edit distance between the pattern and any substring of the text.
The problem can be thus solved by computing the edit distance between the text and the pattern without penalizing leading and trailing deletions in the text.

Consider equation~\ref{eq:dp-min} and pose $x=p$ and $y=t$.
Since an occurrence of the pattern can start anywhere in the text, we change the initialization of the top row $D[0:]$ of the DP matrix according to the condition:
\begin{eqnarray}
d(\epsilon, y_{1..j}) = 0 \text{ for all } 1 \leq j \leq n
\end{eqnarray}
and since an occurrence of the pattern can end anywhere in the text, we check all cells $D[m,j]$ for all $1 \leq j \leq n$ in the bottom row of $D$ for the condition $D[m,j] \leq k$.

\begin{figure}[h]
\begin{center}
\caption[Example of approximate string matching via DP]{DP table representing the match of $p=...$ in $t=...$.}
\label{fig:asm-dp}
\input{figures/dp_asm.tikz}
\end{center}
\end{figure}

% --- Indexed methods ---

\subsection{Indexed methods}
\label{sub:introindex}

%Indexed methods are more attractive than online methods when several patterns are to be searched in the same static text.
%These methods work by building an index of the text beforehand to speed up subsequent searches.
%To this intent,
Here I introduce \emph{suffix tries}, idealized data structures to index full-texts.
Moreover, I define a set of generic operations to traverse suffix tries in a top-down fashion.
In chapter~\ref{chr:index}, I will expose various data structures replacing suffix tries in practice.
The generic traversal operations here introduced lead to the formulation of generic algorithms solving string matching problems on any of these data structures.

\subsubsection{Trie}

Consider a padded string collection $\Sc$ (definition~\ref{def:coltd}) consisting of $c$ strings.
Note that padding is necessary to ensure that no string $s^i \in \Sc$ is a prefix of another string $s^j \in \Sc$.
\begin{definition}
The trie $\Si$ of $\Sc$ is a lexicographically ordered tree data structure having one node designated as the root and $c$ leaves, denoted as $\Ln_1 \dots \Ln_c$, where leaf $\Ln_i$ points to string $s^i$.
The entering edge of any node of $\Si$ is labeled with a symbol in $\Sigma$, while the entering edge of any leaf of $\Si$ is labeled with a terminator symbol.
Any path from the root to a leaf $\Ln_i$ spells the string $s^i$, including its terminator symbol $\$^i$.
%Figure~\ref{fig:trie} illustrates.
\end{definition}

\subsubsection{Suffix trie}

We define the suffix trie as the trie of all suffixes of a string.
\begin{definition}
Given a padded string $s$ (definition~\ref{def:strd}) of length $n$, the suffix trie $\Si$ of $s$ has $n$ leaves where leaf $\Ln_i$ points to suffix $s_{i..n}$.
\end{definition}
We generalize the suffix trie to index string collections.
\begin{definition}
Given a padded string collection $\Sc$ (definition~\ref{def:coltd}), any leaf of the \emph{generalized suffix trie} is labeled with a pair $(i,j)$ where $i$ points to the string $s^i \in \Sc$ and $1 \leq j \leq n_i$ points to one of the $n_i$ suffixes of $s^i$.
Thus each path from the root to a leaf $(i,j)$ spells the suffix $\Sc_{(i,j) \dots}$.
\end{definition}
%Figure~\ref{fig:stree} illustrates.

Note that the suffix trie so defined contains $\Oh(n^2)$ nodes, yet we only consider the suffix trie as an idealized data structure to expose our algorithms.
The optimal data structure to represent in linear space all suffixes of a given string is the \emph{suffix tree} \citep{Morrison1968}.
However, the suffix tree comes with the restriction that internal nodes must have more than one child and with the property that edges can labeled by strings of arbitrary length (see Figure~\ref{fig:stree}).
This latter property slightly complicates the exposition of our string matching algorithms but it does not affect their runtime complexity nor their result.
For this reason, in the following of this manuscript we always consider \wlogs suffix tries instead of suffix trees.
%We remark that all given algorithms can be generalized to work on trees.

\begin{figure}[h]
\caption[Example of suffix trie and suffix tree]{Suffix trie and suffix tree for the string ANANAS\$.}
\label{fig:stree}
\begin{subfigure}{.5\textwidth}
\begin{center}
\input{figures/strie_banana.tikz}
\end{center}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\begin{center}
\input{figures/stree_banana.tikz}
\end{center}
\end{subfigure}
\end{figure}

\begin{figure}[h]
\caption[Example of generalized suffix trie]{Generalized suffix trie for the string collection $\Sc = \{$ {\ttfamily ANANAS$\$_1$}, {\ttfamily CACAO$\$_2$} $\}$.}
\label{fig:gstrie}
\begin{center}
\input{figures/gstrie.tikz}
\end{center}
\end{figure}

\subsubsection{Top-down traversal}

We now give a set of generic operations to traverse tries in a top-down fashion.
Given a trie $\Ti$, we define the following operations inspecting any pointed node $\Tn$ of $\Ti$:
\begin{itemize}
\item \textsc{isRoot}($\Tn$) returns true iff the pointed node is the root;
\item \textsc{isLeaf}($\Tn$) returns true iff all outgoing edges are labeled by terminator symbols;
\item \textsc{label}($\Tn$) returns the symbol labeling the edge entering $\Tn$;
\item \textsc{occurrences}($\Tn$) returns the list of positions pointed by leaves below $\Tn$;
\end{itemize}
and the following operations moving from pointed node $\Tn$, and returning true on success and false otherwise:
\begin{itemize}
\item \textsc{goDown}($\Tn$) moves to the lexicographically smallest child of $\Tn$;
\item \textsc{goDown}($\Tn, c$) moves to the child of $\Tn$ whose entering edge is labeled by $c$;
\item \textsc{goRight}($\Tn$) moves to the lexicographically next child of $\Tn$;
\item \textsc{goUp}($\Tn$) moves to the parent node of $\Tn$.
\end{itemize}

Time complexities of the above operations depend on the data structure implementing the trie.
Usually \textsc{label} is $\Oh(1)$, both variants of \textsc{goDown} and \textsc{goRight} can be $\Oh(1)$ or logarithmic in $n$, \textsc{occurrences} can be linear in the number of occurrences, \textsc{goUp} is $\Oh(1)$ but with an additional $\Oh(n)$ space complexity to stack all parent nodes.
Note that is always possible to implement \textsc{goDown} and \textsc{goRight} by relying on \textsc{goDown} a symbol and \textsc{goUp}, but their complexity becomes $\Oh(\sigma)$.
In chapter~\ref{chr:index}, we consider various data structures to implement suffix tries, we show how to implement these operations and give their complexities.

\begin{figure*}
\begin{minipage}[t]{.5\textwidth}
\begin{algorithm}[H]
\Algorithm{goDown}{$\Tn$}
\begin{tabular}{ll}
\textbf{Input}  & $\Tn$ : pointer to a suffix trie node\\
\textbf{Output} & boolean indicating success\\
\end{tabular}
\begin{algorithmic}[1]
\If {\Call{isLeaf}{$\Tn$}}
	\State \Return \False
\EndIf
\If {\Call{goDown}{$\Tn, \min_{lex}{\Sigma}$}}
	\State \Return \True
\Else
	\State \Return $\Call{goRight}{$\Tn$}$		
\EndIf
\end{algorithmic}
\label{alg:st-godown}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{.5\textwidth}
\begin{algorithm}[H]
\Algorithm{goRight}{$\Tn$}
\begin{tabular}{ll}
\textbf{Input}  & $\Tn$ : pointer to a suffix trie node\\
\textbf{Output} & boolean indicating success\\
\end{tabular}
\begin{algorithmic}[1]
\If {\NotB \Call{isRoot}{$\Tn$}}
	\While {$c \gets \text{next}_{lex}{\Sigma,\Call{label}{\Tn}}$}
		\State {$\Call{goUp}{x}$}
		\If {$\Call{goDown}{x, c}$}
			\State \Return \True
		\EndIf
	\EndWhile
\EndIf
\State \Return \False
\end{algorithmic}
\label{alg:st-goright}
\end{algorithm}
\end{minipage}
\end{figure*}


% --- Filtering methods ---

\subsection{Filtering methods}
\label{sec:intro:filtering}

The goal of filtering methods is to obtain algorithms with favorable average running times.
The principle under which they work is that large and uninteresting portions of the text can be quickly discarded, while only narrow and highly similar portions have to be verified with a conventional online method.
Any filtering method can either work in a online fashion or take advantage of an index of the text to speed up the filtration phase.

Filtering methods work under the assumption that given patterns occur in the text with a \emph{low average probability}.
Such probability is a function of the error rate $\epsilon$, in addition to the alphabet size $\sigma$, and can be computed or estimated under the assumption of the text being generated by a specific random source.
Under the uniform Bernoulli model, where each symbol of $\Sigma$ occurs with probability $\frac{1}{\sigma}$, \citet{Navarro2000} estimates that $\epsilon < 1 - \frac{1}{\sigma}$ is a conservative bound on the error rate which ensures few matches, and for which filtering algorithms are effective.
For higher error rates, non-filtering online and indexed methods work better.

\subsubsection{Lossless versus lossy filters}

A filter is said to be \emph{lossless} or \emph{full-sensitive} if it guarantees not to discard any occurrence of the pattern in the text, otherwise it is called \emph{lossy}.
%Lossy filters can be designed to solve approximately $\epsilon$-mismatches or $\epsilon$-differences.
We focus our attention on lossless filters.

\subsubsection{Seeds versus $q$-grams}

I consider two classes of filtering methods: those based on \emph{seeds} and those based on \emph{$q$-grams}.
Filters based on seeds partition the pattern into \emph{non-overlapping} factors called seeds.
Full-sensitive partitioning strategies are derived by applying the pigeonhole principle.
Instead, filters based on $q$-grams consider all \emph{overlapping} substrings of the pattern having length $q$, the so-called $q$-grams.
A simple lemma gives a lower bound on the number of $q$-grams that must be present in a narrow window of the text as necessary condition for an approximate occurrence of the pattern.

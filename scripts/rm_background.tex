% =============================================================================

\chapter{Background}
\label{chr:background}

In this chapter, I provide the reader with background knowledge in the fast-moving field of high-throughput sequencing (HTS).
It goes without saying that the reader familiar with HTS can skip this chapter.
In section suboptimal\ref{sec:background:sequencing}, I briefly introduce the two most prominent HTS technologies and the kind of sequencing data they produce.
Afterwards, in section \ref{sec:background:analisys}, I explain how standard HTS data \emph{analysis pipelines} are structured;
in particular, in section \ref{sec:background:paradigms}, I present two popular paradigms for reference-guided assembly: best-mapping and all-mapping.
In section \ref{sec:mappability}, I review two studies on the limits of HTS data analysis.
Finally, in section \ref{sec:background:mappers}, I give an overview of the most popular read mapping tools.

\section{High-throughput sequencing data}
\label{sec:background:sequencing}

As anticipated in section \ref{sec:intro:hts}, actual HTS technologies produce DNA reads which are shorter than Sanger sequencing and more likely to contain systematic sequencing artefacts.
HTS data does not consist of read sequences only, but includes also base quality scores annotating the quality of the sequencing process.

\subsection{Read sequences}

I consider only sequencing data produced by the most prominent HTS instruments.
\emph{Illumina} is the actual market leader for HTS instruments, followed by \emph{Life Technologies}'s \emph{Ion Torrent}.
Some instruments, \eg the \emph{GS} by \emph{454 Life Sciences} or the \emph{SOLiD} by \emph{Applied Biosystems}, became popular around 2006--2010, but are now discontinued.
Other \emph{third-generation} instruments, like the \emph{single-molecule real-time} sequencing (SMRT) \emph{RS~II} by \emph{Pacific Biosciences}, have great potential but still low impact on the HTS market.

\subsubsection{Illumina}

All Illumina instruments use the \emph{sequencing by synthesis} (SBS) technology \citep{Bentley2008} developed by Solexa.
In the library preparation phase, the DNA sample is sheared into smaller fragments.
During the sequencing phase, single-stranded DNA fragments are attached on a slide called \emph{flow cell} and amplified \emph{in situ} using a variant of \emph{polymerase chain reaction} (PCR) called \emph{bridge amplification}.
Clusters of amplified fragments are then used as templates for multiple cycles of synthetic sequencing with four differentially labeled fluorescent reversible terminator \emph{deoxyribonucleotide triphosphates} (dNTPs).
In each sequencing cycle, dNTPs are incorporated into the fragments in each cluster, their corresponding fluorescent reversible terminators are imaged by a high-resolution camera, and then dNTPs are cleaved to allow incorporation of the next base.

After the sequencing phase, a base calling software converts all images corresponding to one cluster into one read.
The software measures the intensities of the four colors imaged during the $i$-th cycle, calls the $i$-th read base, and assigns a base quality score.
Eventual replication errors, made by DNA polymerase during bridge amplification, result in mixed signal intensities within a cluster, and hence to base calls of lower confidence.

Illumina's SBS technology further allows the sequencing of both ends of each DNA fragment.
Two kind of libraries are available: \emph{paired-end} libraries, consisting of reads from short-insert 300--600~bp fragments, and \emph{mate pair} libraries, consisting of reads separated by several kilobases.
The former libraries are adopted for high-resolution genome resequencing, while the latter ones provide accurate \emph{de novo} sequence assembly or detection of large-scale structural variation.

%\emph{Genome Analyzer IIx}, \emph{HiSeq 2000}, \emph{MiSeq}, HiSeq~X 10.

\subsubsection{Ion Torrent}

\emph{Life Technologies}'s \emph{Ion Torrent} instruments use a semiconductor-based technology \citep{Rusk2010}.
During library preparation, the sheared single-stranded template DNA is embedded into \emph{microwells} on a semiconductor chip with DNA polymerase.
Microwells are sequentially flooded with unmodified A, C, G or T dNTP.
The DNA polymerase incorporates the introduced dNTP into the growing strand only if this is complementary to the leading template nucleotide.
Such polymerization reaction releases an hydrogen ion, which in turn changes the pH of the solution.
A \emph{ion-sensitive field-effect transistor} (ISFET) in the sequencing instrument measures this pH change.
Any \emph{homopolymer} in the template causes multiple dNTP to be incorporated in a single cycle and results in a higher pH change, which is not precisely measured by the instrument and thus causes systematic sequencing errors.

%Ion Torrent \emph{Personalized genome machine} (PGM) Ion Torrent \emph{Proton}

%\subsubsection{Other technologies}

%454 Life Sciences by Roche.

%ABI/SOLiD by Life Technologies.
%\emph{Applied Biosystems} (ABI) entered the market with its \emph{SOLiD} system, based on parallel sequencing by \emph{stepwise ligation}, reading two bases at a time with a florescent label in order to improve sequencing accuracy.

%RS~II by Pacific Biosciences uses \emph{single-molecule real-time} sequencing (SMRT).
%Pacific Biosciences introduced in 2010 a single molecule real time (SMRT) sequencer that enables sequencing a contiguous piece of length Å1500 bp of a single molecule without prior amplification. The fundamental idea is to immobilize DNA polymerase and to film the incorporation of fluorescently labeled nucleotides in real time. As the sequencing is not cycled, the base-calling cannot accurately determine the length of homopolymer runs which must be inferred from signal lengths. However, this new approach permits sequencing reads of length similar to first generation sequencing and promises to detect methylated bases from deviations in the signal length.

\subsection{Phred base quality scores}
\label{sec:background:hts:phred}

Base quality scores have been introduced by the base calling tool \emph{Phred} \citep{Ewing1998, Ewing1998a} to assess sequencing quality of single bases in capillary reads.
Instead of discarding low-quality regions present in capillary reads, Phred output each base and annotates it with the probability that it has been wrongly called.
The tool encodes the probability $\epsilon_i$ of miscalling the $i$-th base in a read under the form of a base quality $Q_i$ in logarithmic space:
\begin{eqnarray}
\label{eq:phred}
Q_i = -10 \log_{10} \epsilon_i.
\end{eqnarray}
This method has been unanimously accepted.
All sequencing technologies complement DNA reads with base quality scores, encoded in Phred-scale or similar.


% =============================================================================

\section{High-throughput sequencing data analysis}
\label{sec:background:analisys}

\subsection{Data analysis pipelines}
\label{sec:background:pipeline}

The analysis of HTS data consists of numerous steps, ranging from the initial instrument-specific data processing to the final application-specific interpretation of the results.
It is conventional wisdom to subdivide such data analysis pipelines in three stages of analysis.
The \emph{primary stage} of analysis consists of instrument specific steps for the generation, collection, and processing of raw sequencing signals.
The \emph{secondary stage} applies sequence analysis methods to the raw sequencing data in order to reconstruct the original sequence of the donor genome.
The \emph{tertiary stage} characterizes application-specific features of the donor genome and then provides interpretations, \eg in whole exome sequencing (WES) it consists of calling genetic variations and predicting their phatogenicity.
Below, I give more 

\subsubsection{Primary analysis}
\label{sec:background:pipeline:primary}

Primary analysis consists of instrument specific steps to call base pairs and compute quality metrics.
The base calling software converts raw sequencing signals into bases, \ie A, C, G, T, or N if the signal is unclear.
The software assigns a quality value to each called base, estimating the probability of a base calling error.
On early generation instruments, users could provide their own base calling tool.
Now this process happens automatically on special hardware (\eg FPGAs or GPUs) bundled within the instrument.
The result of primary analysis is a standard \emph{FASTQ} file containing DNA read sequences and their associated quality scores in \emph{Phred} format (see section \ref{sec:background:hts:phred}).

\subsubsection{Secondary analysis}

Secondary analysis aims at reconstructing the original sequence of the donor genome from its HTS reads.
There are two main \emph{plans} to reassemble the original genome: \begin{inparaenum}[(A)]
\item \emph{de novo} assembly, and
\item \emph{reference-guided} assembly (commonly called \emph{read mapping}).
\end{inparaenum}
\emph{De novo} assembly is very involved as it essentially requires finding a \emph{shortest common superstring} (SCS) of the reads, which is a NP-complete problem \citep{Maier1977,Gallant1980,Turner1989}.
Computational methods in plan A first scaffold the reads by performing \emph{overlap alignments} \citep{Myers2005} or equivalently by constructing \emph{de Bruijn graphs} \citep{Pevzner2001}.
The knowledge of a \emph{reference genome}, highly similar to the donor, simplifies the problem and opens the door to plan B.
The reads are simply aligned to the reference, tolerating a few base pair errors.
It is worth mentioning that plans A and B can be combined, \eg as proposed by \cite{Li2012}.

Plan B is always preferred in resequencing projects, as it is computationally more viable than plan A and directly provides a way to assess genetic variation \wrt a reference genome.
In this work, I consider only plan B and present methods that work within this specific plan.
Nonetheless, many of the algorithmic components that I introduced in the first part of this manuscript are ubiquitous in bioinformatics, thus applicable, if not already applied, to plan A.
According to plan B, the secondary analysis stage consists of three steps: \emph{quality control}, \emph{read mapping}, and \emph{consensus alignment}.

In the \emph{quality control} (Q/C) step, the quality of raw reads is checked.
Reads produced by current HTS technologies contain sequencing artefacts in form of single bases or stretches of miscalled oligonucleotides.
In order to circumvent this problem, various techniques have been developed, ranging from the simple \emph{trimming} of low quality read stretches to sophisticated methods of \emph{read error correction} \citep{Weese2013a}.
Sometimes, Q/C is simply omitted.

In the \emph{read mapping} step, the reads are aligned to the reference genome.
Read mapping tools adopt state of the art \emph{approximate string matching} methods to efficiently analyze the deluge of data produced by HTS instruments.
Approximate matching accounts for two kinds of errors:
residual sequencing artefacts not removed by Q/C and small genetic variations in the donor genome.
Consequently, a read mapper must take into account these errors when aligning the reads.

Mapped reads, often stored in de-facto standard BAM files \citep{Li2009a}, are sorted by genomic coordinate and eventually multiply aligned in order to construct a \emph{consensus sequence} of the donor genome.
The height of the pileup denotes the sequencing depth or coverage at each locus in the donor genome;
%, thus the average height corresponds to the average sequencing coverage;
higher coverage implies more confidence in the consensus sequence of the donor genome and thus more accuracy in the tertiary analysis stage.

\subsubsection{Tertiary analysis}

Tertiary analysis aims at interpreting the information provided by the secondary analysis stage.
This pipeline stage groups a wide range of analyses specific to the sequencing application.
In some pipelines, downstream analysis aggregates data coming from multiple samples.

Within DNA resequencing, \emph{genotyping} consists of determining the variations between the donor and the reference genome.
The result of \emph{variant calling} is a set of variations characterizing the donor genome, usually stored in de-facto standard VCF files.
Subsequently, \emph{genome-wide association studies} (GWAS) associate genetic variants with phenotypic traits by examining \emph{single-nucleotide polymorphisms} (SNPs), variants relatively common among individuals of the same population.

In RNA-seq, tertiary analysis consists of computing transcripts abundance by measuring \emph{reads per kilobase per million mapped reads} (RPKM) \citep{Mortazavi2008}; subsequently, relative gene expression is determined by comparing multiple RNA samples.
In ChIP-seq, this stage consists of calling peaks in correspondence of mapped reads, determining which peaks identify feasible transcription factor binding sites, then interesting sites affecting gene regulation.

\subsection{Secondary analysis paradigms}
\label{sec:background:paradigms}

The goal of secondary analysis is to reconstruct the original sequence of the donor genome.
The \emph{reference-guided} assembly plan assumes prior knowledge of a reference genome which is close the donor genome.
Reads are thus mapped (\ie aligned) to the reference genome \wrt a given scoring scheme and threshold.
The scoring scheme accounts for eventual genomic variation, as well as for any sequencing artefacts.
Under these assumptions, an alignment of optimal score for a read implies its \emph{original location} on the reference genome.
Conversely, no alignment within the score threshold implies too many sequencing errors, too much genetic variation, or sample contamination.
%A pile-up of the reads aligned at their mapping locations thus provides a consensus alignment of the donor genome.
A problem arises in presence of co-optimal or close suboptimal alignments: the read cannot be mapped confidently to one single location.

The problem of confidently mapping high-throughput sequencing reads comes from the non-random nature of genomic sequences.
Genomes evolved through multiple types of duplication events, including whole-genome duplications \citep{Wolfe1997,Dehal2005} or large-scale segmental duplications in chromosomes \citep{Bailey2001,Samonte2002}, transposition of repetitive elements as short tandem repeats (microsatellites) \citep{Wang1994a,Wooster1994} and interspersed nuclear elements (LINE, SINE) \citep{Smit1996}, proliferation of repetitive structural elements such as telomeres and centromeres \citep{Meyne1990}.
As a result of these events, for instance, about 50~\% of the human genome is composed of repeats.

Repeats present in general technical challenges for all \emph{de novo} assembly and sequence alignment programs \citep{Treangen2011}.
Due to repetitive elements, a non-ignorable fraction of high-throughput sequencing reads cannot be mapped confidently.
In general, the shorter the reads, the higher the challenges due to repeats.
I quantify this phenomenon more precisely in section \ref{sec:mappability}.
Here I focus on analysis strategies to deal with \emph{multi-reads}, \ie reads that cannot be mapped confidently as they align equally well to multiple locations.

It is not evident how to treat \emph{multi-reads}.
According to \cite{Treangen2011}, common strategies to deal with multi-reads are
\begin{inparaenum}[(i)]
\item to discard them all,
\item to randomly pick one best mapping location,
\item to consider all or up to $k$ best mapping locations within a given distance threshold.
\end{inparaenum}
A \emph{de facto} standard strategy, combining strategies (i) and (ii), emerged over the last years.
The read mapper randomly picks one best mapping location and complements it with its \emph{mapping quality}, \ie the probability of the mapping location being correct (see section~\ref{sec:background:paradigms:mapqual}).
Subsequently, downstream analysis tools either 
\begin{inparaenum}[(a)]
\item apply a mapping quality score cutoff to discard reads not mapping confidently to any location, or
\item annotate in turn their results with quality scores.
\end{inparaenum}
The other popular strategy adopted by analysis tools is to consider all mapping locations within an edit distance threshold.
In this case, it is not clear whether downstream analysis tools consider all mapping locations equal regardless of their distance.

In the light of these facts, I define two broad paradigms for the secondary and tertiary analysis of HTS data: \emph{best-mapping} and \emph{all-mapping}.
The best-mapping paradigm considers a single mapping location per read along with its confidence, while the all-mapping paradigm considers a comprehensive set of mapping locations per read.
It goes without saying that read mapper and downstream analysis tools must agree on a common paradigm.
Thus these paradigms are valid not only for read mappers but also for any downstream analysis tool, \eg variant callers.
Read mapping and variant calling are indeed tightly coupled steps within reference-based HTS pipelines.

\subsection{Best-mapping}
As said, best-mapping methods rely on a single mapping location per read.
In order to maximize recall, best-mappers often adopt complex scoring schemes taking into account gaps and base quality values, and at the same time implement sophisticated heuristics to speed up the search.
Best-mappers annotate any mapping location with its mapping quality.
Subsequently, in order to maximize precision, variant calling tools decide whether to consider or discard reads not mapping confidently to any location.
The GATK~\citep{DePristo2011} and Samtools~\citep{Li2009a} are popular best-mapping tools to call small variants.
In section~\ref{sec:mappability}, I discuss how this paradigm systematically fails on reads belonging some critical genomic regions, thus is limited to the analysis of \emph{high mappability} regions.

\subsubsection{Mapping quality score}
\label{sec:background:paradigms:mapqual}

Mapping quality has been introduced in the tool MAQ \citep{Li2008}.
The study considers short 30--40~bp reads, produced by early Illumina and ABI/SOLiD sequencing technologies, whose sequencing error rates were quite high.
Given the short lengths and high error rates, a significant fraction of such reads can be aligned to multiple mapping locations, even considering only co-optimal Hamming distance locations.
Since base callers output base call probabilities in Phred-scale along with the reads, \citeauthor{Li2008} propose a novel probabilistic scoring scheme called \emph{mapping quality}, encoding the probability that a read aligns correctly at a mapping location in the reference genome.

%The argument of \cite{Li2008} is that the Hamming distance is not an adequate scoring scheme to guess the correct mapping location of many reads.
%Though, \citeauthor{Li2008} do not show in their study what is the effect of relying on mapping quality rather than on mapping uniqueness.
%Mapping quality scores have been initially used in MAQ \citep{Li2008} to maximize variant calling confidence by discarding reads whose best mapping location is below a given mapping quality threshold.
Mapping quality scores offer a way to prioritize the results produced in downstream data analyses.
\cite{Li2008} write that \emph{``it is possible to act conservatively by discarding reads that map ambiguously at some level, but this leaves no information in the repetitive regions and it also discards data, reducing coverage in an uneven fashion, which may complicate the calculation of coverage.''}
For instance, the \emph{GATK HaplotypeCaller} \citep{DePristo2011} annotates its variant calls with qualities whose value depends on mapping qualities, rather than removing data by applying a hard mapping quality cutoff.
Below, I define the mapping quality score as done in \citep{Li2008}.

Fix the alphabet $\Sigma = \{$~A,~C,~G,~T~$\}$. Consider a known donor genome $g$ over $\Sigma$ and a read $r$ sequenced at location $l$ from the template $g_{l \dots l+|r|-1}$.
The base calling error $\epsilon_i$ from equation \ref{eq:phred} represents the probability $\epsilon_i$ of miscalling a base $r_i$ instead of calling its corresponding base $g_{l+i-1}$ in the donor genome.
The probability $\Pr[r_i | g_{l+i-1}]$ of observing the base $r_i$ given the donor genome base $g_{l+i-1}$, is:
\begin{eqnarray}
\label{eq:baseprob}
\Pr[r_i | g_{l+i-1}] = \left\{
\begin{array}{ll}
1-\epsilon_i                  & \text{ if } g_{l+i-1} = r_i\\
\frac{\epsilon_i}{|\Sigma|-1} & \text{ if } g_{l+i-1} \in \Sigma \setminus \{r_i\}\\
\end{array}
\right.
\end{eqnarray}
and assuming \iid base calling errors, it follows that the probability $\Pr[r | g, l]$ of observing the read $r$, given the donor genome template $g_{l \dots l+|r|-1}$, is:
\begin{eqnarray}
\label{eq:obsprob}
\Pr[r | g, l] = \prod_{i=1}^{|r|}{\Pr[r_i | g_{l+i-1}]}.
\end{eqnarray}

By applying Bayes' theorem, \cite{Li2008} derive the posterior probability $p(l|g,r)$, that location $l$ in the reference genome $g$ is the correct mapping location of read $r$.
Assuming uniform coverage, each location $l \in [1, |g| - |r| + 1]$ has equal probability of being the origin of a read in the donor genome, thus the prior probability $p(l)$ is simply:
\begin{eqnarray}
\Pr[l] = \frac{1}{|g| - |r| + 1}
\end{eqnarray}
Therefore, recalling $\Pr[r | g, l]$ from equation~\ref{eq:obsprob}, the posterior probability $\Pr[l|g,r]$ equals the probability of the read $r$ originating at location $l$, normalized over all possible locations in the reference genome:
\begin{eqnarray}
\label{eq:mapprob}
\Pr[l|g,r] = \frac{\Pr[r|g,l]}{\sum_{i=1}^{|g| - |r| + 1}{\Pr[r|g,i]}}
\end{eqnarray}
which in Phred-scale becomes:
\begin{eqnarray}
\label{eq:mapqual}
Q[l|g,r] = -10 \log_{10}(1 - \Pr[l|g,r]).
\end{eqnarray}

Computing the exact mapping quality as in equation~\ref{eq:mapqual} requires aligning each read to all positions in the reference genome.
On the one hand, this computation is practically infeasible.
On the other hand, suboptimal locations not close to the optimum one contribute very little to the sum in equation~\ref{eq:mapprob}.
Therefore, read mapping programs approximate equation~\ref{eq:mapprob} using only the mapping locations they repute relevant.

Some objections can be raised against the above definition of mapping quality scores.
First, the score is derived under the unlikely assumption of the reference genome being equal to the donor genome.
In other words, equation~\ref{eq:baseprob} considers only errors due to base miscalls and disregards genetic variation; thus the risk is to prefer mapping locations supported by known low base qualities rather than by true but unknown SNVs.
Second, mapping quality is nonetheless strongly correlated to mapping uniqueness, as discussed in section~\ref{sec:mappability}; it is easy to see that the probability of any location in equation~\ref{eq:mapprob} dilutes in presence of a large number of co-optimal or close suboptimal mapping locations.
Therefore, in chapter \ref{sec:yara}, I give an alternative definition of mapping quality.

\subsection{All-mapping}
All-mapping analysis methods consider a comprehensive set of locations per read.
Almost all read mappers in this category adopt edit distance and report all mapping locations within an error threshold, absolute or relative \wrt to the length of the reads.
Variant calling algorithms based on all-mapping have the potential to detect a wider spectrum of genomic variation events than their best-mapping counterparts.
For instance, variant callers based on the all-mapping paradigm detect CNVs \citep{Alkan2009}, and SNVs in homologous regions \citep{Simola2011}.
 % transpositions~\citep{VariationHunter} ?
%A practical challenge of all-mapping is represented by reporting and handling huge sets of mapping locations.

% -----------------------------------------------------------------------------

\section{Limits of high-throughput sequencing}
\label{sec:mappability}

A fraction of high-throughput sequencing reads cannot be mapped confidently due to repetitive elements.
Which regions of a model organism's genome cannot be resequenced confidently by a high-throughput sequencing technology?
And how accurate is downstream analysis on these low confidence regions?
Two recent studies \citep{Derrien2012, Lee2012} answer these questions.
Below, I report their key ideas and most relevant findings.

\subsection{Genome mappability}

\cite{Derrien2012} define \emph{genome mappability} as a function of a genome for a fixed $q$-gram length, distance measure \ie the Hamming or edit distance, and distance threshold $k$.
Given a genomic sequence $g$, they define the $(q,k)$-frequency $F^q_k(l)$ of the $q$-gram $g_{l \dots l+q-1}$ at location $l$ in $g$ as the number of occurrences of the $q$-gram in $g$ and its reverse complement $\bar{g}$.
The $(q,k)$-mappability $M^q_k(l)$ is the inverse $(q,k)$-frequency, \ie $M^q_k(l) = {F^q_k(l)}^{-1}$ with $M^q_k : \N \rightarrow ]0,1]$.
Note that $M^q_k(l)$ can be seen as the prior probability that any read of length $q$ originating at location $l$ will be mapped correctly.
The values of $(q,k)$-frequency and mappability obviously vary with the distance threshold $k$. Nonetheless, under any distance measure, it hold that the $q$-gram at location $l$ is unique up to distance $k$ iff $M^q_k(l) = 1$ and repeated otherwise.

Unique mappability determines which fraction of a genome can be analyzed according to strategy (i) of \citep{Treangen2011} (\ie discarding non-unique reads, see section \ref{sec:background:paradigms}).
\citeauthor{Derrien2012} quantify the \emph{unique mappability} of whole human, mouse, fly, and worm genomes.
Mimicking typical Illumina read mapping setups, they consider $q$-grams of length 36, 50 and 75~bp, and Hamming distance 2.
They find out that about 30~\% of the whole human genome is not unique \wrt $(36,2)$-mappability.
At $(75,2)$-mappability, 17~\% of the human genome is not yet unique.
This last result is slightly optimistic, as typical mapping setups call for up to 3--4 edit distance errors in order to map a significant fraction of the reads.
Table \ref{tab:mappability} shows some results obtained from \citep{Derrien2012}.

%The uniqueome plays an important role in ChIP-seq experiments.
%It is common practice \citep{?} to rely on short (36~bp) reads and discard the non-unique ones.
%Not only a significant fraction of the sequencing data is thrown out.
%Worse than that, one ends up with holes in 30~\% of the genome.
%A ChIP-seq peak caller considering multi-reads calls up to 30~\% more peaks.
%Cite regions of of biological relevance \eg 5S rRNA, and clinical relevance \eg HLA-A.

\begin{table}[b]
\begin{center}
\caption[Mappability of model genomes]{Mappability of model genomes. Data obtained from \citep{Derrien2012}.}
\sffamily
\input{tables/table_mappability}
\label{tab:mappability}
\end{center}
\end{table}

To estimate single-base resequencing accuracy, \citeauthor{Derrien2012} consider the mappability of all possible $q$-grams spanning any single genomic location.
They define \emph{pileup mappability} $P^q_k$ at position $i$ as the average mappability of all $q$-grams spanning position $i$:
\begin{eqnarray}
P^q_k(i) = 1/q \sum_{j=i}^{i+1}{M^q_k(j)}.
\end{eqnarray}
\cite{Derrien2012} find out in their own resequencing studies that \emph{``low pileup-mappability regions are more prone to show a high value of heterozygosity than those with high mappability''}.
Ideally, variant calling tools call a locus as heterozygous whenever the consensus alignment column at that locus contains two distinct bases.
This situation tends to arise more frequently whenever the consensus alignment contains reads originating from similar yet distinct regions.

\subsection{Genome mappability score}

Genome mappability score (GMS) \citep{Lee2012}, analogously to pileup mappability, estimates single-locus resequencing accuracy for a specific sequencing technology.
Instead of considering the inverse $q$-gram frequency, \citeauthor{Lee2012} use mapping quality (see section~\ref{sec:background:paradigms:mapqual}) to estimate the probability that a read originating at a given position can be mapped correctly.
Subsequently, they derive the average mapping probability of any read spanning a location $l$ of a reference genome $g$ as:
%\footnote{Equation~3 in \citep{Lee2012} is not precise, please consider equation~\ref{eq:gms} instead.}
\begin{eqnarray}
\label{eq:gms}
\Pr[l|g] = \sum_{r \in \mathcal{R}(l)}{\frac{\Pr[l|g,r]}{|\mathcal{R}(l)|}}
\end{eqnarray}
which in Phred-scale becomes:
\begin{eqnarray}
Q[l|g] = \sum_{r \in \mathcal{R}(l)}{\frac{1 - 10^{-Q[l|g,r]/10}}{|\mathcal{R}(l)|}}.
\end{eqnarray}
Thus, fixed a genomic sequence $g$, they define the genome mappability score $\text{GMS}(l)$ in percentual value:
\begin{eqnarray}
\text{GMS}(l) = 100 \, Q[l|g]
\end{eqnarray}

\citeauthor{Lee2012} proceed as follow to compute GMS.
They first simulate reads from all genomic locations, having length and error profiles similar to those issue by actual sequencing technologies.
Subsequently, they compute mapping quality scores by mapping all simulated reads with the best-mapper BWA \citep{Li2009}.
Then, as just explained, they compute GMS at any location by averaging the quality scores.
Finally, they define \emph{low GMS} regions as those locations for which $\text{GMS}(l) \leq 10$ and \emph{high GMS} otherwise.
Table \ref{tab:gms} shows the performance of various sequencing technologies on the whole human genome (data obtained from \citep{Lee2012}).

\begin{table}[t]
\begin{center}
\caption[Human genome mappability score]{Human genome mappability score of various sequencing technologies. Data obtained from \citep{Lee2012}.}
\sffamily
\input{tables/table_gms}
\label{tab:gms}
\end{center}
\end{table}

\citeauthor{Lee2012} measure variant calling accuracy by GMS for the popular combination of best-mapping tools BWA and SAMtools \citep{Li2009a}.
They simulate an Illumina-like resequencing study and feed it to such analysis pipeline.
They find out that, at $30\,\times$ sequencing coverage, accuracy approaches 100~\% in high GMS regions, while it levels off to 25~\% in low GMS regions.
Their analysis \emph{``shows that most SNP detection errors are false negatives, and most of the missing variations are in regions with low GMS scores''} \citep{Lee2012}.
These are the limits of the analysis of high-throughput sequencing data.

% =============================================================================


\section{Popular read mappers}
\label{sec:background:mappers}

Following the boom of NGS technologies, recent bioinformatics research has produced dozens of tools to perform read mapping.
Two surveys \citep{Li2010, Fonseca2012} try to help bioinformaticians to find their way in the jungle of of read mapping tools.
The survey by \citeauthor{Li2010} first classifies read mapping algorithms by data structure: those based on hash tables and those based on suffix/prefix trees.
However, the adopted data structure is often an implementation detail, indeed most algorithms covered in their survey could fit into both classes.
The survey primarily considers the application of SNP calling; in the considered setup, tools enumerating a comprehensive set of locations always lag behind those designed to report only one location per read.
The survey by \citeauthor{Fonseca2012} instead catalogs read mappers by the features exposed to the user.
It considers supported input--output formats, rate of errors and variation, number and type (\ie local or semi-global read alignments) of mapping locations reported.
After this exhaustive catalog, the survey concludes that the choice of a read mapper 
\emph{``involves application-specific requirements such as how well it works in conjunction with downstream analysis tools (\ie variant callers)''}.
Read mapping and variant calling are indeed tightly coupled steps within reference-based HTS analysis pipelines.

As explained above, secondary and tertiary analysis methods are based on one of the two following paradigms: best-mapping and all-mapping.
In the light of the above consideration, the most important feature of a read mapper is the number of mapping locations reported, followed by their type, while the other features are mostly of technical relevance.
Most read mappers are specifically designed to fit one paradigm, while others are versatile enough to work well in both cases.

The rest of this section presents most popular read mapping tools.
Table~\ref{tab:mappers} gives an overview of all these tools.
Among them, BWA \citep{Li2009}, Bowtie \citep{Langmead2009} and Bowtie~2\citep{Langmead2012}, and Soap \citep{Li2009b} are prominent tools designed for best-mapping, while SHRiMP~2 \citep{David2011}, mr(s)Fast \citep{Alkan2009,Hach2010}, RazerS \citep{Weese2009} and RazerS~3 \citep{Weese2012}, and Hobbes~2 \citep{Kim2014} are designed for all-mapping.
Grosso modo, most prominent best-mappers recursively enumerate substrings on a suffix/prefix tree of the reference genome via backtracking algorithms.
Backtracking alone is impractical as its time complexity grows exponentially with the number of errors considered, hence best-mappers apply heuristics to reduce and prioritize the enumeration.
Conversely, all-mappers are based on filtering algorithms for approximate string matching.
They quickly determine, often with the help of an index, locations of the reference genome candidate to contain approximate occurrences, then verify them with conventional methods.
Their efficiency is bound to filtration specificity and thus deteriorates with increasing error rates and genome lengths.
GEM \citep{MarcoSola2012} tries to fit both best and all-mapping paradigms.
It speeds up best-mapping by stratifying mapping locations by edit distance and prioritizing filtration accordingly.
Finally, Masai \citep{Siragusa2013} and Yara \citep{Siragusa2015} are read mapping programs developed by myself.
%I initially designed Masai for the all-mapping paradigm and later Yara for both best and all-mapping.
I present the engineering and evaluation of these tools in chapters \ref{sec:masai} and \ref{sec:yara}.

% -----------------------------------------------------------------------------

\subsection{Bowtie and Bowtie~2}
\label{background:mappers:bowtie}

Bowtie \citep{Langmead2009} is a mapper designed to have a small memory footprint and quickly report a few good mapping locations for early generation short Illumina and ABI/SOLiD reads.
The tool achieves the former goal by indexing the reference genome with an FM-index and the latter one by performing a greedy backtracking on it.
The greedy top-down traversal visits first the subtree yielding the least number of mismatches and stops after having found a candidate (not guaranteed to be optimal when $k>1$).
In addition, Bowtie speeds up backtracking by applying \emph{case pruning} \citep{Maekinen2010}, a simple application of the pigeonhole principle.
However, this technique is mostly suited for $k=1$ and requires the index of the forward and reverse reference genome.
Bowtie can be configured to search by strata, but the search time increases significantly while the traversal still misses a large fraction of the search space due to seeding heuristics.

Bowtie~2 \citep{Langmead2012} has been designed to quickly report a couple of mapping locations for longer Illumina, Ion Torrent and Roche/454 reads, usually having lengths in the range from 100~bp to 400~bp.
This tool uses an heuristic seed-and-extend approach, collecting seeds of fixed length, partially overlapping, and searching them exactly in the reference genome using an FM-index.
Bowtie~2 randomly chooses candidate locations, to avoid uncompressing large suffix array intervals and executing many DP instances.
The tool verifies candidate locations using a striped vectorial dynamic programming algorithm by \cite{Farrar2007}, implemented using SIMD instructions.
Bowtie~2 can be configured to report semi-global or local alignments, scored using a tunable affine scoring scheme.
%However, its heuristic filtration method, independent of the scoring scheme, makes it hard to believe what it promises.

% -----------------------------------------------------------------------------

\subsection{BWA}
\label{background:mappers:bwa}

BWA \citep{Li2009} is designed to map Illumina reads and report a few best semi-global alignments.
The program backtracks the FM-index of the reference genome with a \emph{greedy breadth-first search}.
The tool ranks nodes to be visited by edit distance score: the best node is popped from a priority queue and visited, its children are then inserted again in the queue.
The traversal considers indels using a more involved 9-fold recursion.
\citeauthor{Li2009} speed up backtracking by adopting a more stringent pruning strategy \citep{Maekinen2010} that nonetheless takes some preprocessing time and requires the index of the reverse reference genome.
BWA performs paired-end alignments by trying to anchor both paired-end reads and verifying the corresponding mate, within an estimated insert size, using the classic DP-based algorithm by \cite{Smith1981}.
Consequently, the program in paired-end mode aligns reads at a slower rate than in single-end mode.
The program is not fully multi-threaded, therefore it scales poorly on modern multi-core machines.

%BWA-SW \citep{Li2010a} is designed to map Roche/454 reads, which have an average length of 400~bp.
%It is an heuristic version of BWT-SW, designed to report a few good local alignments.
%This variant of BWA adopts a double indexing strategy: it indexes all substrings of one read in a DAWG.
%It performs Smith-Waterman of all read substrings directly on the FM-index, by backtracking as soon as no viable alignment can be obtained.
%As in the first BWA, the traversal proceeds in a greedy fashion.
%In addition, BWA-SW implements some seeding heuristics to limit backtracking and jump in the reference genome to verify candidate locations whenever this becomes favorable.

% -----------------------------------------------------------------------------

\subsection{Soap}
\label{background:mappers:soap}

Soap~2 \citep{Li2009b} has been designed to produce a very quick but shallow mapping of short Illumina reads, up to 2 mismatches and without indels.
The tool performs backtracking using the so-called bi-directional (or 2-way) BWT \citep{Belazzougui2013}.
Soap~2 supports paired-end mapping but at a slower alignment rate, it lacks native output in the \emph{de-facto} standard SAM format, and it is not open source.

% -----------------------------------------------------------------------------

\subsection{SHRiMP~2}
\label{background:mappers:shrimp}

The \emph{SHort Read Mapping Program} (SHRiMP~2) \citep{David2011} is designed to map short Illumina and ABI/SOLiD reads. The tool achieves high accuracy at the expense of speed.
SHRiMP~2 indices the reference genome using multiple gapped $q$-grams.
At query time, it projects each read to identify candidate mapping locations, which are verified with a DP algorithm \citep{Smith1981}.
The SHRiMP~2 project has been recently discontinued.

% -----------------------------------------------------------------------------

\subsection{RazerS and RazerS~3}
\label{background:mappers:razers}

RazerS \citep{Weese2009} has been designed to report all mapping locations within a fixed Hamming or edit distance error rate.
It is based on a full-sensitive $q$-gram counting filtration method (see section \ref{sec:filtering:qgrams-ext}) combined with the edit distance verification algorithm by Myers \citep{Myers1999}.
On demand, the tool throttles filtration to be more specific at the expense of a controlled loss rate.
Stronger filtration reduces the number of candidate locations and improves the overall speed of the program.
All in all, the SWIFT filter is very slow while not highly specific.

RazerS~3 \citep{Weese2012} is a faster version featuring shared-memory parallelism, a banded version of Myers' algorithm, and a quicker filtration method based on exact seeds (see section \ref{sec:filtering:exact}).
Such filtration method however turns out to be very weak on mammal genomes.
Because of this fact, RazerS~3 is one-two orders of magnitude slower than Bowtie~2 and BWA on such datasets.

All RazerS versions index the reads and scan the reference genome.
One positive aspect of this strategy is that no preprocessing of the reference genome is required.
However, other mapping strategies beyond all-mapping, \eg mapping by strata, cannot be efficiently implemented.
Moreover, these programs exhibit an high memory footprint as they remember the mapping locations of all input reads until the whole reference genome has been scanned.

% -----------------------------------------------------------------------------

\subsection{mrFast and mrsFast}
\label{background:mappers:mrsfast}

The tools mrsFast \citep{Hach2010} and mrFast \citep{Ahmadi2012} are designed to map Illumina reads.
They report all mapping locations within a fixed absolute number errors, respectively under the edit and Hamming distance.
Similarly to RazerS~3, these two programs implement full-sensitive filtration using exact seeds (section \ref{sec:filtering:exact}).
Their peculiarity is a cache-oblivious strategy to mitigate the high cost of verifying clusters of candidate locations.
In addition, mrsFast computes the edit distance between one read and one mapping location in the reference genome with an antidiagonal-wise vectorial dynamic programming algorithm, implemented using SIMD instructions.
These tools perform only all-mapping, produce files of impractical size and lack multi-threading support.

% -----------------------------------------------------------------------------

\subsection{Hobbes~2}
\label{background:mappers:hobbes}

Hobbes~2 \citep{Kim2014} is designed to identify all read mapping locations within a fixed Hamming or edit distance threshold.
In order to improve filtering efficiency, the tool employs a novel technique of so-called \emph{prefix $q$-grams} that enriches the reference genome $q$-gram index.
However, this technique does not guarantee full-sensitivity.

% -----------------------------------------------------------------------------

\subsection{GEM}
\label{background:mappers:gem}

The GEM mapper \citep{MarcoSola2012} is a flexible read aligner for Illumina and Ion Torrent reads.
The tool can be configured either as an all-mapper, as a best/unique-mapper, or to search by strata;
however, it supports the best-mapping paradigm only to some extent, as it does not annotate mapping locations with qualities.

GEM implements full-sensitive filtration with approximate seeds (see section \ref{sec:seeds-apx}).
The program indexes the reference genome with an FM-index, tries to find an optimal filtration scheme per read, and verifies candidate locations using Myers' algorithm \cite{Myers1999}.
GEM maps paired-reads in two ways: either it maps both ends independently and then combines them, or maps one end and then verifies the other end using an online method.
Unfortunately, the tool is not open source and provides obscure parameterization.

% -----------------------------------------------------------------------------

%\subsection{Masai and Yara}

% -----------------------------------------------------------------------------

\begin{landscape}
\begin{table}[h]
  \center
  \caption[Overview of popular read mappers]{Overview of popular read mappers.}
  \sffamily
%  \resizebox{1.0\textwidth}{!}
%  {
	\renewcommand{\tabcolsep}{0.8ex}
	\input{tables/table_mappers}
%  }
\label{tab:mappers}
\end{table}
\end{landscape}


% =============================================================================

\chapter{Background}

%In this chapter, I provide the background on read mapping.
%I first introduce HTS technologies from the standpoint of the bioinformatician who performs secondary analysis.
%I consider most popular sequencing instruments and mention aspects of the sequencing data that influence secondary analysis, \eg the paired-end and mate-pairs protocols, sequencing artefacts, and base qualities.
%Afterwards, I give insights on some properties of genomic sequences that make secondary analysis particularly difficult.
%I discuss how mappability helps to pinpoint genomic regions for which reference-guided assembly is hard.
%I present two paradigms for reference-guided assembly methods: best-mapping and all-mapping.
%Subsequently, I give an overview of most popular read mapping tools.
%I give key algorithmic ideas and relevant engineering solutions adopted by the various tools.

\section{High-throughput sequencing data}
\label{sec:background:sequencing}

%HTS technologies produce reads shorter than Sanger sequencing.
%Even worse, reads contain sequencing artefacts.
%Base qualities annotate single base accuracy.
%These properties of HTS data have to be taken into account for read mapping.

\subsection{Read sequences}

\subsubsection{Illumina}
Illumina / Solexa.

\subsubsection{Ion Torrent}
Life Technologies / Ion Torrent.

\subsubsection{Other technologies}

Roche / 454 Life Sciences.

SOLiD
%\emph{Applied Biosystems} (ABI) entered the market with its \emph{SOLiD} system, based on parallel sequencing by \emph{stepwise ligation}, reading two bases at a time with a florescent label in order to improve sequencing accuracy.

\subsection{Phred base quality scores}
\label{sec:background:hts:phred}

Phred base quality scores have been introduced in \citep{Ewing1998, Ewing1998a} to assess the quality of sequencing single bases in capillary reads.
Instead of directly discarding low-quality regions present in capillary reads, the tool Phred calls each base and annotates it with a quality score encoding the probability that it has been wrongly called.
As this method has been widely accepted, base callers annotate reads issue of all sequencing technologies with Phred base quality scores.

To formally define Phred base quality scores, let us fix the alphabet $\Sigma = \{$~A,~C,~G,~T~$\}$, and consider a known donor genome $g$ over $\Sigma$ and a read $r$ sequenced at location $l$ from the template $g_{l \dots l+|r|-1}$.
The base calling error $\epsilon_i$, at position $i$ in the read $r$, is defined as the probability $\epsilon_i$ of miscalling a base $r_i$ instead of calling its corresponding base $g_{l+i-1}$ in the donor genome.
Therefore, the Phred base quality $Q_i$ at position $i$ is:
\begin{eqnarray}
Q_i = -10 \log_{10} \epsilon_i.
\end{eqnarray}

Given the above, the probability $p(r_i | g_{l+i-1})$ of calling the base $r_i$ in the read $r$, given the donor genome base $g_{l+i-1}$, is:
\begin{eqnarray}
p(r_i | g_{l+i-1}) = \left\{
\begin{array}{ll}
1-\epsilon_i                  & \text{ if } g_{l+i-1} = r_i\\
\frac{\epsilon_i}{|\Sigma|-1} & \text{ if } g_{l+i-1} \in \Sigma \setminus \{r_i\}\\
\end{array}
\right.
\end{eqnarray}
and assuming \iid base calling errors, it follows that the probability $p(r | g, l)$ of observing the read $r$, given the donor genome template $g_{l \dots l+|r|-1}$, is:
\begin{eqnarray}
\label{eq:phred}
p(r | g, l) = \prod_{i=1}^{|r|}{p(r_i | g_{l+i-1})}
\end{eqnarray}

% =============================================================================

\section{Data analysis paradigms}
\label{sec:paradigms}

The goal of secondary analysis is to reconstruct the original sequence of the donor genome.
The \emph{reference-guided} assembly plan assumes a prior knowledge of a reference genome similar to the donor.
Reads are thus mapped (\ie aligned) to the reference genome \wrt a given scoring scheme and threshold.
The scoring scheme should account for eventual genomic variation, as well as for any sequencing artefacts.
An alignment of optimal score for a read implies its \emph{original location} on the reference genome.
Conversely, no alignment within the score threshold implies too many sequencing errors, too much genetic variation, or sample contamination.
%A pile-up of the reads aligned at their mapping locations thus provides a consensus alignment of the donor genome.
A problem arises in presence of co-optimal or close sub-optimal alignments: the read cannot be mapped confidently.

The problem of confidently mapping high-throughput sequencing reads comes from the non-random nature of genomic sequences.
Genomes evolved through multiple types of duplication events, including whole-genome duplications \citep{Wolfe1997,Dehal2005} or large-scale segmental duplications in chromosomes \citep{Bailey2001,Samonte2002}, transposition of repetitive elements as short tandem repeats (microsatellites) \citep{Wang1994a,Wooster1994} and interspersed nuclear elements (LINE, SINE) \citep{Smit1996}, proliferation of repetitive structural elements such as telomeres and centromeres \citep{Meyne1990}.
As a result of these events, for instance, about 50~\% of the human genome is composed of repeats.

Repeats present in general technical challenges for all \emph{de novo} assembly and sequence alignment programs \citep{Treangen2011}.
Due to repetitive elements, a non-ignorable fraction of high-throughput sequencing reads cannot be mapped confidently.
In general, the shorter the reads, the higher the challenges due to repeats.
I quantify this phenomenon more precisely in section \ref{sec:mappability}.
Here I focus on analysis strategies to deal with \emph{multi-reads}, \ie reads that cannot be mapped confidently as they align equally well to multiple locations.

It is not evident how to treat \emph{multi-reads}.
According to \citeauthor{Treangen2011}, common strategies to deal with multi-reads are
\begin{inparaenum}[(i)]
\item to discard them all,
\item to randomly pick one best mapping location,
\item to consider all or up to $k$ best mapping locations within a given distance threshold.
\end{inparaenum}
A \emph{de facto} standard strategy emerged over the last years, combining strategies i and ii.
The read mapper randomly picks one best mapping location and complements it with its \emph{mapping quality}, \ie the probability of the mapping location being correct (see section~\ref{sec:mapping:quality}).
Subsequently, downstream analysis tools apply a mapping quality score cutoff to discard reads not mapping confidently to any location.
The other popular strategy adopted by analysis tools is to consider all mapping locations within an edit distance threshold.
In this case, it is not clear whether downstream analysis tools consider equally all mapping locations regardless of their distance.

In the light of these facts, I define two broad paradigms for the secondary and tertiary analysis of HTS data: \emph{best-mapping} and \emph{all-mapping}.
The best-mapping paradigm considers a single mapping location per read along with its confidence, while the all-mapping paradigm considers a comprehensive set of mapping locations per read.
It goes without saying that read mapper and downstream analysis tools must agree on a common paradigm.
Thus these paradigms are valid not only for read mappers but also for any downstream analysis tool, \eg variant callers.
Read mapping and variant calling are indeed tightly coupled steps within reference-based HTS pipelines.

\subsection{Best-mapping}
As said, best-mapping methods rely on one single mapping location per read.
In order to maximize recall, best-mappers often adopt complex scoring schemes taking into account gaps and base quality values, and at the same time implement sophisticate heuristics to speed up the search.
Best-mappers should always complement any mapping location with its mapping quality.
Subsequently, in order to maximize precision, variant calling tools decide whether to consider or discard reads not mapping confidently to any location.
The GATK~\citep{DePristo2011} and Samtools~\citep{Li2009a} are popular best-mapping tools to call small variants.
In section~\ref{sec:mapping:mappability}, I show how these methods are limited to the analysis of high mappability regions, as they systematically discard reads belonging to low mappability regions.

\subsubsection{Mapping quality score}
\label{sub:mapqual}

Mapping quality has been introduced in \citep{Li2008}.
The study considers short reads of length ranging from 30~bp to 40~bp, produced by early Illumina/Solexa and ABI/SOLiD sequencing technologies, whose sequencing error rates were quite high.
Given the short lengths and high error rates, a significant fraction of such reads can be aligned to multiple mapping locations, even considering only co-optimal Hamming distance locations.

The key point is that the Hamming distance is not an adequate scoring scheme to guess the correct mapping location of many reads.
\citeauthor{Li2008} claim that: \emph{it is possible to act conservatively by discarding reads that map ambiguously at some level, but this leaves no information in the repetitive regions and it also discards data, reducing coverage in an uneven fashion, which may complicate the calculation of coverage.}
However, they do not show in their study what is the effect of relying on mapping quality rather than on mapping uniqueness.

Since base callers output base call probabilities in Phred-scale along with the reads, \citeauthor{Li2008} propose a novel probabilistic scoring scheme called mapping quality, giving the probability that a given read has been aligned correctly at a given mapping location in the reference genome.

The posterior probability $p(l|g,r)$, that location $l$ in the reference genome $g$ is the correct mapping location of read $r$, is derived by applying Bayes' theorem.
Assuming uniform coverage, each location $l \in [1, |g| - |r| + 1]$ has equal probability of being the origin of a read in the donor genome, thus the prior probability $p(l)$ is simply:
\begin{eqnarray}
p(l) = \frac{1}{|g| - |r| + 1}
\end{eqnarray}
Therefore, recalling $p(r | g, l)$ from equation~\ref{eq:phred}, the posterior probability $p(l|g,r)$ equals the probability of the read $r$ originating at location $l$ normalized over all possible locations in the reference genome:
\begin{eqnarray}
\label{eq:mapprob}
p(l|g,r) = \frac{p(r|g,l)}{\sum_{i=1}^{|g| - |r| + 1}{p(r|g,i)}}
\end{eqnarray}
which in Phred-scale becomes:
\begin{eqnarray}
\label{eq:mapqual}
Q(l|g,r) = -10 \log_{10}[1 - p(l|g,r)]
\end{eqnarray}

Computing the exact mapping quality as in equation~\ref{eq:mapqual} requires aligning each read to all positions in the reference genome.
On one hand, this computation would not be practical, indeed the vast majority of a reference genome is discarded when mapping reads by means of filtering and fully-indexed methods.
On the other hand, the contribution of discarded locations to the sum in equation~\ref{eq:mapprob} can be neglected.
Therefore, equation~\ref{eq:mapprob} is approximated using only relevant mapping locations found by the read mapper.

Mapping quality has been initially used in \citep{Li2008} and \citep{Li2009} to maximize variant calling confidence by discarding reads whose best mapping location is below a given mapping quality threshold.
This measure has been widely accepted: nowadays it is computed by most popular read mappers and used by almost all variant calling tools \eg the Genome Analysis ToolKit (GATK) \citep{DePristo2011}.

Nonetheless, some important objections can be moved against mapping quality.
First, the mapping quality score is derived under the unlikely assumption of the reference genome being equal to the donor genome.
In other words, mapping quality considers only errors due to base miscalls and disregards genetic variation; thus the risk is to prefer mapping locations supported by known low base qualities rather than by true but unknown SNVs.
Second, mapping quality is nonetheless strongly correlated to mapping uniqueness, as discussed in section~\ref{sec:mappability}; it is easy to see that the mapping probability in equation~\ref{eq:mapprob} is diluted in presence of a large number of co-optimal mapping locations.
Third, mapping quality tends to become less relevant as base calls improve, due to advances of sequencing technologies, and thus degenerates in a shallow measure of uniqueness.

\subsection{All-mapping}
All-mapping analysis methods consider a comprehensive set of locations per read.
Almost all read mappers in this category adopt edit distance, the simplest scoring scheme, and report all mapping locations within an error threshold, absolute or relative \wrt to the length of the reads.
Variant calling algorithms based on all-mapping have the potential to detect a wider spectrum of genomic variation events than in best-mapping.
For instance, variant callers based on the all-mapping paradigm detect CNVs \citep{Alkan2009}, and SNVs in homologous regions \citep{Simola2011}.
 % transpositions~\citep{VariationHunter} ?
A practical challenge of all-mapping is represented by reporting and handling huge sets of mapping locations.

% -----------------------------------------------------------------------------

\section{Limits of high-throughput sequencing}
\label{sec:mappability}

Due to repetitive elements, a non-ignorable fraction of high-throughput sequencing reads cannot be mapped confidently.
Which regions of a model organism's genome cannot be resequenced confidently by a high-throughput sequencing technology?
And how accurate is downstream analysis on these low confidence regions?
Two recent studies \citep{Derrien2012, Lee2012} answer these questions.
Below, I report their key ideas and most relevant findings.

\subsection{Genome mappability}

In \citep{Derrien2012}, \emph{genome mappability} is defined for a given $q$-gram length, a distance measure \ie the Hamming or edit distance, and a distance threshold $k$.
Given a genomic sequence $g$, the $(q,k)$-frequency $F^q_k(l)$ of the $q$-gram $g_{l \dots l+q-1}$ at location $l$ in $g$ denotes the number of occurrences of the $q$-gram in $g$ and its reverse complement $\bar{g}$.
The $(q,k)$-mappability $M^q_k(l)$ is the inverse $(q,k)$-frequency, \ie $M^q_k(l) = {F^q_k(l)}^{-1}$ with $M^q_k : \N \rightarrow ]0,1]$.
Note that $M^q_k(l)$ can be seen as the prior probability that any read of length $q$ originating at location $l$ will be mapped correctly.
The values of $(q,k)$-frequency and mappability obviously vary with the distance threshold $k$. Nonetheless, under any distance measure, it hold that the $q$-gram at location $l$ is unique up to distance $k$ iff $M^q_k(l) = 1$ and repeated otherwise.

Unique mappability determines which fraction of a genome can be analyzed according to strategy i (\ie discarding non-unique reads, see section \ref{sec:paradigms}).
\citeauthor{Derrien2012} quantify the \emph{unique mappability} of whole human, mouse, fly, and worm genomes.
Mimicking typical Illumina read mapping setups, they consider $q$-grams of length 36, 50 and 75~bp, and Hamming distance 2.
They find out that about 30~\% of the whole human genome is not unique \wrt $(36,2)$-mappability.
At $(75,2)$-mappability, 17~\% of the human genome is not yet unique.
This last result is quite optimistic, as typical mapping setups call for 3--4 edit distance errors in order to map a significant fraction of the reads.
Table \label{tab:mappability} shows some results extrapolated from \citep{Derrien2012}.

%The uniqueome plays an important role in ChIP-seq experiments.
%It is common practice \citep{?} to rely on short (36~bp) reads and discard the non-unique ones.
%Not only a significant fraction of the sequencing data is thrown out.
%Worse than that, one ends up with holes in 30~\% of the genome.
%A ChIP-seq peak caller considering multi-reads calls up to 30~\% more peaks.
%Cite regions of of biological relevance \eg 5S rRNA, and clinical relevance \eg HLA-A.

\begin{table}[h]
\begin{center}
\caption[Mappability of model genomes]{Mappability of model genomes. Data extrapolated from \citep{Derrien2012}.}
\sffamily
\input{tables/table_mappability}
\end{center}
\label{tab:mappability}
\end{table}

To estimate resequencing accuracy at a single location, \citeauthor{Derrien2012} consider the mappability of all possible $q$-grams spanning it.
They define \emph{pileup mappability} $P^q_k$ at position $i$ as the average mappability of all $q$-grams spanning position $i$, \ie:
\begin{eqnarray}
P^q_k(i) = 1/q \sum_{j=i}^{i+1}{M^q_k(j)}
\end{eqnarray}
\citeauthor{Derrien2012} find out in their own resequencing studies that \emph{low pileup-mappability regions are more prone to show a high value of heterozygosity than those with high mappability} \citep{Derrien2012}.
Variant callers ideally call a locus heterozygous whenever its consensus alignment column, consisting of piled up reads, contains two distinct bases.
This situation tends to arise whenever the consensus alignment contains reads originating from similar yet distinct regions.

\subsection{Genome mappability score}

Genome mappability score (GMS) \citep{Lee2012}, analogously to pileup mappability, estimates single-locus resequencing accuracy for a specific sequencing technology.
Instead of considering the inverse $q$-gram frequency, \citeauthor{Lee2012} use mapping quality (see subsection~\ref{sub:mapqual}) to estimate the probability that a read originating at a given position can be mapped correctly.
Subsequently, they derive average mapping probability of any read spanning a location $l$ of a reference genome $g$ as\footnote{Equation~3 in \citep{Lee2012} is not precise, please consider equation~\ref{eq:gms} instead.}:
\begin{eqnarray}
\label{eq:gms}
p(l|g) = \sum_{r \in \mathcal{R}(l)}{\frac{p(l|g,r)}{|\mathcal{R}(l)|}}
\end{eqnarray}
which in Phread-scale becomes:
\begin{eqnarray}
Q(l|g) = \sum_{r \in \mathcal{R}(l)}{\frac{1 - 10^{-\frac{Q(l|g,r)}{10}}}{|\mathcal{R}(l)|}}
\end{eqnarray}
Thus, fixed a genomic sequence $g$, they define the genome mappability score $\text{GMS}(l)$ in percentual value:
\begin{eqnarray}
\text{GMS}(l) = 100 \, Q(l|g)
\end{eqnarray}

\citeauthor{Lee2012} proceed as follow to compute GMS.
They first simulate reads from all genomic locations, having length and error profiles similar to those issue by actual sequencing technologies.
Subsequently, they compute mapping quality scores by mapping all simulated reads with BWA (see section \ref{sec:background:mappers:bwa}).
Then, as just explained, they compute GMS at any location by averaging the quality scores.
Finally, they define \emph{low GMS} regions as those locations for which $\text{GMS}(l) \leq 10$, and \emph{high GMS} otherwise.
Table \ref{tab:gms} shows performance of various sequencing technologies on the whole human genome (data extrapolated from \citep{Lee2012}).

\begin{table}[h]
\begin{center}
\caption[Human genome mappability score]{Human genome mappability score of various sequencing technologies. Data extrapolated from \citep{Lee2012}.}
\sffamily
\input{tables/table_gms}
\end{center}
\label{tab:gms}
\end{table}

\citeauthor{Lee2012} simulate an Illumina-like resequencing study.
They measure variant calling accuracy by GMS for the popular combination of best-mapping tools BWA and SAMtools \citep{Li2009a}.
They find out that at $30\,\times$ sequencing coverage accuracy tends to 100~\% in high GMS regions, while it levels off to 25~\% in low GMS regions.
Their analysis \emph{shows that most SNP detection errors are false negatives, and most of the missing variations are in regions with low GMS scores} \citep{Lee2012}.
These are the limits of \emph{de facto} standard tools for the analysis of high-throughput sequencing data.

% =============================================================================


\section{Popular read mappers}

Following the boom of NGS technologies, recent bioinformatics research has produced dozens of tools to perform read mapping.
Two surveys \citep{Li2010, Fonseca2012} try to help bioinformaticians to extricate themselves from the jungle of read mapping tools.
The survey by \citeauthor{Li2010} first classifies read mapping algorithms by data structure: those based on hash tables and those based on suffix/prefix trees.
However, the adopted data structure is often an implementation detail, indeed most algorithms covered in the survey fit into both classes.
The survey primarily considers the application of SNP calling; in the considered setup, tools enumerating a comprehensive set of locations always lag behind those designed to report only one location per read.
The survey by \citeauthor{Fonseca2012} catalogs read mappers by the features exposed to the user.
It considers supported input--output formats, rate of errors and variation, number and type (\ie local or semi-global read alignments) of mapping locations reported.
After this exhaustive catalog, the survey concludes that the choice of a read mapper 
\emph{involves application-specific requirements such as how well it works in conjunction with downstream analysis tools (\ie variant callers)}.

Read mapping and variant calling are indeed tightly coupled steps within reference-based HTS analysis pipelines.
Secondary and tertiary analysis methods are based on one of the two following paradigms: best-mapping and all-mapping.
In the light of the above consideration, the most important feature of a read mapper is the number of mapping locations reported, followed by their type, while the other features are mostly of technical relevance.
Most read mappers are specifically designed to fit one paradigm, while others are versatile enough to work well in both cases.

The rest of this section presents most popular read mapping tools.
Among them, BWA \citep{Li2009,Li2010a}, Bowtie \citep{Langmead2009,Langmead2012} and Soap \citep{Li2009b} are prominent tools designed for best-mapping, while mr(s)Fast \citep{Alkan2009,Hach2010}, RazerS \citep{Weese2009,Weese2012}, SHRiMP \citep{Rumble2009,David2011} are designed for all-mapping.
Grosso modo, most prominent best-mappers recursively enumerate substrings on a suffix/prefix tree of the reference genome via backtracking algorithms.
Backtracking alone is impractical as its time complexity grows exponentially with the number of errors considered, hence best-mappers apply heuristics to reduce and prioritize enumeration.
Conversely, all-mappers are based on filtering algorithms for approximate string matching.
They quickly determine, often with the help of an index, locations of the reference genome candidate to contain approximate occurrences, then verify them with conventional methods.
Their efficiency is bound to filtration specificity and thus deteriorates with increasing error rates and genome lengths.
Finally, the most recent tools GEM \citep{MarcoSola2012}, Masai \citep{Siragusa2013}, and Yara \citep{Siragusa2014}, fit both best and all-mapping paradigms.
They speed up best-mapping by stratifying mapping locations by edit distance and prioritizing filtration accordingly.
In addition to that, they also speed up all-mapping by means of more specific filters based on backtracking.


% -----------------------------------------------------------------------------

\subsection{Bowtie}

Bowtie \citep{Langmead2009} is a mapper designed to have a small memory footprint and quickly report a few good mapping locations for early generation Illumina/Solexa and ABI/SOLiD short reads of length up to 50~bp.
It achieves the former goal by indexing the reference genome with an FM-index and the latter goal by performing a greedy depth-first traversal on it.

The greedy depth-first traversal visits first the subtree yielding the least number of mismatches and stops after having found a candidate (not guaranteed to be optimal when $k>1$).
In addition, Bowtie speeds up backtracking by applying case pruning, a simple application of the pigeonhole principle.
However this technique is mostly suited for $k=1$ and requires the index of the forward and reverse text.

Bowtie can be configured to search by strata, however the search time increases significantly while the traversal still misses a large fraction of the search space due to seeding heuristics.
Main practical drawbacks of the tool are too many cryptic options.

Bowtie~2 \citep{Langmead2012} has been designed to quickly report a couple of mapping locations for recent Illumina/Solexa, Ion Torrent and Roche/454 reads, usually having lengths in the range from 100~bp to 400~bp.

This tool uses an heuristic seed-and-extend approach, collecting seeds of fixed length, partially overlapping, and searching them exactly in the reference genome using an FM-index.
Candidate locations to verify are chosen randomly, to avoid uncompressing large CSA intervals and executing many DP instances.
Each mapping location is verified using a striped vectorial dynamic programming algorithm, implemented using SIMD instructions, previously introduced by \citep{Farrar2007} and extended to compute end-to-end alignments.

Bowtie~2 can be configured to report end-to-end or local alignments, scored using a tunable affine scoring scheme.
For this reason, it is believed to be good at reporting alignments containing indels.
However, its completely heuristic filtration method, independent of the scoring scheme, makes it hard to believe what it promises.

% -----------------------------------------------------------------------------

\subsection{BWA}

BWA \citep{Li2009} is designed to map Illumina/Solexa reads up to 100~bp and report a few best end-to-end alignments.
The program performs a greedy breadth-first search on an FM-index of the reference genome.
Nodes to be visited are ranked by edit distance score: the best node is popped from a priority queue and visited, its children are then inserted again in the queue.
The traversal considers indels using a more involved 9-fold recursion.
Backtracking is sped up by adopting a more stringent pruning strategy that nonetheless takes some preprocessing time and requires the index of the reverse reference genome.

BWA performs paired-end alignments by trying to anchor both paired-end reads and verifying the corresponding mate, within an estimated insert size, using the classic DP-based Smith-Waterman algorithm.
Consequently, the program in paired-end mode aligns reads at a slower rate than in single-end mode.
The program is not fully multi-threaded, therefore BWA scales poorly on modern multi-core machines.

BWA-SW \citep{Li2010a} is designed to map Roche/454 reads, which have an average length of 400~bp.
It is an heuristic version of BWT-SW, designed to report a few good local alignments.

This version of BWA adopts a double indexing strategy: it indexes all substrings of one read in a DAWG.
It performs Smith-Waterman of all read substrings directly on the FM-index, by backtracking as soon as no viable alignment can be obtained.
As in BWA-backtrack, the traversal proceeds in a greedy fashion.
In addition, BWA-SW implements some seeding heuristics to limit backtracking and jump in the reference genome to verify candidate locations whenever this becomes favorable.

This version of BWA does not support paired-end reads, presumably because it was meant for Roche/454 reads.

% -----------------------------------------------------------------------------

\subsection{Soap}

Soap~2 \citep{Li2009b} is very similar to Bowtie: it has been designed to produce a very quick but shallow mapping of Illumina/Solexa reads up to 75~bp with no more than 2 mismatches and no indels.
However, its underlying algorithm is based on the so-called bi-directional (or 2-way) BWT.
The tool support paired-end mapping but at a slower alignment rate.
Practical drawbacks are the lack of native output in the de-facto standard SAM format and is closed source.
Soap~3 \citep{Liu2012} is algorithmically similar to Soap~2 but targets only NVIDIA CUDA accelerators.

% -----------------------------------------------------------------------------

\subsection{SHRiMP}

SHRiMP~2 TODO.

% -----------------------------------------------------------------------------

\subsection{RazerS}

RazerS \citep{Weese2009} has been designed to report all mapping locations within a fixed hamming or edit distance error rate.
It is based on a full-sensitive $q$-gram filtration method (SWIFT semi-global) combined with the Myers edit distance verification algorithm.
On demand, the SWIFT filter can be configured to become lossy within a fixed loss rate.
The lossy filter becomes more stringent and produces a lower number of candidates to verify, thus improving the overall speed of the program.
All in all, the SWIFT filter is very slow while not highly specific.

RazerS~3 \citep{Weese2012} is a faster version featuring shared-memory parallelism, a faster banded-Myers verification algorithm, and a faster filtration scheme based on exact seeds that however turns out to be very weak on mammal genomes.
Because of this, RazerS~3 is one-two orders of magnitude slower than Bowtie~2 and BWA-backtrack on mammal genomes.

All RazerS versions index the reads and scan the reference genome.
One positive aspect of this strategy is that no preprocessing of the reference genome is required.
However, other mapping strategies beyond all-mapping, \eg mapping by strata, cannot be efficiently implemented.
Moreover, the program exhibit an high memory footprint as it must remember the mapping locations of all input reads until the whole reference genome has been scanned.

% -----------------------------------------------------------------------------

\subsection{mr(s)Fast}

The tools mrFast \citep{Ahmadi2012} and mrsFast \citep{Hach2010} are designed to report all mapping locations within a fixed absolute number errors, respectively under the hamming and edit distance, given Illumina/Solexa reads of length ranging from 50~bp to 125~bp.
Similarly to RazerS~3, they are based on a full-sensitive filtration method using exact seeds, which turns out to be very weak on mammal genomes.

The peculiarity of their underlying method is a cache-oblivious strategy to mitigate the high cost of verifying clusters of candidate locations.
In addition, mrsFast computes the edit distance between one read and one mapping location in the reference genome with an antidiagonal-wise vectorial dynamic programming algorithm, implemented using SIMD instructions.

These tools are as slow as RazerS~3 and appealing for nothing more than all-mapping.
They lack multi-threading support and exhibit various bugs.
Furthermore, they only accept reads of fixed length and produce files of impractical size.

% -----------------------------------------------------------------------------

\subsection{GEM}

The GEM mapper \citep{MarcoSola2012} is a flexible read aligner for Illumina/Solexa, ABI/SOLiD, and Ion Torrent reads.
It is full-sensitive and can be configured either as an all-mapper, as a best/unique-mapper, or to search by strata.

GEM uses a combination of state of the art approximate string matching methods, \eg approximate seeds and suffix filters.
The program indexes the reference genome with an FM-index, tries to find an optimal filtration scheme per read, and verifies candidate locations using Myers algorithm.
Paired-reads are either mapped independently and then combined, or left/right are mapped and their mates verified using an online strategy.

Unfortunately the tool lacks direct SAM output, it is not open source, and provides many obscure parameters.

% -----------------------------------------------------------------------------

%\subsection{Masai and Yara}

% -----------------------------------------------------------------------------

\begin{landscape}
\begin{table}[h]
  \center
  \sffamily
%  \resizebox{1.0\textwidth}{!}
%  {
	\renewcommand{\tabcolsep}{0.8ex}
	\input{tables/table_mappers}
%  }
\end{table}
\end{landscape}


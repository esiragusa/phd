\chapter{Indexed methods}
\label{chr:index}

Suffix trees are elegant data structures but they are rarely used in practice.
Although suffix trees provides theoretically optimal construction and query time, their high space consumption prohibits practical indexing of large string collections.
A practical study on suffix trees by \cite{Kurtz1999} reports that efficient implementations achieve sizes between $12n$ and $20n$ bytes per character.
For instance, two years before completing the sequencing of the human genome, \citeauthor{Kurtz1999} conjectured the resources required for computing the suffix tree for the complete human genome (consisting of about $3 \cdot 10^9$~bp) in 45.31~GB of memory and nine hours of CPU time, and concluded that \emph{``it seems feasible to compute the suffix tree for the entire human genome on some computers''}.

One might be tempted to think that such memory requirements are not anymore a limiting factor as, at the time of writing, even standard workstations come with 32~GB of main memory.
Indeed, over the last decades, the semiconductors industry followed the exponential trends dictated by Moores' law and yielded not only exponentially faster microprocessors but also larger memories.
Unfortunately, memory latency improvements have been more modest, leading to the so called memory wall effect \citep{Wilkes1995}: data access times are taking an increasingly fraction of total computation times.
Thus, if \cite{Knuth1973} wrote that \emph{``space optimization is closely related to time optimization in a disk memory''}, forty years later one can simply say that space optimization is always related to time optimization.

Over the last years, significant effort has been devoted to the engineering of more space-efficient data structures to replace the suffix tree in practical applications.
In particular, much research has been done into designing succinct (or even compressed) data structures providing efficient query times using space proportional to that of the uncompressed (or compressed) input.
Thanks to these advances, a succinct index of the human genome consumes as little as 3.5~GB of memory and often even improves query time over classic indices.

In this chapter, I introduce some classic full-text indices (suffix arrays and $q$-gram indices) and subsequently succinct full-text indices (uncompressed variants of the FM-index).
Afterwards, I introduce generic string matching algorithms that work on any of these data structures, and at the same time provide their experimental evaluation.
My implementation of all these algorithms and data structures is publicly available in source form within the \CC library SeqAn \citep{Doering2008}.

\section{Classic full-text indices}

\subsection{Suffix array}
\label{sec:index:sa}

The key idea of the suffix array (SA) \citep{Manber1990} is that most information explicitly encoded in a suffix trie is superfluous for string matching.
The explicit representation of suffix trie's internal nodes and outgoing edges can be omitted.
Leaves pointing to the sorted suffixes are sufficient to perform exact string matching or even top-down traversals.
On the SA, any path from the root to an internal node is computed on the fly via binary search over the leaves.
In this way, an additional logarithmic time complexity is paid to reduce space consumption by a constant factor.
I formally define the (generalized) suffix array and later show how to emulate suffix trie traversals.

\begin{definition}
The \emph{suffix array} of a padded string $s$ of length $n$ is an array $A$ containing a permutation of the interval $[1,n]$, \st $s_{A[i] \dots n} <_{lex} s_{A[i+1] \dots n}$ for all $1 \leq i < n$.
\end{definition}

\begin{definition}
The \emph{generalized suffix array} (GSA) of a padded string collection $\Strings$ (definition~\ref{def:coltd}), consisting of $c$ strings of total length $n$, is an array $A$ of length $n$ containing a permutation of all pairs $(i,j)$ where $i$ points to a string $s^i \in \Strings$ and $j$ points to one of the $n_i$ suffixes of $s^i$.
Pairs are ordered \st $\Strings_{A[i] \dots} <_{lex} \Strings_{A[i+1] \dots}$ for all $1 \leq i < n$.
\end{definition}

\begin{figure}[b]
\begin{center}
\caption[Example of (generalized) suffix array]{(Generalized) suffix array. (\subref{fig:sa}) Suffix array of the string {\ttfamily ANANAS\$}. (\subref{fig:gsa}) Generalized suffix array of the string collection $\Strings = \{$ {\ttfamily ANANAS$\$_1$}, {\ttfamily CACAO$\$_2$} $\}$.}

\begin{subfigure}[t]{0.45\textwidth}
\begin{center}
\caption[Example of suffix array]{Suffix array.}
\label{fig:sa}
\ttfamily
\begin{tabular}{ccl}
$i$ & $A[i]$ & $s_{A[i]\dots n}$\\
\midrule
1 & 7 & \$\\
2 & 1 & ANANAS\$\\
3 & 3 & ANAS\$\\
4 & 5 & AS\$\\
5 & 2 & NANAS\$\\
6 & 4 & NAS\$\\
7 & 6 & S\$\\
\end{tabular}
\end{center}
\end{subfigure}%
\begin{subfigure}[t]{0.45\textwidth}
\begin{center}
\caption[Example of generalized suffix array]{Generalized suffix array.}
\label{fig:gsa}
\ttfamily
\begin{tabular}{rcl}
$i$ & $A[i]$ & $\Strings_{A[i]\dots}$\\
\midrule
1 & (1,7) & $\$_1$\\
2 & (2,6) & $\$_2$\\
3 & (2,2) & ACAO$\$_2$\\
4 & (1,1) & ANANAS$\$_1$\\
5 & (1,3) & ANAS$\$_1$\\
6 & (2,4) & AO$\$_2$\\
7 & (1,5) & AS$\$_1$\\
8 & (2,1) & CACAO$\$_2$\\
9 & (2,3) & CAO$\$_2$\\
10 & (1,2) & NANAS$\$_1$\\
11 & (1,4) & NAS$\$_1$\\
12 & (2,5) & O$\$_2$\\
13 & (1,6) & S$\$_1$\\
\end{tabular}
\end{center}
\end{subfigure}

\end{center}
\end{figure}

The SA is constructed in $\Oh(n)$ time, for instance using the \citep{Kaerkkaeinen2003} algorithm, or using non-optimal but practically faster algorithms, \eg \citep{Schuermann2007}.
The space consumption of the suffix array is $n \log{n}$ bits.
When $n < 2^{32}$, a 32 bit integer is sufficient to encode any value in the range $[1,n]$.
Consequently, the space consumption of suffix arrays for texts shorter than 4~GB is $4 n$ bytes.

\cite{Weese2013} gives a generalization of \citeauthor{Kaerkkaeinen2003} algorithm to construct the GSA in $\Oh(n)$ time.
The space consumption of the GSA is $n \log{cn^*}$ bits, where $n^* = \max{\,n_i}$.
For instance, for collections consisting of not more than 256 texts shorter than 4~GB, a pair of $1+4$~bytes suffices to encode any suffix position.

%For instance, the \emph{H.~sapiens} reference genome (hg19) is a collection of 24 sequences, among whose the largest one consists of 248~Mbp.
%Thus, the GSA of hg19 stores pairs of $1+4$~bytes and fits in 15~GB \citep{Weese2013}.

\begin{figure*}[t!]
\begin{minipage}[t]{.5\textwidth}
\begin{algorithm}[H]
\Algorithm{L}{$\StrieNode,c$}
\begin{tabular}{ll}
\textbf{Input}  & $\StrieNode$ : pointer to a suffix array node\\
				& $c$ : character to query\\
\textbf{Output} & integer denoting the left interval\\
\end{tabular}
\begin{algorithmic}[1]
\State {$l_1 \gets \StrieNode.l$}
\State {$l_2 \gets \StrieNode.r$}
\While {$l_1 < l_2$}
	\State {$i \gets \lfloor \frac{l_1+l_2}{2} \rfloor$}
	\If {$\Strings_{A[i] + \StrieNode.d} <_{lex} c$}
		\State {$l_1 \gets i+1$}
	\Else
		\State {$l_2 \gets i$}
	\EndIf
\EndWhile
\State \Return $l_1$
\end{algorithmic}
\label{alg:sa-lower}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{.5\textwidth}
\begin{algorithm}[H]
\Algorithm{R}{$\StrieNode,c$}
\begin{tabular}{ll}
\textbf{Input}  & $\StrieNode$ : pointer to a suffix array node\\
				& $c$ : character to query\\
\textbf{Output} & integer denoting the right interval\\
\end{tabular}
\begin{algorithmic}[1]
\State {$r_1 \gets \StrieNode.l$}
\State {$r_2 \gets \StrieNode.r$}
\While {$r_1 < r_2$}
	\State {$i \gets \lfloor \frac{r_1+r_2}{2} \rfloor$}
	\If {$\Strings_{A[i] + \StrieNode.d} \leq_{lex} c$}
		\State {$r_1 \gets i+1$}
	\Else
		\State {$r_2 \gets i$}
	\EndIf
\EndWhile
\State \Return $r_1$
\end{algorithmic}
\label{alg:sa-upper}
\end{algorithm}
\end{minipage}
\end{figure*}

\subsubsection{Top-down traversal}

I now concentrate on describing suffix trie functionalities, as I implemented them within the SeqAn library.
Any suffix trie node is univocally identified by an interval of the suffix array $A$.
Thus, while traversing the trie, I maintain the interval $[l,r]$ associated to the current node.
In addition, I also remember the depth $d$ of the current node.
The root node is represented by the interval $[1,n]$ containing the whole suffix array.
The label of any edge entering some internal node at depth $d$ is defined by the $d$-th symbol within the corresponding suffix $\Strings_{A[i]}$ with $i \in [l,r]$.
Any leaf node is defined (see section~\ref{sub:introindex}) to have all outgoing edges labeled by terminator symbols.
The occurrences below any node correspond by definition to the interval $[l,r]$ of $A$.
Summing up, I represent the current node $\StrieNode$ by the integers $\{ l, r ,d \}$ and define the following operations on it:
\begin{itemize}
\item \textsc{goRoot}($\StrieNode$) initializes $\StrieNode$ to $\{ 1, n, 0\}$;
%\item \textsc{isRoot}($\StrieNode$) returns true iff $d = 0$;
\item \textsc{isLeaf}($\StrieNode$) returns true iff $A[\StrieNode.r] + \StrieNode.d = n_{A[\StrieNode.r]}$;
\item \textsc{label}($\StrieNode$) returns $\Strings_{A[\StrieNode.l] + \StrieNode.d}$;
\item \textsc{occurrences}($\StrieNode$) returns $A[\StrieNode.l \dots \StrieNode.r]$.
\end{itemize}

\begin{figure*}[t!]
\begin{minipage}[t]{.5\textwidth}
\begin{algorithm}[H]
\Algorithm{goDown}{$\StrieNode$}
\begin{tabular}{ll}
\textbf{Input}  & $\StrieNode$ : pointer to a suffix array node\\
\textbf{Output} & boolean indicating success\\
\end{tabular}
\begin{algorithmic}[1]
\If {\Call{isLeaf}{$\StrieNode$}}
	\State \Return \False
\EndIf
\State {$\StrieNode.d \gets \StrieNode.d+1$}
\State {$\StrieNode.l \gets$ \Call{R}{$\StrieNode,\epsilon$}}
%\While {$A[l] + d < n_{A[l]}$}
%	\State {$l \gets l+1$}
%\EndWhile
%\If {$l \geq r$}
%	\State \Return \False
%\EndIf
\State {$c_l \gets \Strings_{A[\StrieNode.l] + \StrieNode.d}$}
\State {$c_r \gets \Strings_{A[\StrieNode.r] + \StrieNode.d}$}
\If {$c_l \neq c_r$}
	\State {$\StrieNode.r \gets$ \Call{R}{$\StrieNode,c_l$}}
\EndIf
\State \Return $\StrieNode.l < \StrieNode.r$
\end{algorithmic}
\label{alg:sa-godown}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{.5\textwidth}
\begin{algorithm}[H]
\Algorithm{goRight}{$\StrieNode$}
\begin{tabular}{ll}
\textbf{Input}  & $\StrieNode$ : pointer to a suffix array node\\
\textbf{Output} & boolean indicating success\\
\end{tabular}
\begin{algorithmic}[1]
\If {\Call{isRoot}{$\StrieNode$}}
	\State \Return \False
\EndIf
\State {$c_l \gets \Strings_{A[\StrieNode.l] + \StrieNode.d}$}
\State {$c_r \gets \Strings_{A[\StrieNode.r] + \StrieNode.d}$}
\If {$c_l \neq c_r$}
	\State {$\StrieNode.l \gets \StrieNode.r$}
	\State {$\StrieNode.r \gets$ \Call{R}{$\StrieNode,c_l$}}
\EndIf
\State \Return $\StrieNode.l < \StrieNode.r$
\item[]
\end{algorithmic}
\label{alg:sa-goright}
\end{algorithm}
\end{minipage}
\end{figure*}

Binary search is the key to implement function \textsc{goDown} a symbol.
Functions \textsc{L} (algorithm~\ref{alg:sa-lower}) and \textsc{R} (algorithm~\ref{alg:sa-upper}) compute in $\Oh(\log{n})$ binary search steps the position in $A$ of the left and right intervals corresponding to the child node that is reached by going down the edge labeled by a given symbol $c$.
Note that line 6 of algorithms~\ref{alg:sa-lower}--\ref{alg:sa-upper} may involve a comparison beyond the end of strings in $\Strings$, hence I define $t_i$ as the empty word $\epsilon$ if $i > |t|$ and $\epsilon <_{lex} c$ for all $c \in \Sigma$.

\begin{figure*}[b!]
\begin{center}
\begin{minipage}[t]{.6\textwidth}
\begin{algorithm}[H]
\Algorithm{goDown}{$\StrieNode,c$}
\begin{tabular}{ll}
\textbf{Input}  & $\StrieNode$ : pointer to a suffix array node\\
				& $c$ : character to query\\
\textbf{Output} & boolean indicating success\\
\end{tabular}
\begin{algorithmic}[1]
\If {\Call{isLeaf}{$\StrieNode$}}
	\State \Return \False
\EndIf
\State {$\StrieNode.d \gets \StrieNode.d+1$}
\State {$\StrieNode.l \gets \Call{L}{\StrieNode,c}$}
\State {$\StrieNode.r \gets \Call{R}{\StrieNode,c}$}
\State \Return $\StrieNode.l < \StrieNode.r$
\end{algorithmic}
\label{alg:sa-godownc}
\end{algorithm}
\end{minipage}
\end{center}
\end{figure*}

Algorithms~\ref{alg:sa-godown} and \ref{alg:sa-goright} show how to implement respectively \textsc{goDown} and \textsc{goRight} with a time complexity independent of the alphabet size $\sigma$.
As they rely on a single call of \textsc{R}, their time complexity is $\Oh(\log{n})$.
In this way, the SA supports exact string matching (see algorithm~\ref{alg:st-exact}) in $\Oh(m \log{n})$ time.

Note that the suffix array can be binary searched by spelling a full pattern within a single call of \textsc{L} and \textsc{R}: in line 6 of algorithms~\ref{alg:sa-lower}--\ref{alg:sa-upper}, instead of comparing a single character, it suffices to compare the full pattern to the current suffix.
Nonetheless, the worst case runtime of algorithm~\ref{alg:st-exact} stays $\Oh(m \log{n})$, as each step of the binary search requires a full lexicographical comparison between the pattern and any suffix of the text, which takes $\Oh(m)$ time in the worst case.
As shown in \citep{Manber1990}, the worst case runtime can be decreased to $\Oh(m + \log{n})$ at the expense of additional $n \log{n}$ bits, by storing the precomputed longest common prefixes (LCP) between any two consecutive suffixes $\Strings_{A[i] \dots}$, $\Strings_{A[i+1] \dots}$ for all $1 \leq i < n$.

Alternatively, the \emph{average case} runtime is reduced to $\Oh(m + \log{n})$, without storing any additional information, by using the MLR heuristic \citep{Manber1990}.
In practice, the MLR heuristic outperforms the SA + LCP algorithm, due to the higher cost of fetching additional data from the LCP table \citep{Weese2013}.
%Thus I replace algorithm~\ref{alg:st-exact} by algorithm~\ref{alg:sa-exact}.

%\begin{figure}[h]
%\begin{center}
%\caption[Exact string matching on a suffix array]{Exact string matching on a suffix array. The pattern NA is searched exactly in the text ANANAS.}
%\label{fig:sa-exact}
%%\input{figures/stree.tikz}
%\end{center}
%\end{figure}

\subsection{Suffix tree realizations}
\label{sec:index:stree}

I briefly introduce two suffix tree realizations from the literature: the enhanced suffix array (ESA) \citep{Abouelhoda2004} and the lazy suffix tree (LST) \citep{Giegerich2003}.
These realizations explicitly implement or implicitly emulate a suffix tree rather than a suffix trie.
The string matching algorithms of section \ref{sec:index:algo} work on tries, yet any tree can be easily traversed as a trie.
%Here, I only show how to adapt the traversal of any tree as a trie.
The implementation of these data structures within SeqAn, equally generalized to multiple sequence, is due to \citeauthor{Weese2013}.
Hence, for an extensive illustration, the reader is invited to consult \citep{Weese2013}.

\subsubsection{Enhanced suffix array}
\label{sec:index:esa}

The enhanced suffix array (ESA) \citep{Abouelhoda2004} supplements the SA and LCP tables (see section \ref{sec:index:sa}) with another table called \emph{child} table.
Each SA value represents one leaf of the suffix tree, while each LCP value represent the length of one edge of the suffix tree.
What is still missing, in order to represent a full suffix tree, are the SA intervals of the children of each inner node.
These intervals would have to be computed in logarithmic time by \textsc{goRight} during a top-down SA traversal.
As proposed by \cite{Abouelhoda2004}, these intervals are computed in linear time, within one single bottom-up traversal, and stored in the child table, which consumes additional $n \log n$ bits, thus $4n$ bytes for collections smaller than 4 GB.

\subsubsection{Lazy suffix tree}
\label{sec:index:lst}

The lazy suffix tree (LST) \citep{Giegerich1999} variant proposed by \citep{Weese2013} is composed by a partially sorted SA plus a node directory.
The SA initially reflects the ordering of the suffixes up to depth 1, and the node directory table contains only the root node.
During a top-down traversal, the current node at depth $i$ is expanded by means of the \emph{wotd} algorithm \citep{Giegerich1999}, which calls one round of radix sort to refine the ordering of the suffixes up to depth $i + 1$ and inserts the newly computed children nodes in the directory.
The construction of the full LST takes $\Oh(n^2 + \sigma n)$ time in the worst case.

\subsection{$q$-Gram index}
\label{sec:index:qgram}

If the traversal of the idealized suffix trie can be bounded to a maximum depth $q$, the logarithmic factor paid by using the SA vanishes.
The idea is to supplement the SA with a so-called $q$-gram directory: an additional array $D$ of $\Sigma^q + 1$ integers, storing the SA ranges computed by algorithm~\ref{alg:sa-lower} for any possible word of length $q$.

\begin{figure}[b!]
\begin{center}
\caption[Example of $q$-gram index]{$2$-Gram index of the string {\ttfamily ANANAS\$} over the alphabet $\Sigma = \{ A, N, S \}$. The example shows the lookup of the $2$-gram {\ttfamily NA}. The hash value $h(\text{\ttfamily NA})=4$ addresses two lookups in $D[4]$ and $D[5]$, that in turn provide the range $[5,6]$ in $A$.}
\label{fig:qgram}
\ttfamily
\begin{tabular}{ccccccl}
$p$ & $h(p)$ & $D[h(p)]$ & \phantom{-} & $i$ & $A[i]$ & $s_{A[i]\dots n}$\\
\midrule
AA & 1 & 2 & & 1 & 7 & \$\\
AN & 2 & 2 & & 2 & 1 & ANANAS\$\\
AS & 3 & 4 & & 3 & 3 & ANAS\$\\
\cell{p}{NA} & \cell{h4}{4} & \cell{d5}{5} & & 4 & 5 & AS\$\\
NN & \cell{h5}{5} & \cell{d6}{6} & & \cell{i5}{5} & \cell{a5}{2} & NANAS\$\\
NS & 6 & 6 & & \cell{i6}{6} & \cell{a6}{4} & NAS\$\\
SA & 7 & 6 & & 7 & 6 & S\$\\
SN & 8 & 6 \\
SS & 9 & 6 \\
   & 10 & 6 \\
\end{tabular}
\around{p}
\eround{h4}{h5}
\eround{d5}{d6}
\eround{i5}{i6}
\eround{a5}{a6}
\end{center}
\end{figure}

With the aim of addressing $q$-grams in the directory $D$, I impose a canonical code on $q$-grams through a bijective function $h : \Sigma^q \rightarrow [1, \sigma^q]$ defined as in \citep{Knuth1973}:
\begin{eqnarray}
h(p) = 1 + \sum_{i=1}^{q}{\rho_0(p_i) \cdot \sigma^{q-i}}
\end{eqnarray}
where $p \in \Sigma^q$ is any $q$-gram and $\rho_0$ is the zero-based lexicographic rank defined on $\Sigma$ (recall the lexicographic rank function $\rho$ from definition~\ref{def:rho} and pose $\rho_0(x) = \rho(x) - 1$).
The canonical code assigned by $h$ preserves the lexicographical ordering for all words not longer than $q$, \ie $v <_{lex} w$ iff $h(v) < h(w)$ for all $v,w \in \Sigma^{\leq q}$.
The hash function $h$ allows to store in and retrieve from $D$ the left SA interval returned by algorithm~\ref{alg:sa-lower} for each $q$-gram, \ie $p \in \Sigma^q$, $D[h(p)] = \textsc{L}(1,n,p)$.
Note that the right interval returned by algorithm~\ref{alg:sa-upper} is equivalent to the left interval of the lexicographical successor $q$-gram and therefore available in $D[h(p)+1]$.

In practice, the $q$-gram index is applicable only to relatively small alphabets and tree depths.
For instance, parameters $|\Sigma| = 4$ and $q=14$ require a $q$-gram directory consisting of 268~M entries.
Using a 32 bits integer encoding, the directory alone consumes 1~GB of memory.

\subsubsection{Top-down traversal}

I now describe how I extended the SA traversal operations of section \ref{sec:index:sa} to use the $q$-gram directory $D$, within the generic text indexing framework of the SeqAn library.
Again, while traversing the trie, I maintain the current range $[l,r]$ and the current depth $d$.
In addition, I maintain the interval $[l_h,r_h]$ in $D$ and, in order to answer \textsc{label}$(\StrieNode)$, the label $e$ of the edge entering the current node.
Summing up, I represent the current node $\StrieNode$ by the elements $\{ l, r, d, l_h, r_h, e \}$.
I define the basic node operations as follows:
\begin{itemize}
\item \textsc{goRoot}($\StrieNode$) initializes $\StrieNode$ to $\{ 1, n, 0, 1, \sigma^q, \epsilon \}$;
\item \textsc{isLeaf}($\StrieNode$) returns true iff $\StrieNode.d=q$;
\item \textsc{label}($\StrieNode$) returns $\StrieNode.e$;
\item \textsc{occurrences}($\StrieNode$) returns $A[\StrieNode.l \dots \StrieNode.r]$.
\end{itemize}

\begin{figure*}[b!]
\begin{minipage}[b]{.5\textwidth}
\begin{algorithm}[H]
\Algorithm{L}{$\StrieNode,c$}
\begin{tabular}{ll}
\textbf{Input}  & $\StrieNode$ : pointer to a $q$-gram node\\
				& $c$ : character to query\\
\textbf{Output} & integer denoting the left interval\\
\end{tabular}
\begin{algorithmic}[1]
\State {$\StrieNode.l_h \gets \StrieNode.l_h + \rho_0(c) \cdot \sigma^{\StrieNode.d} $}
\State \Return $D[\StrieNode.l_h]$
\end{algorithmic}
\label{alg:qgram-lower}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[b]{.5\textwidth}
\begin{algorithm}[H]
\Algorithm{R}{$\StrieNode,c$}
\begin{tabular}{ll}
\textbf{Input}  & $\StrieNode$ : pointer to a $q$-gram node\\
				& $c$ : character to query\\
\textbf{Output} & integer denoting the right interval\\
\end{tabular}
\begin{algorithmic}[1]
\State {$\StrieNode.r_h \gets \StrieNode.r_h - \rho_0(c) \cdot \sigma^{\StrieNode.d}$}
\State \Return $D[\StrieNode.r_h]$
\end{algorithmic}
\label{alg:qgram-upper}
\end{algorithm}
\end{minipage}
\end{figure*}

Algorithms \ref{alg:qgram-lower} and \ref{alg:qgram-upper} respectively show functions \textsc{L} and \textsc{R} using the directory $D$ instead of $A$.
In this way, both variants of \textsc{goDown} (algorithms \ref{alg:sa-godownc} and \ref{alg:sa-godown}) and \textsc{goRight} (algorithm \ref{alg:sa-goright}) take $\Oh(1)$ time.

\begin{figure*}[t]
\begin{center}
\begin{minipage}[t]{.6\textwidth}
\begin{algorithm}[H]
\Algorithm{goDown}{$\StrieNode,c$}
\begin{tabular}{ll}
\textbf{Input}  & $\StrieNode$ : pointer to a $q$-gram node\\
				& $c$ : character to query\\
\textbf{Output} & boolean indicating success\\
\end{tabular}
\begin{algorithmic}[1]
\If {\Call{isLeaf}{$\StrieNode$}}
	\State \Return \False
\EndIf
\State {$\StrieNode.d \gets \StrieNode.d+1$}
\State {$\StrieNode.e \gets c$}
\State {$\StrieNode.l \gets \Call{L}{\StrieNode,\StrieNode.e}$}
\State {$\StrieNode.r \gets \Call{R}{\StrieNode,\StrieNode.e}$}
\State \Return $\StrieNode.l < \StrieNode.r$
\end{algorithmic}
\label{alg:qgram-godownc}
\end{algorithm}
\end{minipage}
\end{center}
\end{figure*}

%Exact string matching algorithm~\ref{alg:st-exact} runs in time $\Oh(q)$, nonetheless it can be improved to perform $\Oh(1)$ lookups in $D$.
%It suffices to compute the canonical code of the pattern in one step, as shown in algorithm~\ref{alg:qgram-exact}.

%\begin{center}
%\begin{minipage}[t]{.8\textwidth}
%\begin{algorithm}[H]
%\Algorithm{ExactSearch}{$t,p$}
%\begin{tabular}{ll}
%\textbf{Input}  & $t$ : pointer to the root node of the $q$-gram index of the text\\
%				& $p$ : string to query\\
%\textbf{Output} & list of all occurrences of the pattern in the text\\
%\end{tabular}
%\begin{algorithmic}[1]
%\State {$l \gets D[h(p)]$}
%\State {$r \gets D[h(p) + 1]$}
%\State \Report $A[l \dots r]$
%\end{algorithmic}
%\label{alg:qgram-exact}
%\end{algorithm}
%\end{minipage}
%\end{center}

The directory $D$ alone is sufficient for top-down traversals bounded to a maximum depth $q$;
the suffix array $A$ is accessed only to locate text locations pointed by the leaves.
In this case, the total ordering of the text suffixes in the SA can be relaxed to prefixes of length $q$.
This gives a twofold advantage, as one can
\begin{inparaenum}[(i)]
\item construct the SA more efficiently using bucket sorting and
\item maintain leaves in each bucket sorted by their relative text positions.
\end{inparaenum}
The latter property allows to compress the SA bucket-wise \eg using Elias $\delta$ encoding \citep{Elias1975} or to devise cache-oblivious strategies to process the occurrences \citep{Hach2010}.

The directory $D$ is still usable, even if the traversal needs to go below depth $q$.
An hybrid traversal can use the directory $D$ up to depth $q$ and later continue with binary searches on the suffix array $A$.
This hybrid traversal cuts the most expensive binary searches and increases memory locality.
Furthermore, this traversal becomes useful whenever the SA is too big to fit in main memory and has to reside in external memory.

\subsection{Trie and radix tree realizations}
\label{sec:index:trie}

Before turning to succinct full-text indices, I briefly describe how I reused the text-indices just exposed to implement tries and radix trees in the SeqAn library.
A trie is easily emulated by means of a partial SA.
I index only the first suffix of each string in the collection and subsequently construct the SA-based trie via quicksort in time $\Oh(n \log n)$, where $n$ is the total length of the string collection.
The top-down traversal based on binary search still works as described in section \ref{sec:index:sa}.
This trie is also extendable by a $q$-gram directory as in section \ref{sec:index:qgram}.
A radix tree is constructed in an analogous way, starting from the LST of section \ref{sec:index:lst}.
I fill the LST's partial SA as described above and subsequently apply the \emph{wotd} algorithm \citep{Giegerich1999} to construct the radix tree in time $\Oh(\sigma n)$.


\section{Succinct full-text indices}

The \emph{Burrows-Wheeler transform} (BWT) \citep{Burrows1994} is a transformation defining a permutation of an input string.
Such transformation exposes two important properties: it is \emph{reversible} and it tends to produce \emph{runs of equal characters}.
The former property allows to recover the original string from its BWT alone, while the latter property makes the transformed string more amenable to compression.
Because of these two properties, the BWT is a fundamental method for text compression.
%and is practically used in the bzip2 tool \citep{Seward1996}.

Some years after its introduction, \citeauthor{Ferragina2000} proposed the BWT for full-text indexing.
They show in \citep{Ferragina2000} that the BWT allows to perform exact string matching and engineer in \citep{Ferragina2001} a compressed full-text index called FM-index.
Over the last years, the FM-index has been widely employed under different re-implementations by many popular bioinformatics tools \eg Bowtie \citep{Langmead2009} and BWA \citep{Li2009}, and is now considered a fundamental method for the indexing of genomic sequences.

In the following, I give the fundamental ideas behind the BWT.
Subsequently, I discuss my generalized FM-index implementation covering strings and string collections.

\subsection{Burrows-Wheeler transform}
\label{sec:index:bwt}

Let $s$ be a padded string (definition~\ref{def:strd}) of length $n$ over an alphabet $\Sigma$.
In the following, consider the string $s$ to be cyclic and its subscript $s_i$ to be \emph{modular}, \eg $s_0 = s_{n}$ and $s_{n+i} = s_i$ for any $i \in \N$.
Consider the square matrix consisting of all cyclic shifts of the string $s$ sorted in lexicographical order, where the $i$-th cyclic shift has the form $s_{i \dots n} s_{1 \dots i-1}$.
Figure~\ref{fig:bwt} shows an example.
Note how the cyclic shifts matrix is related to the suffix array $A$ of $s$: the $i$-th cyclic shift is $s_{A[i] \dots n} s_{1 \dots A[i]-1}$.
% (except when $A[i] = 1$).

\begin{definition}
\label{def:bwt}
The BWT of $s$ is the string $s_{A[i]-1}$, \ie the string obtained concatenating the symbols in the last column of the cyclic shifts matrix of $s$.
\end{definition}

\begin{figure}[t]
\begin{center}
\caption[Example of Burrows-Wheeler transform]{Cyclic shifts matrix of the string {\ttfamily ANANAS\$}. Column $s_{A[i]-1}$ represents the Burrows-Wheeler transform.}
\label{fig:bwt}
\ttfamily
\begin{tabular}{cccccc}
$i$ & $A[i]$ & \phantom{-} & $s_{A[i]}$ & $\dots$ & $s_{A[i]-1}$\\
\midrule
1 & 7 & & \$& ANANA  & \cell{l1}{S}\\
2 & 1 & & A & NANAS  & \$\\
3 & 3 & & A & NAS\$A & N\\
4 & 5 & & A & S\$ANA & N\\
5 & 2 & & N & ANAS\$ & A\\
6 & 4 & & N & AS\$AN & A\\
7 & 6 & & S & \$ANAN & \cell{l7}{A}\\
\end{tabular}
\eround{l1}{l7}
\end{center}
\end{figure}

The BWT easily generalizes to string collections.
Indeed, definition~\ref{def:bwt} still holds for a padded string collection $\Strings$ (definition~\ref{def:coltd}) and its cyclic shifts matrix sorted in lexicographical order.

The cyclic shifts matrix is conceptual and does not have to be constructed explicitly to derive the BWT.
The BWT can be obtained in linear time by scanning the suffix array $A$ and assigning the symbol $s_{A[i]-1}$ to the $i$-th BWT symbol.
However, constructing the BWT from the SA is still not desirable, especially for small alphabets, as the SA consumes $n \log{n}$ bits in addition to the $n \log{\sigma}$ bits of the BWT.
Therefore, various direct BWT construction algorithms working within $o(n \log{\sigma})$ bits plus constant space have been recently proposed in \citep{Bauer2013, Crochemore2013}.

\begin{figure}[b]
\begin{center}
\caption[Example of functions $LF$ and $\Psi$]{Functions $LF$ and $\Psi$ of the string {\ttfamily ANANAS\$}. The example shows that $LF = \Psi^{-1}$, \eg $LF(\Psi(5)) = 5$ and $\Psi(LF(3)) = 3$. Moreover, the example shows that the relative order of characters between $l$ and $r$ is preserved, \eg the first occurrence of $N$ in $l$ corresponds to the first occurrence in $f$.}
\label{fig:lf_psi}
\ttfamily
\begin{tabular}{ccccccc}
$i$ & $\Psi(i)$ & $LF(i)$ & \phantom{-} & $s_{A[i]}$ & $\dots$ & $s_{A[i]-1}$\\
\midrule
1 & 2 & 7                        & & \$           & ANANA  & S\\
2 & 5 & 1                        & & A            & NANAS  & \$\\
3 & 6 & \cell{lf3}{5} & & A      & NAS\$A & \cell{l3}{N}\\
4 & 7 & 6                        & & A            & S\$ANA & N\\
5 & \cell{psi5}{3} & 2           & & \cell{f5}{N} & ANAS\$ & A\\
6 & 4 & 3                        & & N            & AS\$AN & A\\
7 & 1 & 4                        & & S            & \$ANAN & A\\
\end{tabular}
\around{lf3}
\around{psi5}
\around{f5}
\around{l3}
\end{center}
\end{figure}

\subsubsection{Inversion}

I now describe how to invert the BWT to reconstruct the original string.
For convenience, I denote the first column $s_{A[i]}$ by $f$ and the last column $s_{A[i] - 1}$ by $l$.
Inverting the BWT means being able to know where any BWT character occurs in the original text.
To this intent, I define two permutations $LF : [1,n] \rightarrow [1,n]$ and $\Psi : [1,n] \rightarrow [1,n]$, with $LF = \Psi^{-1}$, where the value of $LF(i)$ gives the position $j$ in $f$ where character $l_i$ occurs and the value $\Psi(j)$ gives back the position $i$ in $l$ where $f_j$ occurs.
Figure~\ref{fig:lf_psi} illustrates.
I define the iterated $\Psi$ as
\begin{eqnarray}
\begin{array}{lcl}
\Psi^0(j)     &=& j\\
\Psi^{i+1}(j) &=& \Psi(\Psi^{i}(j))
\end{array}
\end{eqnarray}
and the iterated $LF$ as
\begin{eqnarray}
\begin{array}{lcl}
LF^0(j)     &=& j\\
LF^{i+1}(j) &=& LF(LF^{i}(j)).
\end{array}
\end{eqnarray}


The character $s_i$ is recovered as $f_{\Psi^{i-1}(j)}$, while $\bar{s}_i$ is recovered as $l_{LF^{i-1}(j)}$.
The full string $s$ is recovered by starting in $f$ at the position of \$ and following the cycle defined by the permutation $\Psi$.
Conversely, the reverse string $\bar{s}$ is recovered by starting in $l$ at the position of \$ and following the cycle defined by the permutation $LF$.
Figure \ref{fig:psi} exemplifies.

\begin{figure}[t]
\begin{center}
\caption[Example of BWT inversion]{Recovering the string {\ttfamily ANANAS\$} from the permutation $\Psi$. The example shows only the first two steps of the inversion recovering {\ttfamily AN}.}
\label{fig:psi}
\ttfamily
\begin{tabular}{ccc}
$s_{A[i]}$ & $i$ & $\Psi(i)$\\
\midrule
\cell{s1}{\$} & \cell{i1}{1} & \cell{psi1}{2}\\
\cell{s2}{A}  & \cell{i2}{2} & \cell{psi2}{5}\\
\cell{s3}{A}  & \cell{i3}{3} & \cell{psi3}{6}\\
\cell{s4}{A}  & \cell{i4}{4} & \cell{psi4}{7}\\
\cell{s5}{N}  & \cell{i5}{5} & \cell{psi5}{3}\\
\cell{s6}{N}  & \cell{i6}{6} & \cell{psi6}{4}\\
\cell{s7}{S}  & \cell{i7}{7} & \cell{psi7}{1}\\
\end{tabular}
\link{s1}{i1}
\linkdown{i1}{psi1}{i2}
\linkdown{i2}{psi2}{i5}
%\linkup{i5}{psi5}{i3}
\around{s2}
\around{s5}
\end{center}
\end{figure}

Inverting the generalized BWT works in the same way.
Indeed, permutations $\Psi$ and $LF$ are composed of $c$ cycles, where each cycle corresponds to a distinct string in the collection.
The string $s^j$ is recovered by starting at the position of $\$^j$ and following the cycle of $\Psi$ (or $LF$) associated to $s^j$.

\subsubsection{Permutation LF}

Permutation $LF$ is conceptual: it is not necessary to encode it explicitly.
Luckily, it is possible to deduce it from the BWT with the help of some additional character counts.
This is possible due to two simple observations on the cyclic shifts matrix.

\begin{observation}
\label{obs:lf-a}
\citep{Burrows1994}
For all $i \in [1,n]$, the character $l_i$ precedes the character $f_i$ in the original string $s$.% (except when $l_i = \$$);
\end{observation}
\begin{observation}
\label{obs:lf-b}
\citep{Burrows1994}
For all characters $c \in \Sigma$, the $i$-th occurrence of $c$ in $f$ corresponds to the $i$-th occurrence of $c$ in $l$.
\end{observation}

These observation are evident, indeed $f = s_{A[i]}$ and $l = s_{A[i] - 1}$ (see figure~\ref{fig:lf_psi}).
Given the two above observations, \cite{Ferragina2000} define the permutation $LF$ as:
\begin{eqnarray}
LF(i) = C(l_i) + Occ(l_i, i)
\end{eqnarray}
where $C : \Sigma \rightarrow [1,n]$ denotes the total number of occurrences in $s$ of all characters alphabetically smaller than $c$, and $Occ :  \Sigma \times [1,n] \rightarrow [1,n]$ the number of occurrences of character $c$ in the prefix $l_{1 \dots i}$.
The key problem of encoding the permutation $LF$ lies in representing function $Occ$, as function $C$ is easily tabulated by a small array of size $\sigma \log{n}$ bits.
In the next subsection, I address the problem of representing function $Occ$ efficiently. Subsequently, I explain how to implement generic full-text index traversal using the permutation $LF$.

\subsection{Rank dictionaries}
\label{sec:index:succinct:rd}

The question \emph{``how many times a given character $c$ occurs in the prefix $l_{1 \dots i}$?''}, has to be answered efficiently, ideally in constant time and linear space.
The general problem on arbitrary strings has been tackled by several studies on the succinct representation of data structures \citep{Jacobson1989}.
This specific question takes the name of \emph{rank query} and a data structure answering rank queries is called \emph{rank dictionary} (RD).

\begin{definition}
Given a string $s$ over an alphabet $\Sigma$ and a character $c \in \Sigma$, $\rank_c(s, i)$ returns the number of occurrences of $c$ in the prefix $s_{1 \dots i}$.
\end{definition}

The key idea of RDs is to maintain a succinct (or even compressed) representation of the input string and attach a dictionary to it.
By doing so, \cite{Jacobson1989} shows how to answer rank queries in constant time (on the RAM model) using $n+o(n)$ bits for an input binary string of $n$ bits.
Here, I cover only the most practical succinct RDs and discuss some implementation aspects, crucial to obtain practical efficiency.
I first consider the binary alphabet $\mathbb{B} = \{ 0, 1 \}$ and subsequently the case of an arbitrary alphabet.

\subsubsection{Binary alphabet}
\label{sec:index:rd:binary}

Here, I follow the explanation of \citep{Navarro2007}.
Hence, I start by describing a simple \emph{one-level} rank dictionary answering rank queries in constant time but consuming $2n$ bits.
Subsequently, I describe an extended \emph{two-levels} RD consuming only $n + o(n)$ bits.
In addition to that, I briefly discuss my implementation of practical \emph{multi-level} RDs.
%In the following, I assume \wlogs the string length $n$ to be a multiple of $b$, otherwise values in all subsequent formulas must be rounded accordingly.

The binary one-level RD partitions the binary input string $s \in \mathbb{B}^*$ in blocks of $b$ symbols and
complements it with an array $R$ of length $\xfloor{n/b}$.
The $j$-th entry of $R$ provides a summary of the number of occurrences of the bit $1$ in $s$ before position $j b$, \ie $R[1] = 0$ and $R[j] = \rank_1(s, j b - 1)$ for any $j > 1$.
$R$ summarizes only $\rank_1$, as $\rank_0(s, i) = i - \rank_1(s, i)$.
Therefore, the rank query is rewritten as:
\begin{eqnarray}
\rank_1(s,i) = R[\xfloor{i/b}] + \rank_1(s_{\xfloor{i/b} \dots \xfloor{i/b}+b}, \, i \imod{b}).
\end{eqnarray}
The query is answered in time $\Oh(b)$ by 
\begin{inparaenum}[(i)]
\item \label{itm:fetch} fetching the rank summary from $R$ in constant time and
\item \label{itm:count} counting the number of occurrences of the bit 1 within a block of $\Oh(b)$ bits.
\end{inparaenum}
Figure \ref{fig:rd1} illustrates.
\citeauthor{Jacobson1989} poses $b=\log{n}$ in order to answer step (\ref{itm:count}) in time $\Oh(1)$ with the four-Russians tabulation technique \citep{Arlazarov1970}.
As the array $R$ stores $\xfloor{n/\log{n}}$ positions and each position in $s$ requires $\log{n}$ bits, $R$ consumes $n$ bits.
Thus, the binary one-level RD consumes $2n$ bits.

\begin{figure}[t]
\begin{center}
\caption[Example of binary rank dictionaries]{Binary rank dictionaries (RDs) of the string $s=$ {\ttfamily 010101100100}. (\subref{fig:rd1}) One-level RD with $b=4$; in the example, $\rank_1(s, 6) = R[2] + \rank_1(s_{5 \dots 8}, 2) = 3$. (\subref{fig:rd2}) Two-levels RD with $b=2$ (note that $R$ is now different from $b=4$); in the example, $\rank_1(s, 6) = R^2[2] + R[3] + \rank_1(s_{5 \dots 8}, 1) = 3$.}

\begin{subfigure}[b]{0.45\textwidth}
\begin{center}
\caption{One-level rank dictionary.}
\ttfamily
\begin{tabular}{rcc}
$i$	& $s_i$	& $R[\xfloor{i/4}]$\\
\midrule
1   & 0     & 0\\
2   & 1\\
3   & 0\\
4   & 1\\
5   & \cell{s5}{0} & \cell{R5}{2}\\
\cell{i6}{6} & \cell{s6}{1}\\
7   & 1\\
8   & 0\\
9   & 0     & 4\\
10  & 1\\
11  & 0\\
12  & 0\\
\end{tabular}
\around{i6}
\eround{s5}{s6}
\around{R5}
\label{fig:rd1}
\end{center}
\end{subfigure}%
\begin{subfigure}[b]{0.45\textwidth}
\begin{center}
\caption{Two-levels rank dictionary.}
\ttfamily
\begin{tabular}{rccc}
$i$	& $s_i$	& $R[\xfloor{i/2}]$ & $R^2[\xfloor{i/4}]$\\
\midrule
1   & 0 & 0 & 0\\
2   & 1\\
3   & 0	& 1\\
4   & 1\\
5   & \cell{s5}{0} & \cell{Rb5}{0} & \cell{R5}{2}\\
\cell{i6}{6} & \cell{s6}{1}\\
7   & 1 & 1\\
8   & 0\\
9   & 0 & 0 & 4\\
10  & 1\\
11  & 0 & 1\\
12  & 0\\
\end{tabular}
\around{i6}
\eround{s5}{s6}
\around{R5}
\around{Rb5}
\label{fig:rd2}
\end{center}
\end{subfigure}

\end{center}
\end{figure}

A binary \emph{two-levels} RD squeezes space consumption down to $n+o(n)$ bits.
The idea is to add another array $R^2$ summarizing the ranks on $b^2$ bits boundaries and let the initial array $R$ store only local positions within the corresponding blocks defined by $R^2$.
Accordingly, the rank query becomes:
\begin{eqnarray}
\rank_1(s,i) = R^2[\xfloor{i/b^2}] + R[\xfloor{i/b}] + \rank_1(s_{\xfloor{i/b} \dots \xfloor{i/b}+b}, \, i \imod{b}).
\end{eqnarray}
Figure \ref{fig:rd2} exemplifies.
Each entry of $R$ now represents only $b^2$ possible values and thus consumes only $2\log{b}$ bits.
Summing up, this RD consumes $n$ bits for the input string, $\Oh(\frac{n \log{n}}{b^2})$ bits for $R^2$ and $\Oh(\frac{n \log{b}}{b})$ bits for $R$.
By posing $b=\log{n}$ as above, it follows $\Oh(\frac{n}{\log{n}})$ bits for $R^2$ and $\Oh(\frac{n \log{\log{n}}}{\log{n}})$ bits for $R$.
Hence, the binary two-levels RD consumes $n + o(n)$ bits.

I implemented generic \emph{multi-levels} RDs, where the block size $b$ is a template parameter adjustable at compile time.
Typically, the input string is smaller than 4~GB, thus I employ the one-level RD with $b = 32$ bits or the two-level RD with $b = 16$ bits and $b^2 = 32$ bits;
otherwise, for longer strings, I employ the two-levels RD with $b = 32$ bits and $b^2 = 64$ bits, or the three-levels RD with $b = 16$ bits, $b^2 = 32$ bits and $b^3 = 64$ bits.
In order to reduce the number of cache misses, the succinct representation of the input string is \emph{interleaved} with the lowest level summaries array $R$.
Moreover, I use the SSE~4.2 popcnt instruction \citep{Intel2011} to count symbols within a block in time $\Oh(b/w)$, where $w$ is the total SSE register width (on modern processors $w=256$ bits).

\subsubsection{Small alphabets}

The extension of binary RDs to arbitrary alphabets is easy.
However, the space consumption of such RD has a linear dependency in the alphabet size.
This fact renders such extension appealing only for small alphabets, \eg $\Sigma_{\text{DNA}}$.
Here, I show how to extend the one-level RD.

Consider an input string $s$ of length $n$ over $\Sigma$, thus consisting of $n \log{\sigma}$ bits.
As in the binary case, this one-level RD partitions $s$ in blocks of $b$ symbols.
It complements the string $s$ with a matrix $R_{\sigma}$ of size $\xfloor{n/b} \times \sigma$ entries, summarizing the number of occurrences for each symbol in $\Sigma$.
The rank query is rewritten accordingly:
\begin{eqnarray}
\rank_c(s,i) = R_{\sigma}[\xfloor{i/b},\rho(c)] + \rank_c(s_{\xfloor{i/b} \dots \xfloor{i/b}+b}, \, i\imod{b}).
\end{eqnarray}
Figure \ref{fig:rd_dna} shows an example of one-level DNA RD.
Answering this query requires counting the number of occurrences of the character $c$ inside a block of $\Oh(b)$ symbols.
In order to answer this query in constant time, I consider blocks of $\xfloor{\log{n}/\log{\sigma}}$ symbols, \ie I pose $b=\log{n}$ bits as in the binary RD case.
The matrix $R_{\sigma}$ has thus $\xfloor{n \log{\sigma}/\log{n}} \times \sigma$ entries, each one consuming $\log{n}$ bits.
Thus, $R_{\sigma}$ consumes $\sigma n \log{\sigma}$ bits and the whole RD $n \log{\sigma} (\sigma + 1)$ bits.

\begin{figure}[t]
\begin{center}
\caption[Example of one-level DNA rank dictionary]{One-level DNA rank dictionary of the string $s=$ {\ttfamily CTCGCA} with $b=2$. In the example, $rank_{\text{C}}(s,4)= R_{\sigma}[2,2] + rank_{\text{C}}(s_{3 \dots 4}, 1)=2.$}
\label{fig:rd_dna}
\ttfamily
\begin{tabular}{rcr}
$i$	& $s_i$ & $R_{\sigma}[\xfloor{i/2}, \{\text{A}, \, \text{C}, \, \text{G}, \, \text{T}\}]$\\
\midrule
1   & C & $[0, \,0, \,0, \,0]\phantom{]}$\\
2   & T\\
3   & \cell{s3}{C} & $[0, \, \cell{R3C}{1}, \, 0, \, 1]\phantom{]}$\\
\cell{i4}{4}   & \cell{s4}{G}\\
5   & C & $[0, \,2, \,1, \,1]\phantom{]}$\\
6   & A\\
\end{tabular}
\around{i4}
\eround{s3}{s4}
\around{R3C}
\end{center}
\end{figure}

%In practice, I encode the DNA alphabet $\{ A, C, G, T \}$ as $\{ 00, 01, 10, 11 \}$.

\subsubsection{Wavelet tree}

\citeauthor{Grossi2003} propose a \emph{hierarchical} RD, called the \emph{wavelet tree} (WT), to mitigate the factor $\sigma$ affecting the RD just exposed.
This tree data structure recursively partitions the alphabet $\Sigma$ in balanced subsets and therefore decomposes the input string in \emph{subsequences} containing symbols from one subset.
Any tree node represents one alphabet partition and its associated subsequence.
I first give the formal definition of WT and then discuss how to answer rank queries.

\begin{definition}
\citep{Grossi2003, Navarro2007}
The wavelet tree of a string $s \in \Sigma^*$ is a balanced binary tree of height $\lceil \log \sigma \rceil$. The root represents all symbols in $\Sigma$ and each leaf exactly one symbol $c \in \Sigma$.
Any non-leaf node $v$ represents some subset of symbols $\Sigma_v$ whose lexicographic rank is in range $[i,j]$ \ie $\Sigma_v = \{ c \in \Sigma : \rho(c) \in [i,j] \}$, its left child $l$ represents the subset $\Sigma_l$ of symbols in range $[i,\frac{i+j}{2}]$ while its right child $r$ represents in $\Sigma_r$ those in range $[\frac{i+j}{2}+1, j]$.
Node $v$ implicitly represents the subsequence $s^v$ of all symbols of $s$ in $\Sigma_v$ and explicitly encodes its decomposition as a binary string $b^v$ \st $b^v_i = 0$ if $s^v_i \in \Sigma_l$ and 1 otherwise.
\end{definition}

\begin{figure}[b]
\begin{center}
\caption[Example of wavelet tree]{Wavelet tree of the DNA string $s=$ {\ttfamily CTCGCA}. The alphabet $\Sigma_{\text{DNA}}$ is recursively partitioned as $\{ \{A,C\}, \{G,T\} \}$. In the example, $\rank_{\text{C}}(s,4)=2$ is decomposed as $\rank_0(b,4) = 2$ on the root node and then $\rank_1(b^{AC}, 2) = 2$ on the left inner node.}
\label{fig:wt_dna}
\ttfamily
\input{figures/wavelet_tree.tikz}
\around{i4}
\eround{b1}{b4}
\around{iac2}
\eround{bac1}{bac2}
\end{center}
\end{figure}

Any query $\rank_c(s,i)$ is decomposed as a sequence of $\Oh(\log{\sigma})$ binary rank queries.
The sequence of queries starts in the root node and follows the path to the leaf corresponding to symbol $c$.
On any non-leaf node $v$, the traversal goes left if $c$ belongs to $\Sigma_l$, otherwise it goes right.
Suppose \wlogs that $c$ belongs to $\Sigma_l$.
The rank of symbol $c$ in $s^v$ is established as $\rank_0(b^v, j)$, where $j$ is the rank of $c$ in the parent node or $i$ in the root node.
Figure \ref{fig:wt_dna} illustrates.

The WT encodes any binary string $b^v$ associated to some non-leaf node $v$ using a separate binary RD.
The WT contains $\lceil \log \sigma - 1 \rceil$ non-leaf levels and any such level encodes $n$ bits overall.
Using two-levels binary RDs (section~\ref{sec:index:rd:binary}), the WT consumes $(n + o(n)) \log \sigma$ bits, \ie $n \log \sigma (1 + o(1))$ bits, and answers any rank query in time $\Oh(\log{\sigma})$.
At the same time, the WT does not need to store the original input string $s$ of $n \log \sigma$ bits.
%The more involved analysis by \citep{Grossi2003} shows that the WT is a \emph{compressed} RD, \ie its space consumption is proportional to that of the \emph{zero-order empirical entropy} \citep{Manzini2001} of the input string.

\subsection{FM-index}
\label{sub:fmtrie}

I now come back to the problem of implementing a full-text index based on the permutation $LF$.
%This is possible because of the relationship between the cyclic shifts matrix and the suffix array.
First, I show how to emulate a top-down traversal of the suffix trie, which is sufficient to count the number of occurrences of any substring in the original text.
Later, I focus on how to represent the leaves of the suffix trie, which are necessary to locate occurrences in the original text.

\subsubsection{Top-down traversal}

Given a padded string collection $\Strings$, as shown in section \ref{sec:index:bwt}, its associated permutation $LF$ recovers substrings of $\bar{\Strings}$, \ie substrings of $\Strings$ in \emph{backward} direction.
Nonetheless, the top-down traversal needs to recover the substrings of $\Strings$ in \emph{forward} direction, as the suffix trie $\StrieOf{\Strings}$ spells all forward substrings of $\Strings$.
Therefore, I consider the BWT of $\bar{\Strings}$, such that $LF$ recovers any substring of $\Strings$.
To encode $LF$, I supplement the BWT of $\bar{\Strings}$ with its rank dictionary, either multi-levels or wavelet tree.
In this way, the top-down traversal is able to use the permutation $LF$ to decode $SA$ intervals.

I represent the current node $\StrieNode$ by the elements $\{ l, r, e \}$, where $[l,r]$ represents the current suffix array interval and $e$ is the label of the edge entering the current node.
Therefore, I define the following node operations:
\begin{itemize}
\item \textsc{goRoot}($\StrieNode$) initializes $\StrieNode$ to $\{ 1, n, \epsilon \}$;
%\item \textsc{isLeaf}($\StrieNode$) returns true iff $\StrieNode.r \in \mathbb{I}$;
\item \textsc{isLeaf}($\StrieNode$) returns true iff $\StrieNode.l_r = \$$;
\item \textsc{label}($\StrieNode$) returns $\StrieNode.e$.
\end{itemize}
%In order to answer \textsc{isLeaf}, I consider the set $\mathbb{I}$ of positions of the terminator symbols $s^i$ in the BWT.

The traversal easily goes from the root node to its child node labeled by $c$: it suffices to derive the interval $[C(c),C(c+1)]$.
Suppose the traversal is on an arbitrary node $v$ of known interval $[b_v, e_v]$ \st the path from the root to $v$ spells the substring $s_v$.
Now, the traversal goes down to a child node $w$ of unknown interval $[b_w, e_w]$ \st the path from the root to $w$ spells $c \cdot s_v$ for some $c \in \Sigma$.
The known interval $[b_v, e_v]$ contains all prefixes of $\bar{\Strings}$ ending with $s_v$, \ie all suffixes of $\Strings$ starting with $s_v$, while the unknown interval $[b_w, e_w]$ contains all prefixes of $\bar{\Strings}$ ending with $c \cdot s_v$, \ie all suffixes of $\Strings$ starting with $s_v \cdot c$.
All these characters $c$ are in $l_{b_v \dots e_v}$, since $l_i$ is the character $\Strings_{A[i] - 1}$ preceding the suffix pointed by $A[i]$.
Moreover, these characters $c$ are \emph{contiguous} and \emph{in relative order} in $f$ (see observations~\ref{obs:lf-a}--\ref{obs:lf-b}).
If $b$ and $e$ are the first and last position in $l$ within $[b_v, e_v]$ such that $l_b = c$ and $l_e = c$, then $b_w = LF(b)$ and $e_w = LF(e)$.
Therefore $LF(b)$ becomes:
\begin{eqnarray}
\begin{array}{lcl}
LF(b) &=& C(l_b) + Occ(l_b, b)\\
 	  &=& C(c) + Occ(c, b)\\
	  &=& C(c) + Occ(c, b_v - 1) + 1
\end{array}
\end{eqnarray}
and analogously $LF(e)$ becomes $C(c) + Occ(c, e_v)$ \cite{Ferragina2000}.

Algorithm \ref{alg:fm-godownc} implements the operation \textsc{goDown} a symbol.
This algorithm computes two values of permutation $LF$ and thus runs in time $\Oh(1)$.
Conversely, \textsc{goDown} and \textsc{goRight} are provided by the generic algorithms \ref{alg:st-godown} and \ref{alg:st-goright} running in time $\Oh(\sigma)$.

% This is backward search. The traversal computes the interval.

\begin{figure*}
\begin{center}
\begin{minipage}[t]{.5\textwidth}
\begin{algorithm}[H]
\Algorithm{goDown}{$\StrieNode, c$}
\begin{tabular}{ll}
\textbf{Input}  & $\StrieNode$ : pointer to an FM-index node\\
				& $c$ : char to query\\
\textbf{Output} & boolean indicating success\\
\end{tabular}
\begin{algorithmic}[1]
\If {\Call{isLeaf}{$\StrieNode$}}
	\State \Return \False
\EndIf
\State {$x.l \gets \Call{LF}{x.l,c}$}
\State {$x.r \gets \Call{LF}{x.r,c}$}
\State {$x.e \gets c$}
\State \Return $x.l < x.r$
\end{algorithmic}
\label{alg:fm-godownc}
\end{algorithm}
\end{minipage}
\end{center}
\end{figure*}

\subsubsection{Sampled suffix array}

The suffix array $A$ is required to locate occurrences, yet it is not appealing to maintain the whole array.
As proposed by \cite{Ferragina2000}, I maintain a \emph{sampled} suffix array $A^{\epsilon}$ containing positions sampled at regular intervals in the input string.
In order to determine if and where I sampled any $A[i]$ in $A^{\epsilon}$, I maintain a binary rank dictionary $S$ of length $n$: if $S[i]=1$, then I sampled $A[i]$ in $A^{\epsilon}[rank_1(S,i)]$.
I obtain any $A[i]$ by finding the smallest $j \geq 0$ such that $LF^j(i)$ is in $A^{\epsilon}$, and then $A[i] = A[LF^j(i)] + j$.

By sampling one text position out of $\log^{1+\epsilon}{n}$, for some $\epsilon > 0$, then $A^{\epsilon}$ consumes $\Oh(\frac{n}{\log^{\epsilon}{n}})$ space and \textsc{occurrences}($\StrieNode$) returns all occurrences in $\Oh(o \cdot \log^{1+\epsilon}{n})$ time \citep{Ferragina2000}.
In practice, I sample text positions at rates between $2^{-3}$ and $2^{-5}$.
The rank dictionary $S$ consumes $n+o(n)$ extra space, independently of the sampling rate.

\section{Algorithms}
\label{sec:index:algo}

In this section, I give string matching algorithms that use the generic suffix trie traversal operations defined in section \ref{sub:introindex}.
Thus the following algorithms can be applied to all of the suffix trie implementations presented so far.
I first consider a simple algorithm performing a top-down traversal bounded by depth.
Then, I present algorithms for exact string matching and $k$-mismatches.% and $k$-differences.
I finally give, for the first time to the best of my knowledge, algorithms solving multiple variants of indexed exact string matching and $k$-mismatches.
At the same time, I show the results of an experimental evaluation of all of these algorithms on various suffix trie implementations.

As data structures, I consider the fully-constructed lazy suffix tree (LST), the enhanced suffix array (ESA), the suffix array (SA), the $q$-gram index for $q=12$ (q-Gram), the FM-index with a two-levels DNA rank dictionary (FM-TL), the FM-index with a wavelet tree composed of two-levels binary rank dictionaries (FM-WT).
As text, I take the \emph{C.~elegans} reference genome (WormBase WS195), \ie a collection of 6 DNA strings of about 100~Mbp total length.
As patterns, I use sequences extrapolated from an Illumina sequencing run (SRA/ENA id: SRR065390).

All experiments run on a desktop computer running Linux~3.10.11, equipped with one Intel\textsuperscript{\textregistered} Core i7-4770K CPU @ 3.50\,GHz, 32\,GB RAM and a 2\,TB HDD @ 7200\,RPM.
The plots show always \emph{average} runtimes per pattern, both in single and multiple string matching variants.
Moreover, they consider only traversal times, while they exclude the time to locate the patterns in the text by following leaf pointers, \eg uncompressing SA values.
%The total search time for short pattern is dominated the time to locate the occurrences, yet no practical application needs to locate so many occurrences.

\subsection{Construction}
\label{sec:index:algo:construction}

Table \ref{tab:index:construction} shows construction times and memory consumption for all indices.
The construction of the LST uses the quadratic-time in-memory \emph{wotd} algorithm \citep{Giegerich1999}, while the construction of all other indices relies on the linear-time external-memory \emph{DC7} suffix array construction algorithm \citep{Dementiev2008}.
In addition, the construction of the ESA uses the linear-time algorithms in \citep{Kasai2001,Abouelhoda2004}, while the FM-WT construction follows the linear-time algorithm proposed in \citep{Grossi2003}.
For additional information on the SA and ESA construction algorithms and their runtimes, refer to \citep{Weese2013}.

On the chosen text, the practical runtime of the \emph{wotd} algorithm (LST) is in line with that one of the \emph{DC7} algorithm (SA). The construction of the $q$-gram index directory adds only 3\% runtime over the plain SA construction, the FM-indices adds a 15--18\% additional runtime, while the more involved ESA's LCP and child tables add 36\% overhead.
As expected, the FM-TL and FM-WT are the most compact data structures, while the LST followed by the ESA are the most space inefficient ones.

\begin{table}[t]
\begin{center}
\caption[Index construction times and memory footprints]{Index construction times and memory footprints.}
\sffamily
\input{tables/table_index_construction}
\label{tab:index:construction}
\end{center}
\end{table}

\subsection{Top-down traversal bounded by depth}
\label{sec:index:algo:traversal}

Before turning to proper string matching algorithms, I present a simple algorithm that helps to comprehend subsequent backtracking algorithms.
Algorithm \ref{alg:st-dfs} performs the top-down traversal of a suffix trie in depth-first order.
The traversal is bounded, \ie after reaching the nodes at depth $d$ it stops going down and goes right instead.

The experimental evaluation shown in figure \ref{fig:visit-dna} provides a first glimpse on what are the practical performances of various suffix trie implementations.
As expected, the WT FM-index is always slower than the TL FM-index.
The $q$-gram index is never slower than the SA alone, however the contribution of the $q$-gram directory becomes insignificant for deep traversals.
%The SA is significantly slower than all other indices as binary search converges slowly.

Depth 12 marks the turning point, as the indices become sparse.
The TL FM-index is the fastest index up to depth $10$, while the $q$-gram index and the LST are the fastest at depths 10--12.
Below depth 12, the tree indices (ESA and LST) become significantly faster than the trie indices (SA, $q$-gram and FM-indices).
In particular, the FM-indices, which are based on backward search, become more than one order of magnitude slower than tree indices.

\begin{figure*}[t]
\begin{center}
\begin{minipage}[t]{.7\textwidth}
\begin{algorithm}[H]
\Algorithm{Dfs}{$x,d$}
\begin{tabular}{ll}
\textbf{Input}  & $x$ : pointer to the root node of a suffix trie\\
 			    & $d$ : integer bounding the traversal depth\\
\end{tabular}
\begin{algorithmic}[1]
\If {$d > 0$}
	\If {\Call{goDown}{$x$}}
		\Repeat
			\State \Call{Dfs}{$x,d - 1$}
		\Until \Call{goRight}{$x$}
	\EndIf
\EndIf
\end{algorithmic}
\label{alg:st-dfs}
\end{algorithm}
\end{minipage}
\end{center}
\end{figure*}

\begin{figure}[b]
\begin{center}
\caption[Top-down traversal runtime]{Runtime of the bounded top-down traversal of various suffix trie implementations.}
\label{fig:visit-dna}
\includegraphics{visit.dna.celegans.pdf}
\end{center}
\end{figure}

\subsection{Exact string matching}
\label{sec:index:algo:exact}

I now give a simple algorithm performing exact string matching on a generic suffix trie.
In the following, I assume the text $t$ to be indexed by its suffix trie $\StrieOf{\Strings}$.
Algorithm~\ref{alg:st-exact} searches the pattern $p$ by starting on the root node of $\StrieOf{\Strings}$ and following the path spelling the pattern.
If the search ends up in a node $\StrieNode$, then each leaf $\StrieLeaf[i]$ below $\StrieNode$ points to a distinct suffix $t_{i..n}$ such that $t_{i..i+m}$ equals $p$.
%Algorithm~\ref{alg:st-exact} is correct since each path from the root to any internal node of the suffix trie $\StrieOf{\Strings}$ spells a different unique substring of $t$; consequently all equal substrings of $t$ are represented by a single common path.
If \textsc{goDown} is implemented in constant time and \textsc{occurrences} in linear time, all occurrences of $p$ into $t$ are found in optimal time $\Oh(m+o)$, where $m$ is the length of $p$ and $o$ its number of occurrences in $t$.

Figure \ref{fig:query-dna-exact} shows the results of the experimental evaluation of algorithm~\ref{alg:st-exact}.
On forward indices (LST, ESA, SA and $q$-gram index) the search time becomes practically constant for patterns of length above 15, \ie when the tree becomes sparse.
Conversely, on backward (FM) indices the practical search time stays linear in the pattern length. 

The ESA and LST are never faster than the $q$-gram index despite their higher memory consumption.
The SA alone is at least 20\,\% slower than the $q$-gram index, hence never competitive.
In particular, the SA shows a runtime peak for patterns of length 10, due to the fact that binary search algorithms \ref{alg:sa-lower}--\ref{alg:sa-upper} converge more slowly for shorter patterns.

\begin{figure*}[t]
\begin{center}
\begin{minipage}[t]{.8\textwidth}
\begin{algorithm}[H]
\Algorithm{ExactSearch}{$t,p$}
\begin{tabular}{ll}
\textbf{Input}  & $t$ : pointer to the root node of the suffix trie of the text\\
				& $p$ : pointer to the pattern\\
\textbf{Output} & list of all occurrences of the pattern in the text\\
\end{tabular}
\begin{algorithmic}[1]
\If {\Call{atEnd}{$p$}}
	\State \Report \Call{occurrences}{$t$}
\ElsIf {\Call{goDown}{$t,\Call{value}{p}$}}
		\State {\Call{goNext}{$p$}}
		\State \Call{ExactSearch}{$t,p$}
\EndIf
\end{algorithmic}
\label{alg:st-exact}
\end{algorithm}
\end{minipage}
\end{center}
\end{figure*}

\begin{figure}[b]
\begin{center}
\caption[Exact string matching runtime]{Runtime of exact string matching on various suffix trie implementations.}
\label{fig:query-dna-exact}
\includegraphics{query.dna.celegans.0.pdf}
\end{center}
\end{figure}


Concerning FM-indices, the WT variant is almost twice as slow as the TL variant, as the WT-based rank dictionary performs twice the number of random memory accesses than the levels rank dictionary.
Summing up, the TL FM-index is the fastest index to match exact patterns within length 30, while the $q$-gram index is the fastest for patterns above length 30.

%\begin{figure}[h]
%\begin{center}
%\caption[Exact string matching on a suffix trie]{Exact string matching on a suffix trie. The pattern NA is searched exactly in the text ANANAS\$.}
%\label{fig:st-exact}
%\input{figures/strie_exact.tikz}
%\end{center}
%\end{figure}

\subsection{Backtracking $k$-mismatches}
\label{sec:index:algo:kmismatches}

I now give an algorithm that solves $k$-mismatches by backtracking a generic suffix trie.
The idea of backtracking a suffix tree has been first proposed in \citep{Ukkonen1993}.
Recently, various popular bioinformatics tools, \eg Bowtie \citep{Langmead2009} and BWA \citep{Li2009}, adopted variations of this method in conjunction with an FM-index.
Yet, the idea dates back to more than twenty years ago.

Algorithm~\ref{alg:st-hamming} performs a top-down traversal on the suffix trie $\StrieOf{\Strings}$, spelling incrementally all distinct substrings of $t$.
While traversing each branch of the trie, this algorithm incrementally computes the distance between the query and the spelled string.
If the computed distance exceeds $k$, the traversal backtracks and proceeds on the next branch.
Conversely, if the pattern $p$ is completely spelled and the traversal ends up in a node $\StrieNode$, each leaf $\StrieLeaf[i]$ below $\StrieNode$ points to a distinct suffix $t_{i..n}$ such that $d_H(t_{i..i+m}, p) \leq k$.

\begin{figure}[b]
\begin{center}
\caption[$k$-mismatches runtime]{Runtime of $1$-mismatch search on various suffix trie implementations.}
\label{fig:query-dna-apx}
\includegraphics{query.dna.celegans.1.pdf}
\end{center}
\end{figure}

\begin{figure*}[t]
\begin{center}
\begin{minipage}[t]{.8\textwidth}
\begin{algorithm}[H]
\Algorithm{KMismatches}{$t,p,k$}
\begin{tabular}{ll}
\textbf{Input}  & $t$ : pointer to the root node of the suffix trie of the text\\
 			    & $p$ : pointer to the pattern\\
 			    & $k$ : integer bounding the number of mismatches\\
\textbf{Output} & list of all occurrences of the pattern in the text\\
\end{tabular}
\begin{algorithmic}[1]
\If {$k = 0$}
	\State {\Call{ExactSearch}{$t,p$}}
\Else
	\If {\Call{atEnd}{$p$}}
		\State \Report \Call{occurrences}{$t$}
	\ElsIf {\Call{goDown}{$t$}}
		\Repeat
			\State {$d \gets \delta(\Call{label}{t}, \Call{value}{p})$}
			\State \Call{goNext}{$p$}
			\State \Call{KMismatches}{$t,p,k - d$}
			\State \Call{goPrevious}{$p$}
		\Until \Call{goRight}{$t$}
	\EndIf
\EndIf
\end{algorithmic}
\label{alg:st-hamming}
\end{algorithm}
\end{minipage}
\end{center}
\end{figure*}

%The \emph{worst case} runtime of algorithm \ref{alg:st-hamming} is clearly independent of the text length $n$.
%According to \citep{Navarro2000}, this algorithm exhibits average time sublinear in $n$.

%\begin{figure}[h]
%\begin{center}
%\caption[Backtracking $k$-mismatches]{$k$-mismatches on a suffix trie.}
%\label{fig:st-hamming}
%\input{figures/strie_exact.tikz}
%\end{center}
%\end{figure}

Figure \ref{fig:query-dna-apx} shows the results of the experimental evaluation of algorithm~\ref{alg:st-hamming} for $k=1$.
The TL FM-index is always faster than any other index: for instance, on patterns of length 30, the SA is 3 times slower; even the $q$-gram index is 50\,\% slower than the TL FM-index.
Despite their tree structure, ESA and LST are always slower than $q$-gram and FM-indices.

On the TL FM-index, $1$-approximate matching of patterns of length 30 is 16 times slower than exact matching: on average, $1$-approximate matching spends $21$ microseconds ($\mu s$), while exact matching takes $1.3 \, \mu s$.
On the ESA, $1$-approximate matching shows a slow-down of 24 times: $1$-approximate matching spends $40.8 \, \mu s$, compared to the $1.7 \, \mu s$ for exact matching.

%\subsection{Backtracking $k$-differences}
%\label{sec:index:algo:kdifferences}
%
%I present two alternative algorithms for $k$-differences.
%Algorithm~\ref{alg:st-edit-explicit} explicitly enumerates errors while traversing the suffix trie. Conversely, algorithm~\ref{alg:st-edit} computes the edit distance between the pattern and any branch of the suffix trie.
%This latter algorithm necessitates of a method capable of checking incrementally whether the edit distance at any node is within the imposed threshold $k$.
%
%%Algorithm~\ref{alg:st-edit-explicit} reports more occurrences than algorithm~\ref{alg:st-edit}.
%%Discuss neighborhood, condensed neighborhood, and super-condensed neighborhood.
%%To obtain a better theoretical runtime, \citep{Navarro2000} consider an algorithm that computes in $\Oh(1)$ per node.
%
%\begin{figure*}[b]
%\begin{center}
%\begin{minipage}[t]{.8\textwidth}
%\begin{algorithm}[H]
%\Algorithm{KDifferences}{$t,p,e$}
%\begin{tabular}{ll}
%\textbf{Input}  & $t$ : pointer to the root node of the suffix trie of the text\\
% 			    & $p$ : pointer to the root node of the trie of the patterns\\
% 			    & $e$ : integer bounding the number of errors\\
%\textbf{Output} & list of all occurrences of the pattern in the text\\
%\end{tabular}
%\begin{algorithmic}[1]
%\If {$e = k$}
%	\State \Call{ExactSearch}{$t,p$}
%\Else
%	\State \Call{goNext}{$p$}
%	\State \Call{KDifferences}{$t,p,e+1$}
%	\State \Call{goPrevious}{$p$}
%	\If {\Call{goDown}{$t$}}
%		\Repeat
%			\State \Call{KDifferences}{$t,p,e+1$}
%			\State {$d \gets \delta(\Call{label}{t}, \Call{value}{p})$}
%			\State \Call{goNext}{$p$}
%			\State \Call{KDifferences}{$t,p,e + d$}
%			\State \Call{goPrevious}{$p$}
%		\Until {\Call{goRight}{$t$}}
%	\EndIf
%\EndIf
%\end{algorithmic}
%\label{alg:st-edit-explicit}
%\end{algorithm}
%\end{minipage}
%\end{center}
%\end{figure*}
%
%\begin{figure*}[b]
%\begin{center}
%\begin{minipage}[t]{.8\textwidth}
%\begin{algorithm}[H]
%\Algorithm{KDifferences}{$t,p,D$}
%\begin{tabular}{ll}
%\textbf{Input}  & $t$ : pointer to the root node of the suffix trie of the text\\
% 			    & $p$ : pointer to the root node of the trie of the patterns\\
% 			    & $D$ : vector of integers representing a DP column\\
%\textbf{Output} & list of all occurrences of the pattern in the text\\
%\end{tabular}
%\begin{algorithmic}[1]
%\If {$D[m] \leq k$}
%	\State \Report \Call{occurrences}{$t$}
%\ElsIf {$\min{D} \leq k$}
%	\If {\Call{goDown}{$t$}}
%		\Repeat
%			\State {$D' \gets$ \Call{DP}{$D, \Call{label}{t}, p$}}
%			\State \Call{goNext}{$p$}
%			\State \Call{KDifferences}{$t,p,D'$}
%			\State \Call{goPrevious}{$p$}
%		\Until {\Call{goRight}{$t$}}
%	\EndIf
%\EndIf
%\end{algorithmic}
%\label{alg:st-edit}
%\end{algorithm}
%\end{minipage}
%\end{center}
%\end{figure*}
%
%%\begin{algorithm}[h]
%%\caption{$k$-difference on a suffix trie.}
%%\label{alg:st-edit}
%%\begin{algorithmic}[1]
%%\Procedure{KDifferences}{$\StrieNode,p,e$}
%%	\ForAll {$\StrieChar \in \Ci(\StrieNode)$}
%%		\State \Call{KDifferences}{$\StrieChar,p_{2..|p|},e - d_E(repr(\StrieNode), p)$}
%%	\EndFor
%%\EndProcedure
%%\end{algorithmic}
%%\end{algorithm}


\subsection{Multiple exact string matching}
\label{sec:index:algo:multiexact}

Before turning to multiple $k$-mismatches, I describe a simpler algorithm for multiple exact string matching.
In addition to the text $t$, multiple exact string matching provides a collection of patterns $\Patterns$.
Hence, in addition to the suffix trie $\mathcal{T}$ of $t$, algorithm \ref{alg:st-exact-multi} considers the trie $\mathcal{P}$ of $\Patterns$.
Algorithm \ref{alg:st-exact-multi} matches simultaneously in $\mathcal{T}$ all patterns indexed in $\mathcal{P}$.
The traversal performed by algorithm \ref{alg:st-exact-multi} visits pairs of nodes in $\mathcal{T} \times \mathcal{P}$ whose entering edges have the same label.
Such traversal implicitly \emph{intersects} the two tries.
However, algorithm \ref{alg:st-exact-multi} is not symmetric: $\mathcal{T}$ and $\mathcal{P}$ cannot be interchanged.
The traversal stops whenever it reaches a leaf node in $\mathcal{P}$ and reports the occurrences pointed by all the leaves beneath the current node in $\mathcal{T}$.
%If the collection $\Patterns$ contains two equal strings, some leaf of $\mathcal{P}$ points to more than one pattern.

\begin{figure*}[t]
\begin{center}
\begin{minipage}[t]{.8\textwidth}
\begin{algorithm}[H]
\Algorithm{MultipleExactSearch}{$t,p$}
\begin{tabular}{ll}
\textbf{Input}  & $t$ : pointer to the root node of the suffix trie of the text\\
 			    & $p$ : pointer to the root node of the trie of the patterns\\
\textbf{Output} & list of all occurrences of any pattern in the text\\
\end{tabular}
\begin{algorithmic}[1]
\If {\Call{isLeaf}{$p$}}
	\State \Report \Call{occurrences}{$t$} $\times$ \Call{occurrences}{$p$}
\Else
	\State \Call{goDown}{$p$}
	\Repeat
		\If {\Call{goDown}{$t, \Call{label}{p}$}}
			\State \Call{MultipleExactSearch}{$t,p$}
			\State \Call{goUp}{$t$}
		\EndIf
	\Until {\Call{goRight}{$p$}}
\EndIf
\end{algorithmic}
\label{alg:st-exact-multi}
\end{algorithm}
\end{minipage}
\end{center}
\end{figure*}

\begin{figure}[b]
\begin{center}
\caption[Multiple exact string matching runtime]{Runtime of multiple exact string matching on various suffix trie implementations. Pattern length is fixed to 15. Preprocessing times are shown in black.}
\label{fig:query-dna-exact-multi}
\includegraphics{multi.dna.celegans.0.15.pdf}
\end{center}
\end{figure}

The experimental evaluation compares algorithm~\ref{alg:st-exact-multi} (Multiple) with algorithm~\ref{alg:st-exact} processing patterns in random order (Single) and in lexicographic order (Sorted).
Figure~\ref{fig:query-dna-exact-multi} shows the results.
These three methods ran on 10~M patterns of length 30: runtimes shown in figure \ref{fig:query-dna-exact-multi} (histogram Single) correspond to runtimes shown in figure \ref{fig:query-dna-exact} (plots at pattern length 15).
%For instance, the indices ranked by query speed are: TL FM-index, WT FM-index, $q$-gram index, ESA and SA.

Figure \ref{fig:query-dna-exact-multi} shows that a simple lexicographical sort of the patterns (histogram Sorted) speeds up algorithm \ref{alg:st-exact} on the SA and ESA by a factor of 2.
The same trick does not yield a significant speed-up on FM-indices nor on the $q$-gram index, as the $q$-gram directory already provides a cache local access pattern.

Algorithm~\ref{alg:st-exact-multi} (histogram Multiple) further reduces the traversal time.
Nonetheless, its runtime is dominated by the additional preprocessing time paid to construct the trie of the patterns.
This algorithm becomes more useful as a primitive within the multiple $k$-mismatches algorithm, presented in the following section.

\subsection{Multiple $k$-mismatches}
\label{sec:index:algo:multimismatch}

Algorithm \ref{alg:st-hamming-multi} is the straightforward generalization of algorithm \ref{alg:st-exact-multi} to $k$-mismatches.
The algorithm receives a collection of patterns $\Patterns$ and performs backtracking on $\mathcal{T}$ as in algorithm  \ref{alg:st-hamming}, this time using the associated trie $\mathcal{P}$.

\begin{figure*}[t]
\begin{center}
\begin{minipage}[t]{.8\textwidth}
\begin{algorithm}[H]
\Algorithm{MultipleKMismatches}{$t,p,k$}
\begin{tabular}{ll}
\textbf{Input}  & $t$ : pointer to the root node of the suffix trie of the text\\
 			    & $p$ : pointer to the root node of the trie of the patterns\\
 			    & $k$ : integer bounding the number of mismatches\\
\textbf{Output} & list of all occurrences of any pattern in the text\\
\end{tabular}
\begin{algorithmic}[1]
\If {$k = 0$}
	\State {\Call{MultipleExactSearch}{$t,p$}}
\Else
	\If {\Call{isLeaf}{$p$}}
		\State \Report \Call{occurrences}{$t$} $\times$ \Call{occurrences}{$p$}
	\ElsIf {\Call{goDown}{$t$}}
		\Repeat
			\State {\Call{goDown}{$p$}}
			\Repeat
				\State {$d \gets \delta(\Call{label}{t}, \Call{label}{p})$}
				\State \Call{MultipleKMismatches}{$t,p,k-d$}
			\Until {\Call{goRight}{$p$}}
			\State {\Call{goUp}{$p$}}
		\Until {\Call{goRight}{$t$}}
	\EndIf
\EndIf
\end{algorithmic}
\label{alg:st-hamming-multi}
\end{algorithm}
\end{minipage}
\end{center}
\end{figure*}

The experimental evaluation compares algorithm~\ref{alg:st-hamming-multi} (Multiple) with algorithm~\ref{alg:st-hamming} processing patterns in random order (Single) and in lexicographic order (Sorted).
All three methods ran on 10~M patterns of length 30, with $k$ fixed to 1.
Thus, runtimes shown in figure \ref{fig:query-dna-apx-multi} (histogram Single) correspond to runtimes shown in figure \ref{fig:query-dna-apx} (plots at pattern length 30).

\begin{figure}[b]
\begin{center}
\caption[Multiple $k$-mismatches runtime]{Runtime of multiple $1$-mismatch on various suffix trie implementations. Pattern length is fixed to 30. Preprocessing times are shown in black.}
\label{fig:query-dna-apx-multi}
\includegraphics{multi.dna.celegans.1.30.pdf}
\end{center}
\end{figure}

Algorithm \ref{alg:st-hamming} on lexicographically sorted patterns (histogram Sorted) is faster by a factor of 2 or more, on all indices.
The time to sort the patterns becomes insignificant compared to the traversal time.
Algorithm \ref{alg:st-exact-multi} (histogram Multiple) reduces traversal time by a factor of 4--5 on LST, ESA, and SA.
Thus, the time to construct the trie of the patterns is easily justified.
In practice, algorithm \ref{alg:st-exact-multi} fills the gap between the runtime of the SA and the $q$-gram index.
Surprisingly, algorithm \ref{alg:st-exact-multi} increases traversal time on FM-indices.

This algorithms works according to a \emph{cache-friendly} memory access pattern, which holds for forward search but not for backward search.
Using forward search, the traversal of a suffix trie becomes less expensive as it proceeds towards bottom nodes.
Indeed, traversal towards a child node involves the computation of a subinterval of the current suffix array interval; such computation accesses memory locations within the current interval, having good chances to be in the cache.
Conversely, using backward search, the traversal becomes more expensive as it proceeds deeper in the trie; traversal downwards involves the computation of intervals outside of the current one, unlikely to be in the cache as they are accessed less often than top intervals.
Multiple backtracking factorizes the traversal of top nodes, thus it pays off with forward search rather than with backward search.

Figure \ref{fig:query-dna-apx-multi-sa} shows the average runtime of the three approaches on the SA by varying the number of patterns.
While the average runtime of the single method is constant, both multiple methods clearly benefit from receiving a higher number of patterns.
In particular, method Multiple is constantly faster than Sorted, and the runtime gap increases with the number of patterns.
The speed-up of multiple methods slowly decreases, though there is still some space of improvement with more than 10~M patterns.

Figure \ref{fig:query-dna-apx-multi-fmtl} presents the same evaluation of figure \ref{fig:query-dna-apx-multi-sa}, but for the TL FM-index.
Multiple methods exhibit again decreasing average runtimes by number of patterns.
However, here method Sorted is constantly faster than Multiple, but the runtime gap decreases with the number of patterns.
Moreover, the speed-up of both multiple methods slowly increases with the number of patterns instead of decreasing.

\begin{figure}[t]
\begin{center}
\caption[Multiple $k$-mismatches speed-up on SA]{Speed-up of multiple $1$-mismatch by number of patterns on the SA. Pattern length is fixed to 30. Traversal times without preprocessing are shown by dashed lines.}
\label{fig:query-dna-apx-multi-sa}
\includegraphics{multi.dna.celegans.1.sa.pdf}
\end{center}
\end{figure}

\begin{figure}[b]
\begin{center}
\caption[Multiple $k$-mismatches speed-up on FM-index]{Speed-up of multiple $1$-mismatch by number of patterns on the TL FM-Index. Pattern length is fixed to 30. Traversal times without preprocessing are shown by dashed lines.}
\label{fig:query-dna-apx-multi-fmtl}
\includegraphics{multi.dna.celegans.1.fm-tl.pdf}
\end{center}
\end{figure}
